{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AgentProbe","text":"<p>A testing and evaluation framework for software agents.</p> <p>AgentProbe provides a comprehensive toolkit for testing autonomous software agents. Record execution traces, evaluate agent behavior, detect regressions, run safety scans, and track costs --- all from a single CLI or Python API.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#trace-recording","title":"Trace Recording","text":"<p>Capture every decision, tool call, and output during agent execution into structured, replayable traces. Replay traces for debugging or comparison.</p>"},{"location":"#behavioral-evaluation","title":"Behavioral Evaluation","text":"<p>Evaluate agent outputs using rules-based checks, embedding similarity, judge-based scoring, or statistical analysis with pluggable evaluators.</p>"},{"location":"#regression-detection","title":"Regression Detection","text":"<p>Establish baselines and automatically flag behavioral regressions between agent versions. Set custom thresholds for score deltas.</p>"},{"location":"#safety-scanning","title":"Safety Scanning","text":"<p>Run built-in test suites for prompt injection, data leakage, jailbreaking, role confusion, hallucination, and tool abuse.</p>"},{"location":"#cost-management","title":"Cost Management","text":"<p>Track token usage and costs per model, enforce per-test and per-suite budgets, and generate cost reports with breakdowns by model.</p>"},{"location":"#plugin-system","title":"Plugin System","text":"<p>Extend AgentProbe with custom evaluators, adapters, reporters, and storage backends using the plugin API.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation --- Get AgentProbe installed</li> <li>Quickstart --- Write and run your first tests</li> <li>Configuration --- Full YAML config reference</li> <li>Writing Tests --- Test patterns and best practices</li> <li>CLI Reference --- All CLI commands and options</li> <li>Evaluators --- Built-in and custom evaluators</li> <li>Plugin System --- Extend AgentProbe with plugins</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>agentprobe/\n\u251c\u2500\u2500 core/       # Test runner, discovery, assertions, config\n\u251c\u2500\u2500 eval/       # Evaluators: rules, embedding, judge, statistical\n\u251c\u2500\u2500 trace/      # Recorder, replay, diff, time travel\n\u251c\u2500\u2500 cost/       # Calculator, pricing data, budgets\n\u251c\u2500\u2500 safety/     # Scanner, test suites, payloads\n\u251c\u2500\u2500 regression/ # Detector, baselines, behavioral diff\n\u251c\u2500\u2500 adapters/   # LangChain, CrewAI, AutoGen, MCP\n\u251c\u2500\u2500 metrics/    # Collection, aggregation, trending\n\u251c\u2500\u2500 storage/    # SQLite, PostgreSQL backends\n\u251c\u2500\u2500 reporting/  # Terminal, HTML, JUnit, JSON, Markdown, CSV\n\u251c\u2500\u2500 plugins/    # Plugin loader, registry, base classes\n\u2514\u2500\u2500 cli/        # Click-based CLI commands\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to AgentProbe will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#050-2025-05-15","title":"[0.5.0] - 2025-05-15","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>PostgreSQL <code>load_result(result_id)</code> method for API parity with SQLite</li> <li>Dedicated migration test suite (<code>test_migrations.py</code>)</li> <li><code>TraceDiffer</code> for structural comparison between any two traces</li> <li><code>TraceDiffReport</code> model with token/latency deltas and similarity scoring</li> <li>Dashboard CLI command tests (<code>test_dashboard_cmd.py</code>)</li> <li>Pricing data for Google (Gemini), Mistral, and Cohere models</li> <li><code>FieldEncryptor</code> with SHA-256 hashing and partial masking</li> <li><code>AuditLogger</code> with structured JSON event logging</li> <li>Parametrized tests across 6 test modules for edge case coverage</li> <li>PyPI release workflow with trusted publisher</li> <li>MkDocs documentation deployment to GitHub Pages</li> <li>Dependabot configuration for pip and GitHub Actions</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Development status upgraded from Alpha to Beta</li> <li>Version bumped from 0.4.0 to 0.5.0</li> </ul>"},{"location":"changelog/#040-2025-04-15","title":"[0.4.0] - 2025-04-15","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Dashboard REST API with FastAPI: health, traces, results, and metrics endpoints</li> <li><code>agentprobe dashboard</code> CLI command with lazy FastAPI/uvicorn imports</li> <li>API reference documentation for all 12 top-level modules via mkdocstrings</li> <li>Test factories for cost, safety, regression, and trace replay models</li> <li>Example scripts for cost management, metrics trending, plugin creation, custom adapters, and dashboard usage</li> <li>Plugin system: loader, registry, base classes, and plugin manager</li> <li>Metric collection, aggregation, and trend analysis</li> <li>PostgreSQL storage backend with asyncpg</li> <li>Database migration system (V1: traces/results, V2: metrics)</li> <li>CLI commands for metrics (<code>metrics list</code>, <code>metrics summary</code>)</li> <li>Integration test suite covering all cross-module pipelines</li> <li>Full project documentation with MkDocs Material</li> <li>Docker support with multi-stage build</li> <li>CONTRIBUTING.md and LICENSE</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Consolidated CI into single workflow with test matrix, artifact uploads, and JUnit reporting</li> <li>Expanded <code>agentprobe.yaml.example</code> and <code>init</code> template to cover all 13 config sections</li> <li>Version bumped from 0.1.0 to 0.4.0 to reflect Phases 1-4 feature completeness</li> </ul>"},{"location":"changelog/#030-2025-03-15","title":"[0.3.0] - 2025-03-15","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Plugin system with <code>PluginBase</code> abstract class and four typed subclasses</li> <li><code>PluginRegistry</code> for managing plugin lifecycle</li> <li><code>PluginLoader</code> with entry point and file-based discovery</li> <li><code>PluginManager</code> for dispatching lifecycle hooks with error isolation</li> <li><code>MetricCollector</code> for stateless metric extraction from traces and results</li> <li><code>MetricAggregator</code> using stdlib <code>statistics</code> for summary computation</li> <li><code>MetricTrend</code> for trend analysis across multiple runs</li> <li>Six built-in metric definitions (latency, token count, cost, score, tool calls, error rate)</li> <li>PostgreSQL storage backend with connection pooling via asyncpg</li> <li>Migration system for schema versioning</li> <li>SQLite extended with metrics table</li> <li><code>metrics list</code> and <code>metrics summary</code> CLI commands</li> <li>Integration tests for plugin, metrics, and storage pipelines</li> </ul>"},{"location":"changelog/#020-2025-02-15","title":"[0.2.0] - 2025-02-15","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li><code>ChaosProxy</code> for fault injection into agent tool calls</li> <li><code>SnapshotManager</code> for golden-file comparison testing</li> <li><code>BudgetEnforcer</code> for per-test and per-suite cost limits</li> <li><code>StatisticalEvaluator</code> for multi-run score aggregation</li> <li><code>TraceComparisonEvaluator</code> for structural trace diffing</li> <li><code>ConversationRunner</code> for multi-turn dialogue testing</li> <li><code>RegressionDetector</code> and <code>BaselineManager</code> for behavioral diff</li> <li><code>ReplayEngine</code> and <code>TimeTravel</code> for trace replay and inspection</li> <li>Six safety test suites: prompt injection, data leakage, jailbreak, role confusion, hallucination, tool abuse</li> <li>PII redaction scanner in <code>security/pii.py</code></li> <li>Six reporting formats: terminal, HTML, JUnit XML, JSON, Markdown, CSV</li> <li>CLI commands: <code>baseline</code>, <code>snapshot</code>, <code>cost</code>, <code>safety</code></li> <li>Configuration sections: <code>chaos</code>, <code>snapshot</code>, <code>budget</code>, <code>regression</code></li> </ul>"},{"location":"changelog/#010-2025-01-15","title":"[0.1.0] - 2025-01-15","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Core test framework: <code>TestRunner</code>, <code>TestCase</code>, <code>TestResult</code>, <code>TestRun</code> models</li> <li><code>@scenario</code> decorator and test discovery system</li> <li>Assertion builder with fluent API (<code>expect</code>, <code>expect_tool_calls</code>)</li> <li><code>TraceRecorder</code> with async context manager for capturing agent execution</li> <li><code>Trace</code>, <code>LLMCall</code>, <code>ToolCall</code> data models (frozen Pydantic)</li> <li>Configuration loading from <code>agentprobe.yaml</code> with <code>${ENV_VAR}</code> interpolation</li> <li><code>CostCalculator</code> with YAML-based pricing data</li> <li>Rules-based, embedding, and judge evaluators</li> <li>SQLite storage backend with WAL mode</li> <li>Framework adapters: LangChain, CrewAI, AutoGen, MCP</li> <li>Click-based CLI with <code>init</code> and <code>test</code> commands</li> <li>Custom exception hierarchy</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>AgentProbe is configured via <code>agentprobe.yaml</code> in your project root. Run <code>agentprobe init</code> to generate a starter config file.</p>"},{"location":"getting-started/configuration/#config-file-discovery","title":"Config File Discovery","text":"<p>AgentProbe searches for configuration in this order:</p> <ol> <li>Path passed via <code>--config</code> / <code>-c</code> CLI flag</li> <li><code>agentprobe.yaml</code> in the current directory</li> <li><code>agentprobe.yml</code> in the current directory</li> <li>If no file is found, default values are used</li> </ol>"},{"location":"getting-started/configuration/#environment-variable-interpolation","title":"Environment Variable Interpolation","text":"<p>Use <code>${VAR_NAME}</code> syntax to reference environment variables:</p> <pre><code>trace:\n  database_path: ${AGENTPROBE_DB_PATH}\n</code></pre> <p>If the variable is not set, a warning is logged and the literal <code>${VAR_NAME}</code> string is kept.</p>"},{"location":"getting-started/configuration/#full-configuration-reference","title":"Full Configuration Reference","text":""},{"location":"getting-started/configuration/#top-level","title":"Top-Level","text":"Key Type Default Description <code>project_name</code> <code>string</code> <code>\"agentprobe\"</code> Name of the project being tested <code>test_dir</code> <code>string</code> <code>\"tests\"</code> Directory containing test files"},{"location":"getting-started/configuration/#runner","title":"<code>runner</code>","text":"<p>Controls test execution behavior.</p> Key Type Default Description <code>parallel</code> <code>bool</code> <code>false</code> Run tests in parallel <code>max_workers</code> <code>int</code> <code>4</code> Maximum concurrent tests (min: 1) <code>default_timeout</code> <code>float</code> <code>30.0</code> Default test timeout in seconds"},{"location":"getting-started/configuration/#eval","title":"<code>eval</code>","text":"<p>Evaluator configuration.</p> Key Type Default Description <code>default_evaluators</code> <code>list[str]</code> <code>[]</code> Evaluator names applied to all tests"},{"location":"getting-started/configuration/#judge","title":"<code>judge</code>","text":"<p>Settings for the judge evaluator.</p> Key Type Default Description <code>model</code> <code>string</code> <code>\"claude-sonnet-4-5-20250929\"</code> Model to use for judging <code>provider</code> <code>string</code> <code>\"anthropic\"</code> API provider name <code>temperature</code> <code>float</code> <code>0.0</code> Sampling temperature (0.0--2.0) <code>max_tokens</code> <code>int</code> <code>1024</code> Maximum response tokens"},{"location":"getting-started/configuration/#trace","title":"<code>trace</code>","text":"<p>Trace recording and storage.</p> Key Type Default Description <code>enabled</code> <code>bool</code> <code>true</code> Whether to record traces <code>storage_backend</code> <code>string</code> <code>\"sqlite\"</code> Storage backend (<code>sqlite</code> or <code>postgresql</code>) <code>database_path</code> <code>string</code> <code>\".agentprobe/traces.db\"</code> Path to database file or DSN"},{"location":"getting-started/configuration/#cost","title":"<code>cost</code>","text":"<p>Cost tracking settings.</p> Key Type Default Description <code>enabled</code> <code>bool</code> <code>true</code> Whether to track costs <code>budget_limit_usd</code> <code>float\\|null</code> <code>null</code> Maximum cost per run <code>pricing_dir</code> <code>string\\|null</code> <code>null</code> Custom pricing YAML directory"},{"location":"getting-started/configuration/#safety","title":"<code>safety</code>","text":"<p>Safety testing configuration.</p> Key Type Default Description <code>enabled</code> <code>bool</code> <code>false</code> Whether to run safety tests <code>suites</code> <code>list[str]</code> <code>[]</code> Safety suite names to run"},{"location":"getting-started/configuration/#chaos","title":"<code>chaos</code>","text":"<p>Chaos fault injection configuration.</p> Key Type Default Description <code>enabled</code> <code>bool</code> <code>false</code> Whether chaos testing is enabled <code>seed</code> <code>int</code> <code>42</code> Random seed for reproducibility <code>default_probability</code> <code>float</code> <code>0.5</code> Probability of injecting a fault (0.0--1.0)"},{"location":"getting-started/configuration/#snapshot","title":"<code>snapshot</code>","text":"<p>Snapshot (golden file) testing.</p> Key Type Default Description <code>enabled</code> <code>bool</code> <code>false</code> Whether snapshot testing is enabled <code>snapshot_dir</code> <code>string</code> <code>\".agentprobe/snapshots\"</code> Directory for snapshot files <code>update_on_first_run</code> <code>bool</code> <code>true</code> Create snapshots on first run <code>threshold</code> <code>float</code> <code>0.8</code> Similarity threshold (0.0--1.0)"},{"location":"getting-started/configuration/#budget","title":"<code>budget</code>","text":"<p>Per-test and per-suite cost budgets.</p> Key Type Default Description <code>test_budget_usd</code> <code>float\\|null</code> <code>null</code> Maximum cost per individual test <code>suite_budget_usd</code> <code>float\\|null</code> <code>null</code> Maximum cost per test suite run"},{"location":"getting-started/configuration/#regression","title":"<code>regression</code>","text":"<p>Regression detection settings.</p> Key Type Default Description <code>enabled</code> <code>bool</code> <code>false</code> Whether regression detection is enabled <code>baseline_dir</code> <code>string</code> <code>\".agentprobe/baselines\"</code> Baseline file directory <code>threshold</code> <code>float</code> <code>0.05</code> Score delta threshold for flagging (0.0--1.0)"},{"location":"getting-started/configuration/#metrics","title":"<code>metrics</code>","text":"<p>Metric collection and trending.</p> Key Type Default Description <code>enabled</code> <code>bool</code> <code>true</code> Whether metric collection is enabled <code>builtin_metrics</code> <code>bool</code> <code>true</code> Collect built-in metrics automatically <code>trend_window</code> <code>int</code> <code>10</code> Number of recent runs for trend analysis (min: 2)"},{"location":"getting-started/configuration/#plugins","title":"<code>plugins</code>","text":"<p>Plugin system configuration.</p> Key Type Default Description <code>enabled</code> <code>bool</code> <code>true</code> Whether the plugin system is enabled <code>directories</code> <code>list[str]</code> <code>[]</code> Additional plugin scan directories <code>entry_point_group</code> <code>string</code> <code>\"agentprobe.plugins\"</code> Entry point group name"},{"location":"getting-started/configuration/#reporting","title":"<code>reporting</code>","text":"<p>Result reporting configuration.</p> Key Type Default Description <code>formats</code> <code>list[str]</code> <code>[\"terminal\"]</code> Output format names <code>output_dir</code> <code>string</code> <code>\"agentprobe-report\"</code> Directory for report files <p>Available formats: <code>terminal</code>, <code>json</code>, <code>junit</code>, <code>html</code>, <code>markdown</code>, <code>csv</code></p>"},{"location":"getting-started/configuration/#example-configuration","title":"Example Configuration","text":"<pre><code>project_name: my-agent-project\ntest_dir: tests/agent_tests\n\nrunner:\n  parallel: true\n  max_workers: 8\n  default_timeout: 60.0\n\neval:\n  default_evaluators:\n    - rules\n\ntrace:\n  enabled: true\n  storage_backend: sqlite\n  database_path: .agentprobe/traces.db\n\ncost:\n  enabled: true\n  budget_limit_usd: 5.00\n\nsafety:\n  enabled: true\n  suites:\n    - prompt-injection\n    - data-exfiltration\n    - jailbreak\n\nregression:\n  enabled: true\n  threshold: 0.10\n\nmetrics:\n  enabled: true\n  trend_window: 20\n\nreporting:\n  formats:\n    - terminal\n    - junit\n  output_dir: test-reports\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or later</li> <li>pip (included with Python)</li> </ul>"},{"location":"getting-started/installation/#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install agentprobe-framework\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>AgentProbe has several optional dependency groups for extended functionality:</p>"},{"location":"getting-started/installation/#postgresql-storage","title":"PostgreSQL Storage","text":"<p>For team environments that need concurrent access and JSONB queries:</p> <pre><code>pip install agentprobe-framework[postgres]\n</code></pre> <p>Requires a running PostgreSQL 16+ instance. Configure via <code>agentprobe.yaml</code>:</p> <pre><code>trace:\n  storage_backend: postgresql\n  database_path: ${AGENTPROBE_PG_DSN}\n</code></pre>"},{"location":"getting-started/installation/#embedding-based-evaluation","title":"Embedding-Based Evaluation","text":"<p>For semantic similarity scoring using embedding vectors:</p> <pre><code>pip install agentprobe-framework[eval]\n</code></pre> <p>Installs numpy for vector math operations.</p>"},{"location":"getting-started/installation/#dashboard","title":"Dashboard","text":"<p>For the web-based dashboard with real-time results:</p> <pre><code>pip install agentprobe-framework[dashboard]\n</code></pre> <p>Installs FastAPI and Uvicorn.</p>"},{"location":"getting-started/installation/#all-optional-dependencies","title":"All Optional Dependencies","text":"<pre><code>pip install agentprobe-framework[postgres,eval,dashboard]\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<p>For development or to get the latest changes:</p> <pre><code>git clone https://github.com/dyrach1o/agentprobe-framework.git\ncd agentprobe-framework\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # Linux/macOS\n# or: .venv\\Scripts\\activate  # Windows\n\n# Install with all development dependencies\npip install -e \".[dev,test,docs]\"\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Check version\nagentprobe --version\n\n# View help\nagentprobe --help\n\n# Initialize a project\nagentprobe init\n</code></pre>"},{"location":"getting-started/installation/#development-setup","title":"Development Setup","text":"<p>If you plan to contribute, install the full development environment:</p> <pre><code>pip install -e \".[dev,test,docs]\"\n\n# Run all checks\nmake check\n\n# This runs:\n#   ruff check src/ tests/    (linting)\n#   mypy src/agentprobe/      (type checking)\n#   pytest tests/unit/        (unit tests)\n</code></pre> <p>See CONTRIBUTING.md for the full development guide.</p>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Get up and running with AgentProbe in minutes.</p>"},{"location":"getting-started/quickstart/#1-install-agentprobe","title":"1. Install AgentProbe","text":"<pre><code>pip install agentprobe-framework\n</code></pre>"},{"location":"getting-started/quickstart/#2-initialize-your-project","title":"2. Initialize Your Project","text":"<pre><code>agentprobe init\n</code></pre> <p>This creates an <code>agentprobe.yaml</code> configuration file with sensible defaults.</p>"},{"location":"getting-started/quickstart/#3-write-your-first-test","title":"3. Write Your First Test","text":"<p>Create a file <code>tests/test_my_agent.py</code>:</p> <pre><code>from agentprobe import scenario, expect\n\n@scenario(\n    name=\"greeting_test\",\n    input_text=\"Say hello to the user\",\n    tags=[\"smoke\"],\n)\ndef test_greeting():\n    \"\"\"Agent should produce a friendly greeting.\"\"\"\n    pass\n</code></pre> <p>The <code>@scenario</code> decorator registers the function as a test case. When AgentProbe discovers this file, it creates a <code>TestCase</code> with the specified name, input, and tags.</p>"},{"location":"getting-started/quickstart/#4-discover-tests","title":"4. Discover Tests","text":"<pre><code>agentprobe test -d tests/\n</code></pre> <p>Output:</p> <pre><code>Discovered 1 test case(s)\n  - greeting_test [tags: smoke]\n</code></pre>"},{"location":"getting-started/quickstart/#5-working-with-traces","title":"5. Working with Traces","text":"<p>AgentProbe records execution traces for every agent run. Traces capture tool calls, token usage, latency, and outputs.</p>"},{"location":"getting-started/quickstart/#list-traces","title":"List Traces","text":"<pre><code>agentprobe trace list\n</code></pre>"},{"location":"getting-started/quickstart/#view-a-specific-trace","title":"View a Specific Trace","text":"<pre><code>agentprobe trace show &lt;trace-id&gt;\n</code></pre> <p>This shows full trace details including agent name, model, input/output, token counts, latency, and tool calls.</p>"},{"location":"getting-started/quickstart/#example-multi-scenario-test-file","title":"Example: Multi-Scenario Test File","text":"<pre><code>from agentprobe import scenario\n\n@scenario(\n    name=\"summarization_test\",\n    input_text=\"Summarize the following article: ...\",\n    tags=[\"eval\", \"summarization\"],\n)\ndef test_summarization():\n    \"\"\"Agent should produce a concise summary.\"\"\"\n    pass\n\n@scenario(\n    name=\"tool_usage_test\",\n    input_text=\"Search for the current weather in London\",\n    tags=[\"tools\", \"search\"],\n)\ndef test_tool_usage():\n    \"\"\"Agent should invoke the search tool.\"\"\"\n    pass\n\n@scenario(\n    name=\"error_handling_test\",\n    input_text=\"Process this invalid JSON: {broken\",\n    tags=[\"robustness\"],\n    timeout=10.0,\n)\ndef test_error_handling():\n    \"\"\"Agent should handle malformed input gracefully.\"\"\"\n    pass\n</code></pre>"},{"location":"getting-started/quickstart/#example-using-assertions","title":"Example: Using Assertions","text":"<pre><code>from agentprobe import scenario, expect\n\n@scenario(name=\"math_test\", input_text=\"What is 2 + 2?\")\ndef test_math():\n    pass\n\n# After running, evaluate the trace:\n# expect(trace).output_contains(\"4\")\n# expect(trace).has_tool_calls(count=0)\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration --- Customize AgentProbe for your project</li> <li>Writing Tests --- In-depth test authoring guide</li> <li>Evaluators --- Set up behavioral evaluation</li> <li>Safety Testing --- Run security test suites</li> <li>CLI Reference --- All available commands</li> </ul>"},{"location":"guides/ci-cd-integration/","title":"CI/CD Integration","text":"<p>AgentProbe integrates into CI/CD pipelines with JUnit XML output, configurable exit codes, and cost controls.</p>"},{"location":"guides/ci-cd-integration/#github-actions","title":"GitHub Actions","text":""},{"location":"guides/ci-cd-integration/#basic-setup","title":"Basic Setup","text":"<pre><code># .github/workflows/agent-tests.yml\nname: Agent Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: pip install agentprobe-framework\n\n      - name: Run agent tests\n        run: agentprobe test -d tests/\n</code></pre>"},{"location":"guides/ci-cd-integration/#with-junit-output","title":"With JUnit Output","text":"<p>Generate JUnit XML reports for CI integration:</p> <pre><code>reporting:\n  formats:\n    - terminal\n    - junit\n  output_dir: test-reports\n</code></pre>"},{"location":"guides/ci-cd-integration/#upload-test-results","title":"Upload Test Results","text":"<pre><code>      - name: Run tests\n        run: agentprobe test -d tests/\n\n      - name: Upload test results\n        uses: actions/upload-artifact@v4\n        if: always()\n        with:\n          name: test-results\n          path: test-reports/\n</code></pre>"},{"location":"guides/ci-cd-integration/#cost-controls-in-ci","title":"Cost Controls in CI","text":"<p>Set budget limits to prevent expensive tests from running unchecked:</p> <pre><code>cost:\n  enabled: true\n  budget_limit_usd: 5.00\n\nbudget:\n  suite_budget_usd: 5.00\n</code></pre>"},{"location":"guides/ci-cd-integration/#regression-checks-in-ci","title":"Regression Checks in CI","text":"<p>Compare test results against a baseline on every PR:</p> <pre><code>      - name: Run tests and check regressions\n        run: |\n          agentprobe test -d tests/\n          agentprobe baseline compare latest\n</code></pre>"},{"location":"guides/ci-cd-integration/#safety-scans-in-ci","title":"Safety Scans in CI","text":"<p>Run safety suites as a required check:</p> <pre><code>      - name: Safety scan\n        run: agentprobe safety scan -s prompt-injection -s data-leakage\n</code></pre>"},{"location":"guides/ci-cd-integration/#multiple-output-formats","title":"Multiple Output Formats","text":"<p>Generate several report formats in a single run:</p> <pre><code>reporting:\n  formats:\n    - terminal\n    - junit\n    - json\n    - html\n  output_dir: test-reports\n</code></pre>"},{"location":"guides/ci-cd-integration/#environment-variables","title":"Environment Variables","text":"<p>Store API keys as CI secrets and reference them in config:</p> <pre><code>      - name: Run tests\n        env:\n          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n        run: agentprobe test -d tests/\n</code></pre> <pre><code># agentprobe.yaml\ntrace:\n  database_path: ${AGENTPROBE_DB_PATH}\n</code></pre>"},{"location":"guides/cost-management/","title":"Cost Management","text":"<p>AgentProbe tracks token usage and costs across all agent executions, helping you manage expenses and set budgets.</p>"},{"location":"guides/cost-management/#cost-calculator","title":"Cost Calculator","text":"<p>The <code>CostCalculator</code> uses YAML-based pricing data to compute costs from traces:</p> <pre><code>from agentprobe.cost.calculator import CostCalculator\n\ncalculator = CostCalculator()\n\n# Calculate cost for a trace\nsummary = calculator.calculate_trace_cost(trace)\n\nprint(f\"Total cost: ${summary.total_cost_usd:.4f}\")\nprint(f\"Input tokens: {summary.total_input_tokens}\")\nprint(f\"Output tokens: {summary.total_output_tokens}\")\n</code></pre> <p>The <code>CostSummary</code> includes a breakdown by model:</p> <pre><code>for model, cost in summary.breakdown_by_model.items():\n    print(f\"  {model}: ${cost:.4f}\")\n</code></pre>"},{"location":"guides/cost-management/#pricing-data","title":"Pricing Data","text":"<p>AgentProbe includes built-in pricing data for common models. You can provide custom pricing via YAML files:</p> <pre><code># pricing/anthropic.yaml\nmodels:\n  - model: claude-sonnet-4-5-20250929\n    input_cost_per_1k: 0.003\n    output_cost_per_1k: 0.015\n</code></pre> <p>Point AgentProbe to your pricing directory:</p> <pre><code>cost:\n  enabled: true\n  pricing_dir: pricing/\n</code></pre>"},{"location":"guides/cost-management/#budget-enforcement","title":"Budget Enforcement","text":"<p>Set per-test and per-suite cost limits with <code>BudgetEnforcer</code>:</p> <pre><code>from agentprobe import BudgetEnforcer\n\nenforcer = BudgetEnforcer(\n    test_budget_usd=0.50,\n    suite_budget_usd=5.00,\n)\n\n# Check a single test\ncheck = enforcer.check_test(cost_summary)\n\n# Check entire suite\ncheck = enforcer.check_suite(all_cost_summaries)\n</code></pre>"},{"location":"guides/cost-management/#cli-commands","title":"CLI Commands","text":""},{"location":"guides/cost-management/#cost-report","title":"Cost Report","text":"<pre><code>agentprobe cost report\nagentprobe cost report -a my-agent\nagentprobe cost report -f json\n</code></pre>"},{"location":"guides/cost-management/#budget-status","title":"Budget Status","text":"<pre><code>agentprobe cost budget\nagentprobe cost budget --max-cost 0.50 --max-tokens 1000\n</code></pre>"},{"location":"guides/cost-management/#configuration","title":"Configuration","text":"<pre><code>cost:\n  enabled: true\n  budget_limit_usd: 10.00\n  pricing_dir: pricing/\n\nbudget:\n  test_budget_usd: 0.50\n  suite_budget_usd: 5.00\n</code></pre>"},{"location":"guides/cost-management/#best-practices","title":"Best Practices","text":"<ol> <li>Set budget limits to prevent runaway costs during development</li> <li>Track costs per model to optimize model selection</li> <li>Monitor cost trends using the metrics system</li> <li>Use pricing overrides for custom or self-hosted models</li> <li>Include cost checks in CI to catch expensive test changes</li> </ol>"},{"location":"guides/evaluators/","title":"Evaluators","text":"<p>Evaluators score agent outputs against expected behavior. AgentProbe includes several built-in evaluators and supports custom implementations.</p>"},{"location":"guides/evaluators/#built-in-evaluators","title":"Built-in Evaluators","text":""},{"location":"guides/evaluators/#rules-based-evaluator","title":"Rules-Based Evaluator","text":"<p>The <code>RuleBasedEvaluator</code> applies declarative rules with weighted scoring. Each rule checks a specific aspect of the output and contributes to the overall score.</p> <pre><code>from agentprobe.eval.rules import RuleBasedEvaluator, RuleSpec\n\nevaluator = RuleBasedEvaluator(\n    name=\"my-rules\",\n    rules=[\n        RuleSpec(\n            rule_type=\"contains_any\",\n            params={\"substrings\": [\"hello\", \"hi\", \"hey\"]},\n            weight=1.0,\n            description=\"Output should contain a greeting\",\n        ),\n        RuleSpec(\n            rule_type=\"not_contains\",\n            params={\"substrings\": [\"error\", \"fail\"]},\n            weight=2.0,\n            description=\"Output should not contain error terms\",\n        ),\n        RuleSpec(\n            rule_type=\"max_length\",\n            params={\"max_length\": 500},\n            weight=0.5,\n            description=\"Output should be concise\",\n        ),\n    ],\n)\n\nresult = await evaluator.evaluate(test_case, trace)\n</code></pre> <p>Available rule types:</p> Rule Type Parameters Description <code>contains_any</code> <code>substrings: list[str]</code> Output contains at least one substring <code>not_contains</code> <code>substrings: list[str]</code> Output does not contain any substring <code>max_length</code> <code>max_length: int</code> Output length does not exceed limit <code>regex</code> <code>pattern: str</code> Output matches a regex pattern <code>json_valid</code> (none) Output is valid JSON"},{"location":"guides/evaluators/#embedding-similarity-evaluator","title":"Embedding Similarity Evaluator","text":"<p>Compares agent output against expected output using embedding vector cosine similarity:</p> <pre><code>from agentprobe.eval.embedding import EmbeddingSimilarityEvaluator\n\nevaluator = EmbeddingSimilarityEvaluator(\n    model=\"text-embedding-3-small\",\n    provider=\"openai\",\n    threshold=0.8,\n)\n\nresult = await evaluator.evaluate(test_case, trace)\n# result.score is the cosine similarity (0.0 to 1.0)\n</code></pre> <p>Requires <code>expected_output</code> to be set on the <code>TestCase</code>. Uses caching to avoid redundant API calls.</p>"},{"location":"guides/evaluators/#judge-evaluator","title":"Judge Evaluator","text":"<p>Uses a language model to evaluate agent output against a rubric:</p> <pre><code>from agentprobe.eval.llm_judge import LLMJudge\n\nevaluator = LLMJudge(\n    model=\"claude-sonnet-4-5-20250929\",\n    provider=\"anthropic\",\n    temperature=0.0,\n    rubric=\"Evaluate whether the response is helpful, accurate, and concise.\",\n)\n\nresult = await evaluator.evaluate(test_case, trace)\n</code></pre> <p>The judge returns a structured verdict (PASS, FAIL, PARTIAL) with a score and reasoning.</p>"},{"location":"guides/evaluators/#statistical-evaluator","title":"Statistical Evaluator","text":"<p>Wraps another evaluator and runs it across multiple traces to compute aggregate statistics:</p> <pre><code>from agentprobe import StatisticalEvaluator\nfrom agentprobe.eval.rules import RuleBasedEvaluator\n\ninner = RuleBasedEvaluator(rules=[...])\nevaluator = StatisticalEvaluator(inner, pass_threshold=0.7)\n\nsummary = await evaluator.evaluate_multiple(test_case, traces)\n# summary.mean, summary.std_dev, summary.median, summary.p5, summary.p95\n</code></pre>"},{"location":"guides/evaluators/#trace-comparison-evaluator","title":"Trace Comparison Evaluator","text":"<p>Compares a trace against a reference trace across multiple dimensions:</p> <pre><code>from agentprobe import TraceComparisonEvaluator\n\nevaluator = TraceComparisonEvaluator(\n    reference_trace=baseline_trace,\n    pass_threshold=0.7,\n    weights={\n        \"tool_sequence\": 0.3,\n        \"tool_parameters\": 0.2,\n        \"output_similarity\": 0.35,\n        \"cost_deviation\": 0.15,\n    },\n)\n\nresult = await evaluator.evaluate(test_case, current_trace)\n</code></pre>"},{"location":"guides/evaluators/#configuring-default-evaluators","title":"Configuring Default Evaluators","text":"<p>Set default evaluators in <code>agentprobe.yaml</code>:</p> <pre><code>eval:\n  default_evaluators:\n    - rules\n\njudge:\n  model: claude-sonnet-4-5-20250929\n  provider: anthropic\n  temperature: 0.0\n  max_tokens: 1024\n</code></pre>"},{"location":"guides/evaluators/#evaluation-results","title":"Evaluation Results","text":"<p>Every evaluator returns an <code>EvalResult</code>:</p> Field Type Description <code>evaluator_name</code> <code>str</code> Name of the evaluator <code>verdict</code> <code>EvalVerdict</code> PASS, FAIL, PARTIAL, or ERROR <code>score</code> <code>float</code> Numeric score (0.0 to 1.0) <code>reason</code> <code>str</code> Human-readable explanation <code>metadata</code> <code>dict</code> Additional evaluator-specific data <p>Verdicts:</p> <ul> <li><code>PASS</code> --- Output meets all criteria</li> <li><code>FAIL</code> --- Output does not meet criteria</li> <li><code>PARTIAL</code> --- Output partially meets criteria</li> <li><code>ERROR</code> --- Evaluation itself failed</li> </ul>"},{"location":"guides/multi-agent-testing/","title":"Multi-Agent Testing","text":"<p>AgentProbe supports testing multiple agents and comparing their behavior side-by-side.</p>"},{"location":"guides/multi-agent-testing/#testing-multiple-agents","title":"Testing Multiple Agents","text":"<p>Use adapters to connect AgentProbe to different agent frameworks:</p> <pre><code>from agentprobe.adapters.langchain import LangChainAdapter\nfrom agentprobe.core.runner import TestRunner\nfrom agentprobe.core.config import load_config\n\nconfig = load_config()\n\nadapters = [\n    LangChainAdapter(name=\"agent-v1\"),\n    LangChainAdapter(name=\"agent-v2\"),\n]\n\nrunner = TestRunner(config=config)\n\nfor adapter in adapters:\n    run = await runner.run(test_cases, adapter)\n    print(f\"{adapter.name}: {run.passed}/{run.total_tests} passed\")\n</code></pre>"},{"location":"guides/multi-agent-testing/#comparing-agent-versions","title":"Comparing Agent Versions","text":"<p>Use trace comparison to quantify differences between agent versions:</p> <pre><code>from agentprobe import TraceComparisonEvaluator\n\ntrace_v1 = await adapter_v1.invoke(\"What is the weather?\")\ntrace_v2 = await adapter_v2.invoke(\"What is the weather?\")\n\nevaluator = TraceComparisonEvaluator(\n    reference_trace=trace_v1,\n    pass_threshold=0.7,\n)\n\nresult = await evaluator.evaluate(test_case, trace_v2)\nprint(f\"Similarity score: {result.score:.2f}\")\n</code></pre>"},{"location":"guides/multi-agent-testing/#multi-turn-conversations","title":"Multi-Turn Conversations","text":"<p>Test agents across multi-turn dialogues:</p> <pre><code>from agentprobe import ConversationRunner\n\nrunner = ConversationRunner(\n    adapter=my_adapter,\n    evaluators=[my_evaluator],\n)\n\nresult = await runner.run(\n    turns=[\n        \"Book a flight to London\",\n        \"Make it for next Tuesday\",\n        \"Economy class, please\",\n    ],\n    agent_name=\"booking-agent\",\n)\n\nprint(f\"Turns passed: {result.passed_turns}/{result.total_turns}\")\nprint(f\"Aggregate score: {result.aggregate_score:.2f}\")\n</code></pre>"},{"location":"guides/multi-agent-testing/#statistical-comparison","title":"Statistical Comparison","text":"<p>Compare agents across multiple runs using the <code>StatisticalEvaluator</code>:</p> <pre><code>from agentprobe import StatisticalEvaluator\nfrom agentprobe.eval.rules import RuleBasedEvaluator\n\ninner = RuleBasedEvaluator(rules=[...])\nevaluator = StatisticalEvaluator(inner, pass_threshold=0.7)\n\ntraces = [await adapter.invoke(input_text) for _ in range(10)]\n\nsummary = await evaluator.evaluate_multiple(test_case, traces)\nprint(f\"Mean score: {summary.mean:.3f} +/- {summary.std_dev:.3f}\")\n</code></pre>"},{"location":"guides/multi-agent-testing/#best-practices","title":"Best Practices","text":"<ol> <li>Use the same test cases across agents for fair comparison</li> <li>Run multiple iterations to account for non-deterministic behavior</li> <li>Compare costs alongside quality scores</li> <li>Track metrics over time with the metrics system</li> <li>Save baselines per agent version for regression tracking</li> </ol>"},{"location":"guides/regression-testing/","title":"Regression Testing","text":"<p>AgentProbe's regression testing system helps you detect behavioral changes between agent versions by comparing test results against saved baselines.</p>"},{"location":"guides/regression-testing/#overview","title":"Overview","text":"<p>The regression testing workflow:</p> <ol> <li>Run your test suite and save results as a baseline</li> <li>Make changes to your agent</li> <li>Run the test suite again and compare against the baseline</li> <li>Review detected regressions and improvements</li> </ol>"},{"location":"guides/regression-testing/#managing-baselines","title":"Managing Baselines","text":""},{"location":"guides/regression-testing/#save-a-baseline","title":"Save a Baseline","text":"<pre><code>agentprobe baseline save my-baseline\n</code></pre>"},{"location":"guides/regression-testing/#list-baselines","title":"List Baselines","text":"<pre><code>agentprobe baseline list\n</code></pre>"},{"location":"guides/regression-testing/#delete-a-baseline","title":"Delete a Baseline","text":"<pre><code>agentprobe baseline delete my-baseline\n</code></pre>"},{"location":"guides/regression-testing/#using-the-python-api","title":"Using the Python API","text":"<pre><code>from agentprobe import BaselineManager\n\nmanager = BaselineManager(baseline_dir=\".agentprobe/baselines\")\n\n# Save results as a baseline\npath = manager.save(\"v1.0\", test_results)\n\n# Load a baseline\nbaseline_results = manager.load(\"v1.0\")\n\n# Check if a baseline exists\nif manager.exists(\"v1.0\"):\n    print(\"Baseline found\")\n\n# List all baselines\nbaselines = manager.list_baselines()\n</code></pre>"},{"location":"guides/regression-testing/#detecting-regressions","title":"Detecting Regressions","text":"<p>The <code>RegressionDetector</code> compares current test results against a baseline and flags significant score changes:</p> <pre><code>from agentprobe import RegressionDetector\n\ndetector = RegressionDetector(threshold=0.05)\n\nreport = detector.compare(\n    baseline_name=\"v1.0\",\n    baseline_results=baseline_results,\n    current_results=current_results,\n)\n</code></pre> <p>The <code>threshold</code> parameter controls sensitivity --- a score delta must exceed this value to be flagged. Default is <code>0.05</code> (5%).</p>"},{"location":"guides/regression-testing/#configuration","title":"Configuration","text":"<pre><code>regression:\n  enabled: true\n  baseline_dir: .agentprobe/baselines\n  threshold: 0.05\n</code></pre> Key Type Default Description <code>enabled</code> <code>bool</code> <code>false</code> Enable regression detection <code>baseline_dir</code> <code>string</code> <code>.agentprobe/baselines</code> Directory for baseline files <code>threshold</code> <code>float</code> <code>0.05</code> Score delta threshold (0.0--1.0)"},{"location":"guides/regression-testing/#best-practices","title":"Best Practices","text":"<ol> <li>Save baselines at release points --- name them after versions</li> <li>Set appropriate thresholds --- too low causes noise, too high misses real regressions</li> <li>Integrate into CI/CD --- compare against the latest stable baseline on every PR</li> <li>Review improvements too --- unexpected score increases can indicate test issues</li> </ol>"},{"location":"guides/safety-testing/","title":"Safety Testing","text":"<p>AgentProbe includes built-in safety test suites that probe agents for common vulnerabilities.</p>"},{"location":"guides/safety-testing/#overview","title":"Overview","text":"<p>The safety scanner runs a series of adversarial test cases against your agent and reports which attacks succeeded. This helps identify vulnerabilities before deployment.</p>"},{"location":"guides/safety-testing/#built-in-safety-suites","title":"Built-in Safety Suites","text":"Suite Description <code>prompt-injection</code> Tests for prompt injection attacks that attempt to override agent instructions <code>data-leakage</code> Tests for data exfiltration attempts that try to extract system prompts or internal data <code>jailbreak</code> Tests for jailbreak attempts that try to bypass safety guardrails <code>role-confusion</code> Tests for role confusion attacks that attempt to make the agent assume a different identity <code>hallucination</code> Tests for hallucination patterns where the agent fabricates information <code>tool-abuse</code> Tests for tool abuse scenarios where the agent misuses available tools"},{"location":"guides/safety-testing/#using-the-cli","title":"Using the CLI","text":""},{"location":"guides/safety-testing/#run-all-configured-suites","title":"Run All Configured Suites","text":"<pre><code>agentprobe safety scan\n</code></pre>"},{"location":"guides/safety-testing/#run-specific-suites","title":"Run Specific Suites","text":"<pre><code>agentprobe safety scan -s prompt-injection -s data-leakage\n</code></pre>"},{"location":"guides/safety-testing/#list-available-suites","title":"List Available Suites","text":"<pre><code>agentprobe safety list\n</code></pre>"},{"location":"guides/safety-testing/#configuration","title":"Configuration","text":"<p>Enable safety scanning in <code>agentprobe.yaml</code>:</p> <pre><code>safety:\n  enabled: true\n  suites:\n    - prompt-injection\n    - data-leakage\n    - jailbreak\n    - role-confusion\n    - hallucination\n    - tool-abuse\n</code></pre>"},{"location":"guides/safety-testing/#using-the-python-api","title":"Using the Python API","text":"<pre><code>from agentprobe.safety.scanner import SafetyScanner\n\n# Create scanner from config\nscanner = SafetyScanner.from_config([\n    \"prompt-injection\",\n    \"data-leakage\",\n    \"jailbreak\",\n])\n\n# Run scan\nresult = await scanner.scan(adapter)\n\nprint(f\"Total tests: {result.total_tests}\")\nprint(f\"Passed: {result.total_passed}\")\nprint(f\"Failed: {result.total_failed}\")\n\nfor suite_result in result.suite_results:\n    print(f\"  {suite_result.suite_name}: {suite_result.passed}/{suite_result.total_tests}\")\n</code></pre>"},{"location":"guides/safety-testing/#scan-results","title":"Scan Results","text":"<p>The <code>SafetyScanResult</code> contains aggregate results:</p> Field Type Description <code>total_suites</code> <code>int</code> Number of suites run <code>total_tests</code> <code>int</code> Total tests across all suites <code>total_passed</code> <code>int</code> Tests where no vulnerability was found <code>total_failed</code> <code>int</code> Tests where a vulnerability was detected <code>suite_results</code> <code>tuple[SafetySuiteResult, ...]</code> Per-suite breakdown"},{"location":"guides/safety-testing/#custom-safety-suites","title":"Custom Safety Suites","text":"<p>Register a custom suite by subclassing <code>SafetySuite</code>:</p> <pre><code>from agentprobe.safety.scanner import SafetySuite, SafetySuiteResult, register_suite\n\n@register_suite\nclass CustomSafetySuite(SafetySuite):\n    @property\n    def name(self) -&gt; str:\n        return \"custom-safety\"\n\n    async def run(self, adapter) -&gt; SafetySuiteResult:\n        results = []\n        # ... test logic ...\n        return SafetySuiteResult(\n            suite_name=self.name,\n            total_tests=len(results),\n            passed=sum(1 for r in results if r[\"passed\"]),\n            failed=sum(1 for r in results if not r[\"passed\"]),\n            results=tuple(results),\n        )\n</code></pre>"},{"location":"guides/safety-testing/#best-practices","title":"Best Practices","text":"<ol> <li>Run safety scans regularly as part of CI/CD</li> <li>Test all suites --- different suites catch different vulnerability classes</li> <li>Review failed tests carefully to understand attack vectors</li> <li>Add custom suites for domain-specific safety concerns</li> <li>Track safety metrics over time to catch regressions</li> </ol>"},{"location":"guides/writing-tests/","title":"Writing Tests","text":"<p>This guide covers how to write effective tests for your software agents using AgentProbe.</p>"},{"location":"guides/writing-tests/#pytest-plugin-recommended","title":"pytest Plugin (Recommended)","text":"<p>The fastest way to test agents is with the built-in pytest plugin. Install AgentProbe and the <code>agentprobe</code> fixture is automatically available in your tests --- no configuration needed.</p>"},{"location":"guides/writing-tests/#basic-test","title":"Basic Test","text":"<pre><code># test_my_agent.py\nfrom agentprobe.testing import assert_trace, assert_score, assert_cost\nfrom agentprobe.eval.rules import RuleBasedEvaluator, RuleSpec\n\nevaluator = RuleBasedEvaluator(rules=[\n    RuleSpec(rule_type=\"max_length\", params={\"max\": 3000}),\n    RuleSpec(rule_type=\"not_contains\", params={\"values\": [\"error\", \"fail\"]}),\n])\n\nasync def test_greeting(agentprobe):\n    trace = await agentprobe.invoke(\"Say hello\", adapter=my_adapter)\n\n    # Fluent trace assertions (raise AssertionError on failure)\n    assert_trace(trace).has_output().contains(\"hello\").not_contains(\"error\")\n\n    # Evaluator threshold check\n    result = await assert_score(trace, evaluator, min_score=0.8)\n\n    # Cost budget check\n    assert_cost(trace, max_usd=0.01)\n</code></pre> <p>Run with standard pytest:</p> <pre><code>pytest tests/ -v\n</code></pre>"},{"location":"guides/writing-tests/#the-agentprobe-fixture","title":"The <code>agentprobe</code> Fixture","text":"<p>The <code>agentprobe</code> fixture provides an <code>AgentProbeContext</code> with these methods:</p> Method Type Description <code>invoke(input_text, adapter, **kwargs)</code> async Invoke adapter, collect trace <code>evaluate(trace, evaluator, ...)</code> async Run evaluator on trace <code>calculate_cost(trace)</code> sync Calculate cost summary <code>traces</code> property All traces collected in this test <code>last_trace</code> property Most recent trace <p>The adapter is passed per-call, so you create it in your own fixture or at module level:</p> <pre><code>import pytest\n\n@pytest.fixture\ndef my_adapter():\n    return MyAgentAdapter(name=\"my-agent\", model=\"claude-sonnet-4-5-20250929\")\n\nasync def test_agent(agentprobe, my_adapter):\n    trace = await agentprobe.invoke(\"What is 2+2?\", adapter=my_adapter)\n    assert_trace(trace).has_output().has_tool(\"calculator\")\n</code></pre>"},{"location":"guides/writing-tests/#assert-helpers","title":"Assert Helpers","text":""},{"location":"guides/writing-tests/#assert_tracetrace","title":"<code>assert_trace(trace)</code>","text":"<p>Returns a <code>TraceAssertion</code> with chainable methods:</p> <pre><code>(\n    assert_trace(trace)\n    .has_output()                   # non-empty output\n    .contains(\"Paris\")              # substring check\n    .not_contains(\"error\")          # absence check\n    .matches(r\"\\d+ degrees\")        # regex match\n    .has_tool_calls(min_count=2)    # at least N tool calls\n    .has_tool(\"search\")             # specific tool was called\n    .has_llm_calls(min_count=1)     # at least N model calls\n    .output_length_less_than(5000)  # length limit\n    .output_is_valid_json()         # JSON parse check\n)\n</code></pre> <p>All methods raise <code>AssertionError</code> on failure --- pytest natively introspects these for rich failure output.</p>"},{"location":"guides/writing-tests/#await-assert_scoretrace-evaluator-min_score07","title":"<code>await assert_score(trace, evaluator, min_score=0.7)</code>","text":"<p>Runs the evaluator and asserts <code>score &gt;= min_score</code>. Returns the <code>EvalResult</code>.</p>"},{"location":"guides/writing-tests/#assert_costtrace-max_usd001","title":"<code>assert_cost(trace, max_usd=0.01)</code>","text":"<p>Calculates cost and asserts <code>total_cost_usd &lt;= max_usd</code>. Returns a <code>CostSummary</code>.</p>"},{"location":"guides/writing-tests/#plugin-options","title":"Plugin Options","text":"<pre><code>pytest --agentprobe-config path/to/agentprobe.yaml\npytest --agentprobe-trace-dir ./traces\npytest --agentprobe-store-traces  # persist traces to SQLite\n</code></pre> <p>Disable the plugin with <code>-p no:agentprobe</code>.</p>"},{"location":"guides/writing-tests/#scenario-based-tests","title":"Scenario-Based Tests","text":"<p>For the standalone AgentProbe runner, use the <code>@scenario</code> decorator:</p>"},{"location":"guides/writing-tests/#the-scenario-decorator","title":"The <code>@scenario</code> Decorator","text":"<pre><code>from agentprobe import scenario\n\n@scenario(\n    name=\"greeting_test\",\n    input_text=\"Say hello to the user\",\n    expected_output=\"Hello! How can I help you?\",\n    tags=[\"smoke\", \"greeting\"],\n    timeout=15.0,\n    evaluators=[\"rules\"],\n)\ndef test_greeting():\n    \"\"\"Agent should produce a friendly greeting.\"\"\"\n    pass\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>name</code> <code>str \\| None</code> Function name Test case identifier <code>input_text</code> <code>str</code> <code>\"\"</code> Input prompt for the agent <code>expected_output</code> <code>str \\| None</code> <code>None</code> Expected output for comparison <code>tags</code> <code>list[str] \\| None</code> <code>None</code> Tags for filtering and grouping <code>timeout</code> <code>float</code> <code>30.0</code> Maximum execution time in seconds <code>evaluators</code> <code>list[str] \\| None</code> <code>None</code> Evaluator names to apply"},{"location":"guides/writing-tests/#test-discovery","title":"Test Discovery","text":"<pre><code>agentprobe test                          # default test_dir\nagentprobe test -d tests/agent_tests/    # specific directory\nagentprobe test -p \"check_*.py\"          # custom pattern\n</code></pre>"},{"location":"guides/writing-tests/#fluent-assertions-legacy","title":"Fluent Assertions (Legacy)","text":"<p>The <code>expect()</code> / <code>expect_tool_calls()</code> API collects results without raising:</p> <pre><code>from agentprobe import expect, expect_tool_calls\n\npassed = (\n    expect(output)\n    .to_contain(\"result\")\n    .to_not_contain(\"error\")\n    .to_have_length_less_than(1000)\n    .all_passed()\n)\n\nassert expect_tool_calls(trace.tool_calls).to_contain(\"search\").all_passed()\n</code></pre> <p>For new tests, prefer <code>assert_trace()</code> which raises immediately on failure.</p>"},{"location":"guides/writing-tests/#multi-turn-conversations","title":"Multi-Turn Conversations","text":"<p>Use <code>ConversationRunner</code> for testing multi-turn dialogue:</p> <pre><code>from agentprobe import ConversationRunner\n\nrunner = ConversationRunner(adapter=my_adapter, evaluators=[my_evaluator])\nresult = await runner.run(\n    turns=[\"Hello\", \"Tell me about X\", \"Thanks, goodbye\"],\n    agent_name=\"support-agent\",\n)\n\nassert result.passed_turns == result.total_turns\n</code></pre>"},{"location":"guides/writing-tests/#running-tests","title":"Running Tests","text":""},{"location":"guides/writing-tests/#with-pytest-recommended","title":"With pytest (Recommended)","text":"<pre><code>pytest tests/ -v                        # standard run\npytest tests/ -v --agentprobe-store-traces  # persist traces\npytest tests/ -k \"test_greeting\"        # filter by name\npytest tests/ -m agentprobe             # only @pytest.mark.agentprobe tests\n</code></pre>"},{"location":"guides/writing-tests/#with-the-agentprobe-cli","title":"With the AgentProbe CLI","text":"<pre><code>agentprobe test -d tests/\nagentprobe test --parallel\nagentprobe test -c path/to/agentprobe.yaml\n</code></pre>"},{"location":"guides/writing-tests/#test-organization","title":"Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 test_greeting.py       # Basic interaction tests\n\u251c\u2500\u2500 test_search.py         # Search tool tests\n\u251c\u2500\u2500 test_calculation.py    # Math/calculation tests\n\u251c\u2500\u2500 test_error_handling.py # Edge cases and errors\n\u2514\u2500\u2500 test_multi_turn.py     # Conversation tests\n</code></pre>"},{"location":"plugins/creating-plugins/","title":"Creating Plugins","text":"<p>Step-by-step guide for building custom AgentProbe plugins.</p>"},{"location":"plugins/creating-plugins/#step-1-choose-a-plugin-type","title":"Step 1: Choose a Plugin Type","text":"<p>Decide what kind of plugin you need:</p> <ul> <li>EvaluatorPlugin --- Custom scoring logic for agent outputs</li> <li>AdapterPlugin --- Integration with a new agent framework</li> <li>ReporterPlugin --- Custom output format for test results</li> <li>StoragePlugin --- Custom backend for traces and results</li> </ul>"},{"location":"plugins/creating-plugins/#step-2-create-the-plugin-class","title":"Step 2: Create the Plugin Class","text":"<p>Every plugin extends one of the typed base classes. Here's an evaluator plugin example:</p> <pre><code>from agentprobe.plugins.base import EvaluatorPlugin, PluginType\n\n\nclass SentimentEvaluatorPlugin(EvaluatorPlugin):\n    \"\"\"Plugin that evaluates agent output sentiment.\"\"\"\n\n    @property\n    def name(self) -&gt; str:\n        return \"sentiment-evaluator\"\n\n    @property\n    def plugin_type(self) -&gt; PluginType:\n        return PluginType.EVALUATOR\n\n    @property\n    def version(self) -&gt; str:\n        return \"1.0.0\"\n\n    def on_load(self) -&gt; None:\n        print(f\"Loaded {self.name} v{self.version}\")\n\n    def create_evaluator(self):\n        from my_package.sentiment import SentimentEvaluator\n        return SentimentEvaluator()\n</code></pre>"},{"location":"plugins/creating-plugins/#step-3-implement-lifecycle-hooks","title":"Step 3: Implement Lifecycle Hooks","text":"<p>Override any lifecycle hooks you need:</p> <pre><code>class MyPlugin(EvaluatorPlugin):\n    def on_load(self) -&gt; None:\n        \"\"\"Initialize resources when plugin loads.\"\"\"\n        self._client = create_client()\n\n    def on_unload(self) -&gt; None:\n        \"\"\"Clean up resources when plugin unloads.\"\"\"\n        self._client.close()\n\n    def on_test_start(self, test_name: str, **kwargs) -&gt; None:\n        \"\"\"Called before each test.\"\"\"\n        self._start_time = time.time()\n\n    def on_test_end(self, test_name: str, **kwargs) -&gt; None:\n        \"\"\"Called after each test.\"\"\"\n        elapsed = time.time() - self._start_time\n        print(f\"Test {test_name} took {elapsed:.2f}s\")\n\n    def on_suite_start(self, **kwargs) -&gt; None:\n        \"\"\"Called before the test suite.\"\"\"\n        pass\n\n    def on_suite_end(self, **kwargs) -&gt; None:\n        \"\"\"Called after the test suite.\"\"\"\n        pass\n</code></pre> <p>All hooks have default no-op implementations, so you only need to override the ones you use.</p>"},{"location":"plugins/creating-plugins/#step-4-register-the-plugin","title":"Step 4: Register the Plugin","text":""},{"location":"plugins/creating-plugins/#option-a-entry-points-recommended","title":"Option A: Entry Points (Recommended)","text":"<p>Add an entry point in your <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"agentprobe.plugins\"]\nsentiment = \"my_package.plugins:SentimentEvaluatorPlugin\"\n</code></pre> <p>Install your package and AgentProbe will discover it automatically.</p>"},{"location":"plugins/creating-plugins/#option-b-file-based","title":"Option B: File-Based","text":"<p>Place your plugin module in a directory and configure AgentProbe:</p> <pre><code>plugins:\n  enabled: true\n  directories:\n    - plugins/\n</code></pre>"},{"location":"plugins/creating-plugins/#option-c-programmatic","title":"Option C: Programmatic","text":"<p>Register plugins directly in code:</p> <pre><code>from agentprobe import PluginManager\n\nmanager = PluginManager()\nmanager.load_plugins(classes=[SentimentEvaluatorPlugin])\n</code></pre>"},{"location":"plugins/creating-plugins/#step-5-use-the-plugin","title":"Step 5: Use the Plugin","text":"<p>Once registered, the plugin's artifacts are available through the <code>PluginManager</code>:</p> <pre><code>manager = PluginManager()\nmanager.load_plugins()\n\n# Get all evaluators from plugins\nevaluators = manager.get_evaluators()\n\n# Dispatch lifecycle events\nmanager.dispatch_suite_start()\nmanager.dispatch_test_start(\"my_test\")\nmanager.dispatch_test_end(\"my_test\")\nmanager.dispatch_suite_end()\n\n# Clean up\nmanager.unload_all()\n</code></pre>"},{"location":"plugins/creating-plugins/#testing-your-plugin","title":"Testing Your Plugin","text":"<pre><code>import pytest\nfrom my_package.plugins import SentimentEvaluatorPlugin\n\n\nclass TestSentimentPlugin:\n    def test_name(self) -&gt; None:\n        plugin = SentimentEvaluatorPlugin()\n        assert plugin.name == \"sentiment-evaluator\"\n\n    def test_creates_evaluator(self) -&gt; None:\n        plugin = SentimentEvaluatorPlugin()\n        evaluator = plugin.create_evaluator()\n        assert evaluator is not None\n\n    def test_lifecycle(self) -&gt; None:\n        plugin = SentimentEvaluatorPlugin()\n        plugin.on_load()\n        plugin.on_test_start(\"test_1\")\n        plugin.on_test_end(\"test_1\")\n        plugin.on_unload()\n</code></pre>"},{"location":"plugins/overview/","title":"Plugins Overview","text":"<p>AgentProbe supports a plugin system for extending functionality with custom evaluators, reporters, adapters, and storage backends.</p>"},{"location":"plugins/overview/#architecture","title":"Architecture","text":"<p>The plugin system has three main components:</p> <ol> <li>PluginBase --- Abstract base class that all plugins extend</li> <li>PluginRegistry --- Manages plugin registration and lookup</li> <li>PluginManager --- Orchestrates plugin lifecycle and event dispatch</li> </ol> <pre><code>PluginManager\n\u251c\u2500\u2500 PluginRegistry (stores registered plugins)\n\u251c\u2500\u2500 PluginLoader (discovers plugins from entry points and directories)\n\u2514\u2500\u2500 Event Dispatch (test_start, test_end, suite_start, suite_end)\n</code></pre>"},{"location":"plugins/overview/#plugin-types","title":"Plugin Types","text":"<p>AgentProbe supports four plugin types:</p> Type Base Class Purpose Evaluator <code>EvaluatorPlugin</code> Custom evaluation logic Adapter <code>AdapterPlugin</code> Custom agent framework integration Reporter <code>ReporterPlugin</code> Custom output formats Storage <code>StoragePlugin</code> Custom storage backends <p>See Plugin Types for detailed API reference.</p>"},{"location":"plugins/overview/#plugin-discovery","title":"Plugin Discovery","text":"<p>Plugins are discovered from two sources:</p>"},{"location":"plugins/overview/#entry-points","title":"Entry Points","text":"<p>Register plugins in your package's <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"agentprobe.plugins\"]\nmy-evaluator = \"my_package.evaluator:MyEvaluatorPlugin\"\n</code></pre>"},{"location":"plugins/overview/#file-based-discovery","title":"File-Based Discovery","text":"<p>Point AgentProbe to directories containing plugin modules:</p> <pre><code>plugins:\n  enabled: true\n  directories:\n    - plugins/\n    - /path/to/shared/plugins/\n</code></pre>"},{"location":"plugins/overview/#plugin-lifecycle","title":"Plugin Lifecycle","text":"<p>Plugins receive lifecycle events:</p> <ol> <li><code>on_load()</code> --- Called when the plugin is loaded and registered</li> <li><code>on_test_start(test_name)</code> --- Called before each test execution</li> <li><code>on_test_end(test_name)</code> --- Called after each test execution</li> <li><code>on_suite_start()</code> --- Called before the test suite begins</li> <li><code>on_suite_end()</code> --- Called after the test suite completes</li> <li><code>on_unload()</code> --- Called when the plugin is unloaded</li> </ol>"},{"location":"plugins/overview/#error-isolation","title":"Error Isolation","text":"<p>The <code>PluginManager</code> catches exceptions from individual plugins during event dispatch. A failing plugin does not affect other plugins or the test execution.</p>"},{"location":"plugins/overview/#configuration","title":"Configuration","text":"<pre><code>plugins:\n  enabled: true\n  directories: []\n  entry_point_group: agentprobe.plugins\n</code></pre> Key Type Default Description <code>enabled</code> <code>bool</code> <code>true</code> Enable the plugin system <code>directories</code> <code>list[str]</code> <code>[]</code> Additional plugin directories <code>entry_point_group</code> <code>str</code> <code>\"agentprobe.plugins\"</code> Entry point group name"},{"location":"plugins/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Creating Plugins --- Step-by-step guide</li> <li>Plugin Types --- API reference for each type</li> </ul>"},{"location":"plugins/plugin-types/","title":"Plugin Types","text":"<p>Reference for all supported plugin types in AgentProbe.</p>"},{"location":"plugins/plugin-types/#base-class-pluginbase","title":"Base Class: <code>PluginBase</code>","text":"<p>All plugins extend <code>PluginBase</code>, which defines the common interface:</p> <pre><code>from agentprobe.plugins.base import PluginBase\n\nclass PluginBase(ABC):\n    @property\n    @abstractmethod\n    def name(self) -&gt; str: ...\n\n    @property\n    @abstractmethod\n    def plugin_type(self) -&gt; PluginType: ...\n\n    @property\n    def version(self) -&gt; str:\n        return \"0.1.0\"\n\n    def on_load(self) -&gt; None: ...\n    def on_unload(self) -&gt; None: ...\n    def on_test_start(self, test_name: str, **kwargs) -&gt; None: ...\n    def on_test_end(self, test_name: str, **kwargs) -&gt; None: ...\n    def on_suite_start(self, **kwargs) -&gt; None: ...\n    def on_suite_end(self, **kwargs) -&gt; None: ...\n</code></pre>"},{"location":"plugins/plugin-types/#plugintype-enum","title":"<code>PluginType</code> Enum","text":"<pre><code>class PluginType(StrEnum):\n    EVALUATOR = \"evaluator\"\n    ADAPTER = \"adapter\"\n    REPORTER = \"reporter\"\n    STORAGE = \"storage\"\n</code></pre>"},{"location":"plugins/plugin-types/#evaluatorplugin","title":"EvaluatorPlugin","text":"<p>Provides a custom evaluator for scoring agent outputs.</p> <pre><code>from agentprobe.plugins.base import EvaluatorPlugin, PluginType\n\nclass MyEvaluatorPlugin(EvaluatorPlugin):\n    @property\n    def name(self) -&gt; str:\n        return \"my-evaluator\"\n\n    @property\n    def plugin_type(self) -&gt; PluginType:\n        return PluginType.EVALUATOR\n\n    def create_evaluator(self):\n        \"\"\"Create and return an evaluator instance.\n\n        The returned object must implement the evaluator protocol:\n        - name: str (property)\n        - evaluate(test_case, trace) -&gt; EvalResult (async method)\n        \"\"\"\n        return MyCustomEvaluator()\n</code></pre> <p>Required method:</p> Method Returns Description <code>create_evaluator()</code> <code>EvaluatorProtocol</code> Factory for custom evaluator"},{"location":"plugins/plugin-types/#adapterplugin","title":"AdapterPlugin","text":"<p>Provides a custom adapter for integrating with agent frameworks.</p> <pre><code>from agentprobe.plugins.base import AdapterPlugin, PluginType\n\nclass MyAdapterPlugin(AdapterPlugin):\n    @property\n    def name(self) -&gt; str:\n        return \"my-adapter\"\n\n    @property\n    def plugin_type(self) -&gt; PluginType:\n        return PluginType.ADAPTER\n\n    def create_adapter(self):\n        \"\"\"Create and return an adapter instance.\n\n        The returned object must implement the adapter protocol:\n        - name: str (property)\n        - invoke(input_text, **kwargs) -&gt; Trace (async method)\n        \"\"\"\n        return MyCustomAdapter()\n</code></pre> <p>Required method:</p> Method Returns Description <code>create_adapter()</code> <code>AdapterProtocol</code> Factory for custom adapter"},{"location":"plugins/plugin-types/#reporterplugin","title":"ReporterPlugin","text":"<p>Provides a custom reporter for formatting test results.</p> <pre><code>from agentprobe.plugins.base import ReporterPlugin, PluginType\n\nclass MyReporterPlugin(ReporterPlugin):\n    @property\n    def name(self) -&gt; str:\n        return \"my-reporter\"\n\n    @property\n    def plugin_type(self) -&gt; PluginType:\n        return PluginType.REPORTER\n\n    def create_reporter(self):\n        \"\"\"Create and return a reporter instance.\n\n        The returned object must implement the reporter protocol:\n        - name: str (property)\n        - generate(run) -&gt; str (method)\n        \"\"\"\n        return MyCustomReporter()\n</code></pre> <p>Required method:</p> Method Returns Description <code>create_reporter()</code> <code>ReporterProtocol</code> Factory for custom reporter"},{"location":"plugins/plugin-types/#storageplugin","title":"StoragePlugin","text":"<p>Provides a custom storage backend for traces and results.</p> <pre><code>from agentprobe.plugins.base import StoragePlugin, PluginType\n\nclass MyStoragePlugin(StoragePlugin):\n    @property\n    def name(self) -&gt; str:\n        return \"my-storage\"\n\n    @property\n    def plugin_type(self) -&gt; PluginType:\n        return PluginType.STORAGE\n\n    def create_storage(self):\n        \"\"\"Create and return a storage instance.\n\n        The returned object must implement the storage protocol:\n        - setup() (async method)\n        - save_trace(trace) (async method)\n        - load_trace(trace_id) -&gt; Trace | None (async method)\n        - list_traces(**filters) -&gt; list[Trace] (async method)\n        - close() (async method)\n        \"\"\"\n        return MyCustomStorage()\n</code></pre> <p>Required method:</p> Method Returns Description <code>create_storage()</code> <code>StorageProtocol</code> Factory for custom backend"},{"location":"plugins/plugin-types/#collecting-plugin-artifacts","title":"Collecting Plugin Artifacts","text":"<p>The <code>PluginManager</code> provides methods to collect artifacts from all loaded plugins:</p> <pre><code>manager = PluginManager()\nmanager.load_plugins()\n\nevaluators = manager.get_evaluators()      # From all EvaluatorPlugins\nadapters = manager.get_adapters()          # From all AdapterPlugins\nreporters = manager.get_reporters()        # From all ReporterPlugins\nstorage_backends = manager.get_storage_backends()  # From all StoragePlugins\n</code></pre>"},{"location":"reference/adapters/","title":"Adapters Reference","text":"<p>Adapters connect AgentProbe to agent frameworks. Each adapter wraps a specific framework's agent, invokes it with test inputs, and returns a structured <code>Trace</code>.</p>"},{"location":"reference/adapters/#adapter-interface","title":"Adapter Interface","text":"<p>All adapters implement the same interface:</p> <pre><code>class BaseAdapter:\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the adapter name.\"\"\"\n        ...\n\n    async def invoke(self, input_text: str, **kwargs) -&gt; Trace:\n        \"\"\"Invoke the agent and return a trace.\"\"\"\n        ...\n</code></pre> <p>The <code>invoke</code> method handles trace recording internally using a <code>_TraceBuilder</code>:</p> <ol> <li>Create a <code>_TraceBuilder</code> via <code>_create_builder()</code></li> <li>Record LLM calls and tool calls during execution</li> <li>Build and return a frozen <code>Trace</code></li> </ol>"},{"location":"reference/adapters/#built-in-adapters","title":"Built-in Adapters","text":""},{"location":"reference/adapters/#langchain","title":"LangChain","text":"<pre><code>from agentprobe.adapters.langchain import LangChainAdapter\n</code></pre> <p>Wraps LangChain agents and chains. Records all LLM calls and tool invocations captured by LangChain's callback system.</p> <p>Usage:</p> <pre><code>adapter = LangChainAdapter(name=\"my-langchain-agent\")\ntrace = await adapter.invoke(\"What is the weather?\")\n</code></pre>"},{"location":"reference/adapters/#crewai","title":"CrewAI","text":"<pre><code>from agentprobe.adapters.crewai import CrewAIAdapter\n</code></pre> <p>Wraps CrewAI crews and agents. Captures crew execution including agent delegation and tool usage.</p>"},{"location":"reference/adapters/#autogen","title":"AutoGen","text":"<pre><code>from agentprobe.adapters.autogen import AutoGenAdapter\n</code></pre> <p>Wraps AutoGen agent conversations. Records messages between agents and tool calls.</p>"},{"location":"reference/adapters/#mcp-model-context-protocol","title":"MCP (Model Context Protocol)","text":"<pre><code>from agentprobe.adapters.mcp import MCPAdapter\n</code></pre> <p>Wraps MCP-compatible servers. Records tool calls made through the MCP protocol.</p>"},{"location":"reference/adapters/#creating-custom-adapters","title":"Creating Custom Adapters","text":"<p>Subclass <code>BaseAdapter</code> and implement the <code>_invoke</code> method:</p> <pre><code>from agentprobe.adapters.base import BaseAdapter\nfrom agentprobe.core.models import LLMCall, Trace\n\nclass MyAdapter(BaseAdapter):\n    def __init__(self, agent, name: str = \"my-adapter\") -&gt; None:\n        super().__init__(name)\n        self._agent = agent\n\n    async def _invoke(self, input_text: str, **kwargs) -&gt; Trace:\n        builder = self._create_builder(model=\"my-model\")\n        builder.input_text = input_text\n\n        # Call your agent\n        response = await self._agent.run(input_text)\n\n        # Record the LLM call\n        builder.add_llm_call(LLMCall(\n            model=\"my-model\",\n            input_text=input_text,\n            output_text=response.text,\n            input_tokens=response.input_tokens,\n            output_tokens=response.output_tokens,\n            latency_ms=response.latency_ms,\n        ))\n\n        builder.output_text = response.text\n        return builder.build()\n</code></pre>"},{"location":"reference/adapters/#using-the-trace-builder","title":"Using the Trace Builder","text":"<p>The <code>_TraceBuilder</code> provides methods for recording execution events:</p> Method Description <code>add_llm_call(call)</code> Record a model call with tokens and latency <code>add_tool_call(call)</code> Record a tool invocation with input/output <code>build()</code> Produce a frozen, immutable <code>Trace</code> <p>Set <code>input_text</code>, <code>output_text</code>, <code>tags</code>, and <code>metadata</code> as attributes on the builder before calling <code>build()</code>.</p>"},{"location":"reference/adapters/#registering-adapters-via-plugins","title":"Registering Adapters via Plugins","text":"<p>Adapters can be distributed as plugins. See Plugin Types for the <code>AdapterPlugin</code> interface.</p>"},{"location":"reference/cli/","title":"CLI Reference","text":"<p>Complete reference for all AgentProbe command-line commands.</p>"},{"location":"reference/cli/#global-options","title":"Global Options","text":"<pre><code>agentprobe --version    # Show version\nagentprobe --help       # Show help\n</code></pre>"},{"location":"reference/cli/#agentprobe-init","title":"<code>agentprobe init</code>","text":"<p>Initialize a new AgentProbe configuration file.</p> <pre><code>agentprobe init [OPTIONS]\n</code></pre> Option Short Default Description <code>--output</code> <code>-o</code> <code>agentprobe.yaml</code> Output file path <p>Example:</p> <pre><code>agentprobe init\nagentprobe init -o config/agentprobe.yaml\n</code></pre>"},{"location":"reference/cli/#agentprobe-test","title":"<code>agentprobe test</code>","text":"<p>Discover and run agent test scenarios.</p> <pre><code>agentprobe test [OPTIONS]\n</code></pre> Option Short Default Description <code>--config</code> <code>-c</code> Auto-detect Path to config file <code>--test-dir</code> <code>-d</code> From config Directory containing test files <code>--pattern</code> <code>-p</code> <code>test_*.py</code> Glob pattern for test files <code>--parallel</code> Run tests in parallel <code>--sequential</code> Run tests sequentially <p>Examples:</p> <pre><code>agentprobe test\nagentprobe test -d tests/agent_tests/\nagentprobe test -p \"check_*.py\"\nagentprobe test --parallel\nagentprobe test -c custom-config.yaml -d tests/ --parallel\n</code></pre>"},{"location":"reference/cli/#agentprobe-trace","title":"<code>agentprobe trace</code>","text":"<p>Inspect and manage execution traces.</p>"},{"location":"reference/cli/#agentprobe-trace-list","title":"<code>agentprobe trace list</code>","text":"<p>List recorded traces.</p> <pre><code>agentprobe trace list [OPTIONS]\n</code></pre> Option Short Default Description <code>--config</code> <code>-c</code> Auto-detect Path to config file <code>--agent</code> <code>-a</code> All agents Filter by agent name <code>--limit</code> <code>-n</code> <code>20</code> Maximum traces to show <p>Examples:</p> <pre><code>agentprobe trace list\nagentprobe trace list -a my-agent -n 50\n</code></pre>"},{"location":"reference/cli/#agentprobe-trace-show","title":"<code>agentprobe trace show</code>","text":"<p>Show details for a specific trace.</p> <pre><code>agentprobe trace show TRACE_ID [OPTIONS]\n</code></pre> Argument Description <code>TRACE_ID</code> The trace identifier Option Short Default Description <code>--config</code> <code>-c</code> Auto-detect Path to config file <p>Example:</p> <pre><code>agentprobe trace show abc12345\n</code></pre> <p>Output includes: trace ID, agent name, model, input/output text, token counts, latency, tags, timestamps, and tool call details.</p>"},{"location":"reference/cli/#agentprobe-safety","title":"<code>agentprobe safety</code>","text":"<p>Run safety scans against agents.</p>"},{"location":"reference/cli/#agentprobe-safety-scan","title":"<code>agentprobe safety scan</code>","text":"<p>Run safety test suites.</p> <pre><code>agentprobe safety scan [OPTIONS]\n</code></pre> Option Short Default Description <code>--suite</code> <code>-s</code> All configured Suite names (repeatable) <code>--severity</code> All Filter by severity level <p>Examples:</p> <pre><code>agentprobe safety scan\nagentprobe safety scan -s prompt-injection -s data-leakage\n</code></pre>"},{"location":"reference/cli/#agentprobe-safety-list","title":"<code>agentprobe safety list</code>","text":"<p>List available safety test suites.</p> <pre><code>agentprobe safety list\n</code></pre>"},{"location":"reference/cli/#agentprobe-cost","title":"<code>agentprobe cost</code>","text":"<p>Track and manage execution costs.</p>"},{"location":"reference/cli/#agentprobe-cost-report","title":"<code>agentprobe cost report</code>","text":"<p>Generate a cost report.</p> <pre><code>agentprobe cost report [OPTIONS]\n</code></pre> Option Short Default Description <code>--agent</code> <code>-a</code> All Filter by agent name <code>--format</code> <code>-f</code> <code>terminal</code> Output format"},{"location":"reference/cli/#agentprobe-cost-budget","title":"<code>agentprobe cost budget</code>","text":"<p>Check budget status.</p> <pre><code>agentprobe cost budget [OPTIONS]\n</code></pre> Option Short Default Description <code>--max-cost</code> From config Maximum cost in USD <code>--max-tokens</code> None Maximum token count"},{"location":"reference/cli/#agentprobe-baseline","title":"<code>agentprobe baseline</code>","text":"<p>Manage regression testing baselines.</p>"},{"location":"reference/cli/#agentprobe-baseline-list","title":"<code>agentprobe baseline list</code>","text":"<p>List all saved baselines.</p> <pre><code>agentprobe baseline list [OPTIONS]\n</code></pre> Option Short Default Description <code>--dir</code> <code>-d</code> <code>.agentprobe/baselines</code> Baselines directory"},{"location":"reference/cli/#agentprobe-baseline-save","title":"<code>agentprobe baseline save</code>","text":"<p>Create a new baseline from the latest test run.</p> <pre><code>agentprobe baseline save NAME [OPTIONS]\n</code></pre> Option Short Default Description <code>--dir</code> <code>-d</code> <code>.agentprobe/baselines</code> Baselines directory"},{"location":"reference/cli/#agentprobe-baseline-delete","title":"<code>agentprobe baseline delete</code>","text":"<p>Delete a saved baseline.</p> <pre><code>agentprobe baseline delete NAME [OPTIONS]\n</code></pre> Option Short Default Description <code>--dir</code> <code>-d</code> <code>.agentprobe/baselines</code> Baselines directory"},{"location":"reference/cli/#agentprobe-snapshot","title":"<code>agentprobe snapshot</code>","text":"<p>Manage trace snapshots for golden-file testing.</p>"},{"location":"reference/cli/#agentprobe-snapshot-list","title":"<code>agentprobe snapshot list</code>","text":"<p>List all saved snapshots.</p> <pre><code>agentprobe snapshot list [OPTIONS]\n</code></pre> Option Short Default Description <code>--dir</code> <code>-d</code> <code>.agentprobe/snapshots</code> Snapshots directory"},{"location":"reference/cli/#agentprobe-snapshot-delete","title":"<code>agentprobe snapshot delete</code>","text":"<p>Delete a saved snapshot.</p> <pre><code>agentprobe snapshot delete NAME [OPTIONS]\n</code></pre>"},{"location":"reference/cli/#agentprobe-snapshot-diff","title":"<code>agentprobe snapshot diff</code>","text":"<p>Show diff information for a snapshot.</p> <pre><code>agentprobe snapshot diff NAME [OPTIONS]\n</code></pre>"},{"location":"reference/cli/#agentprobe-metrics","title":"<code>agentprobe metrics</code>","text":"<p>View metric data and summaries.</p>"},{"location":"reference/cli/#agentprobe-metrics-list","title":"<code>agentprobe metrics list</code>","text":"<p>List available metrics.</p> <pre><code>agentprobe metrics list\n</code></pre>"},{"location":"reference/cli/#agentprobe-metrics-summary","title":"<code>agentprobe metrics summary</code>","text":"<p>Show metric summary statistics.</p> <pre><code>agentprobe metrics summary [OPTIONS]\n</code></pre>"},{"location":"reference/config/","title":"Configuration Reference","text":"<p>Full reference for <code>agentprobe.yaml</code> configuration options.</p> <p>For a quick overview, see Getting Started &gt; Configuration.</p>"},{"location":"reference/config/#file-format","title":"File Format","text":"<p>AgentProbe configuration is a YAML file with the following top-level keys:</p> <pre><code>project_name: my-project\ntest_dir: tests\nrunner: { ... }\neval: { ... }\njudge: { ... }\ntrace: { ... }\ncost: { ... }\nsafety: { ... }\nchaos: { ... }\nsnapshot: { ... }\nbudget: { ... }\nregression: { ... }\nmetrics: { ... }\nplugins: { ... }\nreporting: { ... }\n</code></pre> <p>All sections are optional. Unknown keys cause validation errors (<code>extra=\"forbid\"</code>).</p>"},{"location":"reference/config/#environment-variable-interpolation","title":"Environment Variable Interpolation","text":"<p>Use <code>${VAR_NAME}</code> to reference environment variables anywhere in the config:</p> <pre><code>trace:\n  database_path: ${AGENTPROBE_DB_PATH}\n</code></pre>"},{"location":"reference/config/#section-reference","title":"Section Reference","text":""},{"location":"reference/config/#project_name","title":"<code>project_name</code>","text":"<ul> <li>Type: <code>str</code></li> <li>Default: <code>\"agentprobe\"</code></li> <li>Name of the project being tested.</li> </ul>"},{"location":"reference/config/#test_dir","title":"<code>test_dir</code>","text":"<ul> <li>Type: <code>str</code></li> <li>Default: <code>\"tests\"</code></li> <li>Root directory containing test files.</li> </ul>"},{"location":"reference/config/#runner","title":"<code>runner</code>","text":"Key Type Default Constraints Description <code>parallel</code> <code>bool</code> <code>false</code> Run tests concurrently <code>max_workers</code> <code>int</code> <code>4</code> &gt;= 1 Max concurrent tests <code>default_timeout</code> <code>float</code> <code>30.0</code> &gt; 0 Timeout per test (seconds)"},{"location":"reference/config/#eval","title":"<code>eval</code>","text":"Key Type Default Description <code>default_evaluators</code> <code>list[str]</code> <code>[]</code> Evaluator names for all tests"},{"location":"reference/config/#judge","title":"<code>judge</code>","text":"Key Type Default Constraints Description <code>model</code> <code>str</code> <code>\"claude-sonnet-4-5-20250929\"</code> Judge model ID <code>provider</code> <code>str</code> <code>\"anthropic\"</code> API provider <code>temperature</code> <code>float</code> <code>0.0</code> 0.0--2.0 Sampling temperature <code>max_tokens</code> <code>int</code> <code>1024</code> &gt;= 1 Max response tokens"},{"location":"reference/config/#trace","title":"<code>trace</code>","text":"Key Type Default Description <code>enabled</code> <code>bool</code> <code>true</code> Enable trace recording <code>storage_backend</code> <code>str</code> <code>\"sqlite\"</code> <code>sqlite</code> or <code>postgresql</code> <code>database_path</code> <code>str</code> <code>\".agentprobe/traces.db\"</code> DB path or DSN"},{"location":"reference/config/#cost","title":"<code>cost</code>","text":"Key Type Default Description <code>enabled</code> <code>bool</code> <code>true</code> Enable cost tracking <code>budget_limit_usd</code> <code>float \\| null</code> <code>null</code> Max cost per run <code>pricing_dir</code> <code>str \\| null</code> <code>null</code> Custom pricing YAML dir"},{"location":"reference/config/#safety","title":"<code>safety</code>","text":"Key Type Default Description <code>enabled</code> <code>bool</code> <code>false</code> Enable safety testing <code>suites</code> <code>list[str]</code> <code>[]</code> Suite names to run"},{"location":"reference/config/#chaos","title":"<code>chaos</code>","text":"Key Type Default Constraints Description <code>enabled</code> <code>bool</code> <code>false</code> Enable chaos testing <code>seed</code> <code>int</code> <code>42</code> Random seed <code>default_probability</code> <code>float</code> <code>0.5</code> 0.0--1.0 Fault probability"},{"location":"reference/config/#snapshot","title":"<code>snapshot</code>","text":"Key Type Default Constraints Description <code>enabled</code> <code>bool</code> <code>false</code> Enable snapshots <code>snapshot_dir</code> <code>str</code> <code>\".agentprobe/snapshots\"</code> Snapshot directory <code>update_on_first_run</code> <code>bool</code> <code>true</code> Auto-create on first run <code>threshold</code> <code>float</code> <code>0.8</code> 0.0--1.0 Similarity threshold"},{"location":"reference/config/#budget","title":"<code>budget</code>","text":"Key Type Default Description <code>test_budget_usd</code> <code>float \\| null</code> <code>null</code> Max cost per test <code>suite_budget_usd</code> <code>float \\| null</code> <code>null</code> Max cost per suite"},{"location":"reference/config/#regression","title":"<code>regression</code>","text":"Key Type Default Constraints Description <code>enabled</code> <code>bool</code> <code>false</code> Enable regression detection <code>baseline_dir</code> <code>str</code> <code>\".agentprobe/baselines\"</code> Baseline directory <code>threshold</code> <code>float</code> <code>0.05</code> 0.0--1.0 Score delta threshold"},{"location":"reference/config/#metrics","title":"<code>metrics</code>","text":"Key Type Default Constraints Description <code>enabled</code> <code>bool</code> <code>true</code> Enable metric collection <code>builtin_metrics</code> <code>bool</code> <code>true</code> Collect built-in metrics <code>trend_window</code> <code>int</code> <code>10</code> &gt;= 2 Runs for trend analysis"},{"location":"reference/config/#plugins","title":"<code>plugins</code>","text":"Key Type Default Description <code>enabled</code> <code>bool</code> <code>true</code> Enable plugin system <code>directories</code> <code>list[str]</code> <code>[]</code> Additional plugin dirs <code>entry_point_group</code> <code>str</code> <code>\"agentprobe.plugins\"</code> Entry point group"},{"location":"reference/config/#reporting","title":"<code>reporting</code>","text":"Key Type Default Description <code>formats</code> <code>list[str]</code> <code>[\"terminal\"]</code> Output format names <code>output_dir</code> <code>str</code> <code>\"agentprobe-report\"</code> Report output directory <p>Available formats: <code>terminal</code>, <code>json</code>, <code>junit</code>, <code>html</code>, <code>markdown</code>, <code>csv</code></p>"},{"location":"reference/api/adapters/","title":"Adapters","text":"<p>Framework adapters for integrating with agent frameworks.</p>"},{"location":"reference/api/adapters/#base-adapter","title":"Base Adapter","text":""},{"location":"reference/api/adapters/#agentprobe.adapters.base","title":"<code>agentprobe.adapters.base</code>","text":"<p>Abstract base adapter with trace-building helper.</p> <p>Subclasses implement <code>_invoke()</code> while the base class provides a <code>_TraceBuilder</code> for accumulating mutable state before producing a frozen <code>Trace</code>.</p>"},{"location":"reference/api/adapters/#agentprobe.adapters.base.BaseAdapter","title":"<code>BaseAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for agent framework adapters.</p> <p>Provides a public <code>invoke()</code> method that wraps <code>_invoke()</code> with error handling and logging.</p> <p>Attributes:</p> Name Type Description <code>_name</code> <p>The adapter's name.</p> Source code in <code>src/agentprobe/adapters/base.py</code> <pre><code>class BaseAdapter(ABC):\n    \"\"\"Abstract base class for agent framework adapters.\n\n    Provides a public ``invoke()`` method that wraps ``_invoke()``\n    with error handling and logging.\n\n    Attributes:\n        _name: The adapter's name.\n    \"\"\"\n\n    def __init__(self, name: str) -&gt; None:\n        \"\"\"Initialize the adapter.\n\n        Args:\n            name: A unique name identifying this adapter instance.\n        \"\"\"\n        self._name = name\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the adapter name.\"\"\"\n        return self._name\n\n    def _create_builder(self, model: str | None = None) -&gt; _TraceBuilder:\n        \"\"\"Create a new trace builder for this adapter.\n\n        Args:\n            model: Optional primary model name.\n\n        Returns:\n            A mutable trace builder instance.\n        \"\"\"\n        return _TraceBuilder(agent_name=self._name, model=model)\n\n    async def invoke(self, input_text: str, **kwargs: Any) -&gt; Trace:\n        \"\"\"Invoke the agent and return a trace.\n\n        Wraps ``_invoke()`` with error handling and logging.\n\n        Args:\n            input_text: The input prompt to send to the agent.\n            **kwargs: Additional adapter-specific arguments.\n\n        Returns:\n            A complete execution trace.\n\n        Raises:\n            AdapterError: If the agent invocation fails.\n        \"\"\"\n        logger.info(\"Invoking adapter '%s' with input length %d\", self._name, len(input_text))\n        try:\n            trace = await self._invoke(input_text, **kwargs)\n        except AdapterError:\n            raise\n        except Exception as exc:\n            raise AdapterError(self._name, str(exc)) from exc\n        else:\n            logger.info(\n                \"Adapter '%s' completed: %d LLM calls, %d tool calls\",\n                self._name,\n                len(trace.llm_calls),\n                len(trace.tool_calls),\n            )\n            return trace\n\n    @abstractmethod\n    async def _invoke(self, input_text: str, **kwargs: Any) -&gt; Trace:\n        \"\"\"Perform the actual agent invocation.\n\n        Subclasses must implement this method.\n\n        Args:\n            input_text: The input prompt to send to the agent.\n            **kwargs: Additional adapter-specific arguments.\n\n        Returns:\n            A complete execution trace.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/api/adapters/#agentprobe.adapters.base.BaseAdapter.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the adapter name.</p>"},{"location":"reference/api/adapters/#agentprobe.adapters.base.BaseAdapter.__init__","title":"<code>__init__(name)</code>","text":"<p>Initialize the adapter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A unique name identifying this adapter instance.</p> required Source code in <code>src/agentprobe/adapters/base.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    \"\"\"Initialize the adapter.\n\n    Args:\n        name: A unique name identifying this adapter instance.\n    \"\"\"\n    self._name = name\n</code></pre>"},{"location":"reference/api/adapters/#agentprobe.adapters.base.BaseAdapter.invoke","title":"<code>invoke(input_text, **kwargs)</code>  <code>async</code>","text":"<p>Invoke the agent and return a trace.</p> <p>Wraps <code>_invoke()</code> with error handling and logging.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The input prompt to send to the agent.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Trace</code> <p>A complete execution trace.</p> <p>Raises:</p> Type Description <code>AdapterError</code> <p>If the agent invocation fails.</p> Source code in <code>src/agentprobe/adapters/base.py</code> <pre><code>async def invoke(self, input_text: str, **kwargs: Any) -&gt; Trace:\n    \"\"\"Invoke the agent and return a trace.\n\n    Wraps ``_invoke()`` with error handling and logging.\n\n    Args:\n        input_text: The input prompt to send to the agent.\n        **kwargs: Additional adapter-specific arguments.\n\n    Returns:\n        A complete execution trace.\n\n    Raises:\n        AdapterError: If the agent invocation fails.\n    \"\"\"\n    logger.info(\"Invoking adapter '%s' with input length %d\", self._name, len(input_text))\n    try:\n        trace = await self._invoke(input_text, **kwargs)\n    except AdapterError:\n        raise\n    except Exception as exc:\n        raise AdapterError(self._name, str(exc)) from exc\n    else:\n        logger.info(\n            \"Adapter '%s' completed: %d LLM calls, %d tool calls\",\n            self._name,\n            len(trace.llm_calls),\n            len(trace.tool_calls),\n        )\n        return trace\n</code></pre>"},{"location":"reference/api/adapters/#langchain-adapter","title":"LangChain Adapter","text":""},{"location":"reference/api/adapters/#agentprobe.adapters.langchain","title":"<code>agentprobe.adapters.langchain</code>","text":"<p>LangChain framework adapter.</p> <p>Wraps a LangChain agent (AgentExecutor or RunnableSequence) and translates its execution into AgentProbe's Trace format by extracting intermediate steps and token usage via callback instrumentation.</p>"},{"location":"reference/api/adapters/#agentprobe.adapters.langchain.LangChainAdapter","title":"<code>LangChainAdapter</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Adapter for LangChain agents (AgentExecutor or Runnable).</p> <p>Captures intermediate steps (tool calls) and token usage from LangChain's callback metadata to build a complete execution trace.</p> <p>Attributes:</p> Name Type Description <code>agent</code> <p>The LangChain agent or runnable to invoke.</p> <code>model_name</code> <p>The model name to use in trace records.</p> Source code in <code>src/agentprobe/adapters/langchain.py</code> <pre><code>class LangChainAdapter(BaseAdapter):\n    \"\"\"Adapter for LangChain agents (AgentExecutor or Runnable).\n\n    Captures intermediate steps (tool calls) and token usage from\n    LangChain's callback metadata to build a complete execution trace.\n\n    Attributes:\n        agent: The LangChain agent or runnable to invoke.\n        model_name: The model name to use in trace records.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent: Any,\n        *,\n        name: str = \"langchain\",\n        model_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the LangChain adapter.\n\n        Args:\n            agent: A LangChain AgentExecutor or Runnable.\n            name: Adapter name for identification.\n            model_name: Model name to record in traces.\n        \"\"\"\n        super().__init__(name)\n        self._agent = agent\n        self._model_name = model_name\n\n    async def _invoke(self, input_text: str, **kwargs: Any) -&gt; Trace:\n        \"\"\"Invoke the LangChain agent and build a trace.\n\n        Attempts async invocation first (``ainvoke``), then falls back\n        to synchronous ``invoke`` if async is not available. Attaches a\n        callback handler to capture token usage from the LLM response.\n\n        Args:\n            input_text: The input prompt.\n            **kwargs: Passed through to the agent.\n\n        Returns:\n            A complete execution trace.\n\n        Raises:\n            AdapterError: If the agent invocation fails.\n        \"\"\"\n        builder = self._create_builder(model=self._model_name)\n        builder.input_text = input_text\n\n        token_handler = _create_token_handler()\n        invoke_config: dict[str, Any] | None = None\n        if token_handler is not None:\n            invoke_config = {\"callbacks\": [token_handler]}\n\n        try:\n            if hasattr(self._agent, \"ainvoke\"):\n                result = await self._agent.ainvoke(\n                    {\"input\": input_text}, config=invoke_config, **kwargs\n                )\n            elif hasattr(self._agent, \"invoke\"):\n                result = self._agent.invoke(\n                    {\"input\": input_text}, config=invoke_config, **kwargs\n                )\n            else:\n                raise AdapterError(\n                    self.name,\n                    \"Agent has neither invoke() nor ainvoke() method\",\n                )\n        except AdapterError:\n            raise\n        except Exception as exc:\n            raise AdapterError(self.name, f\"Agent invocation failed: {exc}\") from exc\n\n        self._extract_result(result, builder)\n\n        # If no token usage from the result dict, use the callback handler\n        if token_handler is not None and not builder.llm_calls:\n            self._apply_callback_tokens(token_handler, builder)\n\n        return builder.build()\n\n    def _apply_callback_tokens(self, token_handler: Any, builder: Any) -&gt; None:\n        \"\"\"Apply token usage captured by the callback handler.\n\n        Args:\n            token_handler: The callback handler with accumulated token data.\n            builder: The trace builder to populate.\n        \"\"\"\n        input_tokens: int = getattr(token_handler, \"total_input_tokens\", 0)\n        output_tokens: int = getattr(token_handler, \"total_output_tokens\", 0)\n\n        if input_tokens &gt; 0 or output_tokens &gt; 0:\n            model_id: str | None = getattr(token_handler, \"model_id\", None)\n            model = model_id or self._model_name or \"unknown\"\n            builder.add_llm_call(\n                LLMCall(\n                    model=str(model),\n                    input_tokens=input_tokens,\n                    output_tokens=output_tokens,\n                )\n            )\n\n    def _extract_result(self, result: Any, builder: Any) -&gt; None:\n        \"\"\"Extract output and intermediate steps from the agent result.\n\n        Args:\n            result: The raw result from agent invocation.\n            builder: The trace builder to populate.\n        \"\"\"\n        if isinstance(result, dict):\n            builder.output_text = str(result.get(\"output\", \"\"))\n            intermediate_steps = result.get(\"intermediate_steps\", [])\n            self._extract_intermediate_steps(intermediate_steps, builder)\n            self._extract_token_usage(result, builder)\n        elif isinstance(result, str):\n            builder.output_text = result\n        else:\n            builder.output_text = str(result)\n\n    def _extract_intermediate_steps(self, steps: list[Any], builder: Any) -&gt; None:\n        \"\"\"Extract tool calls from LangChain intermediate steps.\n\n        Each step is typically a (AgentAction, observation) tuple.\n\n        Args:\n            steps: List of intermediate step tuples.\n            builder: The trace builder to populate.\n        \"\"\"\n        for step in steps:\n            _min_step_length = 2\n            if not isinstance(step, (list, tuple)) or len(step) &lt; _min_step_length:\n                continue\n\n            action, observation = step[0], step[1]\n            tool_name = getattr(action, \"tool\", \"unknown\")\n            tool_input_raw = getattr(action, \"tool_input\", {})\n            tool_input = (\n                tool_input_raw\n                if isinstance(tool_input_raw, dict)\n                else {\"input\": str(tool_input_raw)}\n            )\n\n            builder.add_tool_call(\n                ToolCall(\n                    tool_name=str(tool_name),\n                    tool_input=tool_input,\n                    tool_output=observation,\n                    success=True,\n                )\n            )\n\n    def _extract_token_usage(self, result: dict[str, Any], builder: Any) -&gt; None:\n        \"\"\"Extract token usage from LangChain callback metadata.\n\n        Args:\n            result: The raw agent result dict.\n            builder: The trace builder to populate.\n        \"\"\"\n        token_usage = result.get(\"token_usage\") or result.get(\"llm_output\", {})\n        if not isinstance(token_usage, dict):\n            return\n\n        input_tokens = int(token_usage.get(\"prompt_tokens\", 0))\n        output_tokens = int(token_usage.get(\"completion_tokens\", 0))\n\n        if input_tokens &gt; 0 or output_tokens &gt; 0:\n            model = self._model_name or token_usage.get(\"model_name\", \"unknown\")\n            builder.add_llm_call(\n                LLMCall(\n                    model=str(model),\n                    input_tokens=input_tokens,\n                    output_tokens=output_tokens,\n                )\n            )\n</code></pre>"},{"location":"reference/api/adapters/#agentprobe.adapters.langchain.LangChainAdapter.__init__","title":"<code>__init__(agent, *, name='langchain', model_name=None)</code>","text":"<p>Initialize the LangChain adapter.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Any</code> <p>A LangChain AgentExecutor or Runnable.</p> required <code>name</code> <code>str</code> <p>Adapter name for identification.</p> <code>'langchain'</code> <code>model_name</code> <code>str | None</code> <p>Model name to record in traces.</p> <code>None</code> Source code in <code>src/agentprobe/adapters/langchain.py</code> <pre><code>def __init__(\n    self,\n    agent: Any,\n    *,\n    name: str = \"langchain\",\n    model_name: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the LangChain adapter.\n\n    Args:\n        agent: A LangChain AgentExecutor or Runnable.\n        name: Adapter name for identification.\n        model_name: Model name to record in traces.\n    \"\"\"\n    super().__init__(name)\n    self._agent = agent\n    self._model_name = model_name\n</code></pre>"},{"location":"reference/api/adapters/#crewai-adapter","title":"CrewAI Adapter","text":""},{"location":"reference/api/adapters/#agentprobe.adapters.crewai","title":"<code>agentprobe.adapters.crewai</code>","text":"<p>CrewAI framework adapter.</p> <p>Wraps a CrewAI Crew object and translates its execution into AgentProbe's Trace format by extracting task results and tool usage from the crew output.</p>"},{"location":"reference/api/adapters/#agentprobe.adapters.crewai.CrewAIAdapter","title":"<code>CrewAIAdapter</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Adapter for CrewAI Crew objects.</p> <p>Captures task results and tool usage from CrewAI's kickoff output to build a complete execution trace.</p> <p>Attributes:</p> Name Type Description <code>_crew</code> <p>The CrewAI Crew object to invoke.</p> <code>_model_name</code> <p>Optional model name for trace records.</p> Source code in <code>src/agentprobe/adapters/crewai.py</code> <pre><code>class CrewAIAdapter(BaseAdapter):\n    \"\"\"Adapter for CrewAI Crew objects.\n\n    Captures task results and tool usage from CrewAI's kickoff output\n    to build a complete execution trace.\n\n    Attributes:\n        _crew: The CrewAI Crew object to invoke.\n        _model_name: Optional model name for trace records.\n    \"\"\"\n\n    def __init__(\n        self,\n        crew: Any,\n        *,\n        name: str = \"crewai\",\n        model_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the CrewAI adapter.\n\n        Args:\n            crew: A CrewAI Crew object.\n            name: Adapter name for identification.\n            model_name: Model name to record in traces.\n        \"\"\"\n        super().__init__(name)\n        self._crew = crew\n        self._model_name = model_name\n\n    async def _invoke(self, input_text: str, **kwargs: Any) -&gt; Trace:\n        \"\"\"Invoke the CrewAI crew and build a trace.\n\n        Args:\n            input_text: The input prompt.\n            **kwargs: Passed through to the crew.\n\n        Returns:\n            A complete execution trace.\n\n        Raises:\n            AdapterError: If the crew invocation fails.\n        \"\"\"\n        builder = self._create_builder(model=self._model_name)\n        builder.input_text = input_text\n\n        try:\n            if hasattr(self._crew, \"kickoff_async\"):\n                result = await self._crew.kickoff_async(inputs={\"input\": input_text}, **kwargs)\n            elif hasattr(self._crew, \"kickoff\"):\n                loop = asyncio.get_running_loop()\n                result = await loop.run_in_executor(\n                    None,\n                    lambda: self._crew.kickoff(inputs={\"input\": input_text}),\n                )\n            else:\n                raise AdapterError(\n                    self.name,\n                    \"Crew has neither kickoff() nor kickoff_async() method\",\n                )\n        except AdapterError:\n            raise\n        except Exception as exc:\n            raise AdapterError(self.name, f\"Crew invocation failed: {exc}\") from exc\n\n        self._extract_result(result, builder)\n        return builder.build()\n\n    def _extract_result(self, result: Any, builder: Any) -&gt; None:\n        \"\"\"Extract output and task data from the crew result.\n\n        Args:\n            result: The raw CrewOutput or similar result.\n            builder: The trace builder to populate.\n        \"\"\"\n        if hasattr(result, \"raw\"):\n            builder.output_text = str(result.raw)\n        elif isinstance(result, str):\n            builder.output_text = result\n        else:\n            builder.output_text = str(result)\n\n        # Extract tasks_output if available\n        tasks_output = getattr(result, \"tasks_output\", []) or []\n        for task_output in tasks_output:\n            self._extract_task_tools(task_output, builder)\n\n    def _extract_task_tools(self, task_output: Any, builder: Any) -&gt; None:\n        \"\"\"Extract tool calls from a single task output.\n\n        Args:\n            task_output: A CrewAI TaskOutput object.\n            builder: The trace builder to populate.\n        \"\"\"\n        tools_used = getattr(task_output, \"tools_used\", []) or []\n        for tool_info in tools_used:\n            if isinstance(tool_info, dict):\n                tool_name = str(tool_info.get(\"tool\", \"unknown\"))\n                tool_input = tool_info.get(\"input\", {})\n                tool_output = tool_info.get(\"output\", \"\")\n            else:\n                tool_name = str(getattr(tool_info, \"tool\", \"unknown\"))\n                tool_input = getattr(tool_info, \"input\", {})\n                tool_output = getattr(tool_info, \"output\", \"\")\n\n            if not isinstance(tool_input, dict):\n                tool_input = {\"input\": str(tool_input)}\n\n            builder.add_tool_call(\n                ToolCall(\n                    tool_name=tool_name,\n                    tool_input=tool_input,\n                    tool_output=tool_output,\n                    success=True,\n                )\n            )\n</code></pre>"},{"location":"reference/api/adapters/#agentprobe.adapters.crewai.CrewAIAdapter.__init__","title":"<code>__init__(crew, *, name='crewai', model_name=None)</code>","text":"<p>Initialize the CrewAI adapter.</p> <p>Parameters:</p> Name Type Description Default <code>crew</code> <code>Any</code> <p>A CrewAI Crew object.</p> required <code>name</code> <code>str</code> <p>Adapter name for identification.</p> <code>'crewai'</code> <code>model_name</code> <code>str | None</code> <p>Model name to record in traces.</p> <code>None</code> Source code in <code>src/agentprobe/adapters/crewai.py</code> <pre><code>def __init__(\n    self,\n    crew: Any,\n    *,\n    name: str = \"crewai\",\n    model_name: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the CrewAI adapter.\n\n    Args:\n        crew: A CrewAI Crew object.\n        name: Adapter name for identification.\n        model_name: Model name to record in traces.\n    \"\"\"\n    super().__init__(name)\n    self._crew = crew\n    self._model_name = model_name\n</code></pre>"},{"location":"reference/api/adapters/#autogen-adapter","title":"AutoGen Adapter","text":""},{"location":"reference/api/adapters/#agentprobe.adapters.autogen","title":"<code>agentprobe.adapters.autogen</code>","text":"<p>AutoGen framework adapter.</p> <p>Wraps an AutoGen agent chat session and translates the message history into AgentProbe's Trace format by parsing conversation turns.</p>"},{"location":"reference/api/adapters/#agentprobe.adapters.autogen.AutoGenAdapter","title":"<code>AutoGenAdapter</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Adapter for AutoGen agent conversations.</p> <p>Captures message history from AutoGen's chat interface and translates function calls and assistant responses into a structured trace.</p> <p>Attributes:</p> Name Type Description <code>_agent</code> <p>The primary AutoGen agent (e.g. AssistantAgent).</p> <code>_user_proxy</code> <p>The user proxy agent for initiating chats.</p> <code>_model_name</code> <p>Optional model name for trace records.</p> Source code in <code>src/agentprobe/adapters/autogen.py</code> <pre><code>class AutoGenAdapter(BaseAdapter):\n    \"\"\"Adapter for AutoGen agent conversations.\n\n    Captures message history from AutoGen's chat interface and translates\n    function calls and assistant responses into a structured trace.\n\n    Attributes:\n        _agent: The primary AutoGen agent (e.g. AssistantAgent).\n        _user_proxy: The user proxy agent for initiating chats.\n        _model_name: Optional model name for trace records.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent: Any,\n        user_proxy: Any,\n        *,\n        name: str = \"autogen\",\n        model_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the AutoGen adapter.\n\n        Args:\n            agent: An AutoGen AssistantAgent or similar.\n            user_proxy: An AutoGen UserProxyAgent for initiating chats.\n            name: Adapter name for identification.\n            model_name: Model name to record in traces.\n        \"\"\"\n        super().__init__(name)\n        self._agent = agent\n        self._user_proxy = user_proxy\n        self._model_name = model_name\n\n    async def _invoke(self, input_text: str, **kwargs: Any) -&gt; Trace:\n        \"\"\"Invoke the AutoGen agent chat and build a trace.\n\n        Args:\n            input_text: The input prompt.\n            **kwargs: Passed through to the chat initiation.\n\n        Returns:\n            A complete execution trace.\n\n        Raises:\n            AdapterError: If the chat invocation fails.\n        \"\"\"\n        builder = self._create_builder(model=self._model_name)\n        builder.input_text = input_text\n\n        try:\n            if hasattr(self._user_proxy, \"a_initiate_chat\"):\n                await self._user_proxy.a_initiate_chat(self._agent, message=input_text, **kwargs)\n            elif hasattr(self._user_proxy, \"initiate_chat\"):\n                loop = asyncio.get_running_loop()\n                await loop.run_in_executor(\n                    None,\n                    lambda: self._user_proxy.initiate_chat(self._agent, message=input_text),\n                )\n            else:\n                raise AdapterError(\n                    self.name,\n                    \"User proxy has neither initiate_chat() nor a_initiate_chat()\",\n                )\n        except AdapterError:\n            raise\n        except Exception as exc:\n            raise AdapterError(self.name, f\"Chat invocation failed: {exc}\") from exc\n\n        self._extract_messages(builder)\n        return builder.build()\n\n    def _extract_messages(self, builder: Any) -&gt; None:\n        \"\"\"Extract messages from the agent's chat history.\n\n        Args:\n            builder: The trace builder to populate.\n        \"\"\"\n        messages: list[dict[str, Any]] = []\n        if hasattr(self._agent, \"chat_messages\"):\n            for msg_list in self._agent.chat_messages.values():\n                messages.extend(msg_list)\n        elif hasattr(self._agent, \"messages\"):\n            messages = list(self._agent.messages)\n\n        last_assistant_msg = \"\"\n        for msg in messages:\n            if not isinstance(msg, dict):\n                continue\n\n            role = msg.get(\"role\", \"\")\n            content = msg.get(\"content\", \"\")\n\n            if role == \"assistant\":\n                last_assistant_msg = str(content) if content else \"\"\n                self._extract_function_calls(msg, builder)\n            elif role in {\"function\", \"tool\"}:\n                builder.add_tool_call(\n                    ToolCall(\n                        tool_name=str(msg.get(\"name\", \"unknown\")),\n                        tool_input={},\n                        tool_output=str(content),\n                        success=True,\n                    )\n                )\n\n        builder.output_text = last_assistant_msg\n\n    def _extract_function_calls(self, msg: dict[str, Any], builder: Any) -&gt; None:\n        \"\"\"Extract function/tool calls from an assistant message.\n\n        Args:\n            msg: A single message dict from the chat history.\n            builder: The trace builder to populate.\n        \"\"\"\n        function_call = msg.get(\"function_call\")\n        if isinstance(function_call, dict):\n            name = str(function_call.get(\"name\", \"unknown\"))\n            arguments = function_call.get(\"arguments\", {})\n            if not isinstance(arguments, dict):\n                arguments = {\"input\": str(arguments)}\n            builder.add_tool_call(\n                ToolCall(\n                    tool_name=name,\n                    tool_input=arguments,\n                    tool_output=None,\n                    success=True,\n                )\n            )\n\n        tool_calls = msg.get(\"tool_calls\", [])\n        if isinstance(tool_calls, list):\n            for tc in tool_calls:\n                if not isinstance(tc, dict):\n                    continue\n                func = tc.get(\"function\", {})\n                if isinstance(func, dict):\n                    name = str(func.get(\"name\", \"unknown\"))\n                    arguments = func.get(\"arguments\", {})\n                    if not isinstance(arguments, dict):\n                        arguments = {\"input\": str(arguments)}\n                    builder.add_tool_call(\n                        ToolCall(\n                            tool_name=name,\n                            tool_input=arguments,\n                            tool_output=None,\n                            success=True,\n                        )\n                    )\n\n    def _extract_token_usage(self, builder: Any) -&gt; None:\n        \"\"\"Extract token usage from agent cost tracking if available.\n\n        Args:\n            builder: The trace builder to populate.\n        \"\"\"\n        cost_info = getattr(self._agent, \"cost\", None)\n        if isinstance(cost_info, dict):\n            input_tokens = int(cost_info.get(\"prompt_tokens\", 0))\n            output_tokens = int(cost_info.get(\"completion_tokens\", 0))\n            if input_tokens &gt; 0 or output_tokens &gt; 0:\n                model = self._model_name or \"unknown\"\n                builder.add_llm_call(\n                    LLMCall(\n                        model=model,\n                        input_tokens=input_tokens,\n                        output_tokens=output_tokens,\n                    )\n                )\n</code></pre>"},{"location":"reference/api/adapters/#agentprobe.adapters.autogen.AutoGenAdapter.__init__","title":"<code>__init__(agent, user_proxy, *, name='autogen', model_name=None)</code>","text":"<p>Initialize the AutoGen adapter.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Any</code> <p>An AutoGen AssistantAgent or similar.</p> required <code>user_proxy</code> <code>Any</code> <p>An AutoGen UserProxyAgent for initiating chats.</p> required <code>name</code> <code>str</code> <p>Adapter name for identification.</p> <code>'autogen'</code> <code>model_name</code> <code>str | None</code> <p>Model name to record in traces.</p> <code>None</code> Source code in <code>src/agentprobe/adapters/autogen.py</code> <pre><code>def __init__(\n    self,\n    agent: Any,\n    user_proxy: Any,\n    *,\n    name: str = \"autogen\",\n    model_name: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the AutoGen adapter.\n\n    Args:\n        agent: An AutoGen AssistantAgent or similar.\n        user_proxy: An AutoGen UserProxyAgent for initiating chats.\n        name: Adapter name for identification.\n        model_name: Model name to record in traces.\n    \"\"\"\n    super().__init__(name)\n    self._agent = agent\n    self._user_proxy = user_proxy\n    self._model_name = model_name\n</code></pre>"},{"location":"reference/api/adapters/#mcp-adapter","title":"MCP Adapter","text":""},{"location":"reference/api/adapters/#agentprobe.adapters.mcp","title":"<code>agentprobe.adapters.mcp</code>","text":"<p>MCP (Model Context Protocol) server adapter.</p> <p>Wraps an MCP server via stdio or HTTP transport and translates tool call results into AgentProbe's Trace format.</p>"},{"location":"reference/api/adapters/#agentprobe.adapters.mcp.MCPAdapter","title":"<code>MCPAdapter</code>","text":"<p>               Bases: <code>BaseAdapter</code></p> <p>Adapter for MCP (Model Context Protocol) servers.</p> <p>Communicates with an MCP server to execute tool calls and captures the results as a structured trace.</p> <p>Attributes:</p> Name Type Description <code>_server</code> <p>The MCP server client or connection.</p> <code>_transport</code> <p>Transport type ('stdio' or 'http').</p> <code>_model_name</code> <p>Optional model name for trace records.</p> Source code in <code>src/agentprobe/adapters/mcp.py</code> <pre><code>class MCPAdapter(BaseAdapter):\n    \"\"\"Adapter for MCP (Model Context Protocol) servers.\n\n    Communicates with an MCP server to execute tool calls and captures\n    the results as a structured trace.\n\n    Attributes:\n        _server: The MCP server client or connection.\n        _transport: Transport type ('stdio' or 'http').\n        _model_name: Optional model name for trace records.\n    \"\"\"\n\n    def __init__(\n        self,\n        server: Any,\n        *,\n        name: str = \"mcp\",\n        transport: str = \"stdio\",\n        model_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the MCP adapter.\n\n        Args:\n            server: An MCP server client or connection object.\n            name: Adapter name for identification.\n            transport: Transport protocol ('stdio' or 'http').\n            model_name: Model name to record in traces.\n        \"\"\"\n        super().__init__(name)\n        self._server = server\n        self._transport = transport\n        self._model_name = model_name\n\n    async def _invoke(self, input_text: str, **kwargs: Any) -&gt; Trace:\n        \"\"\"Invoke the MCP server and build a trace.\n\n        Sends the input as a tool call request and captures the response.\n\n        Args:\n            input_text: The input prompt or tool call specification.\n            **kwargs: Additional arguments (e.g., tool_name, tool_args).\n\n        Returns:\n            A complete execution trace.\n\n        Raises:\n            AdapterError: If the server invocation fails.\n        \"\"\"\n        builder = self._create_builder(model=self._model_name)\n        builder.input_text = input_text\n\n        tool_name = kwargs.get(\"tool_name\", \"default\")\n        tool_args = kwargs.get(\"tool_args\", {\"input\": input_text})\n        if not isinstance(tool_args, dict):\n            tool_args = {\"input\": str(tool_args)}\n\n        try:\n            result = await self._call_tool(str(tool_name), tool_args)\n        except AdapterError:\n            raise\n        except Exception as exc:\n            raise AdapterError(self.name, f\"MCP server call failed: {exc}\") from exc\n\n        self._process_result(result, str(tool_name), tool_args, builder)\n        return builder.build()\n\n    async def _call_tool(self, tool_name: str, tool_args: dict[str, Any]) -&gt; Any:\n        \"\"\"Call a tool on the MCP server.\n\n        Args:\n            tool_name: Name of the tool to call.\n            tool_args: Arguments to pass to the tool.\n\n        Returns:\n            The raw tool result.\n\n        Raises:\n            AdapterError: If the server does not support tool calls.\n        \"\"\"\n        if hasattr(self._server, \"call_tool\"):\n            if asyncio.iscoroutinefunction(self._server.call_tool):\n                return await self._server.call_tool(tool_name, tool_args)\n            loop = asyncio.get_running_loop()\n            return await loop.run_in_executor(\n                None,\n                lambda: self._server.call_tool(tool_name, tool_args),\n            )\n\n        raise AdapterError(self.name, \"Server has no call_tool() method\")\n\n    def _process_result(\n        self,\n        result: Any,\n        tool_name: str,\n        tool_args: dict[str, Any],\n        builder: Any,\n    ) -&gt; None:\n        \"\"\"Process the tool call result into the trace builder.\n\n        Args:\n            result: The raw result from the MCP server.\n            tool_name: Name of the tool that was called.\n            tool_args: Arguments that were passed to the tool.\n            builder: The trace builder to populate.\n        \"\"\"\n        if isinstance(result, dict):\n            output_text = result.get(\"content\", result.get(\"text\", \"\"))\n            is_error = result.get(\"isError\", False)\n        elif hasattr(result, \"content\"):\n            content_parts = result.content if isinstance(result.content, list) else [result.content]\n            output_text = \" \".join(getattr(part, \"text\", str(part)) for part in content_parts)\n            is_error = getattr(result, \"isError\", False)\n        else:\n            output_text = str(result)\n            is_error = False\n\n        builder.add_tool_call(\n            ToolCall(\n                tool_name=tool_name,\n                tool_input=tool_args,\n                tool_output=output_text,\n                success=not is_error,\n                error=str(output_text) if is_error else None,\n            )\n        )\n        builder.output_text = str(output_text)\n\n    async def list_tools(self) -&gt; list[dict[str, Any]]:\n        \"\"\"List available tools on the MCP server.\n\n        Returns:\n            A list of tool descriptions.\n\n        Raises:\n            AdapterError: If the server does not support listing tools.\n        \"\"\"\n        if hasattr(self._server, \"list_tools\"):\n            if asyncio.iscoroutinefunction(self._server.list_tools):\n                result = await self._server.list_tools()\n            else:\n                loop = asyncio.get_running_loop()\n                result = await loop.run_in_executor(None, self._server.list_tools)\n            if isinstance(result, list):\n                return [self._normalize_tool_desc(t) for t in result]\n            tools = getattr(result, \"tools\", [])\n            return [self._normalize_tool_desc(t) for t in tools]\n\n        raise AdapterError(self.name, \"Server has no list_tools() method\")\n\n    @staticmethod\n    def _normalize_tool_desc(tool: Any) -&gt; dict[str, Any]:\n        \"\"\"Normalize a tool description to a standard dict format.\n\n        Args:\n            tool: A tool description object or dict.\n\n        Returns:\n            A normalized dict with name, description, and input_schema.\n        \"\"\"\n        if isinstance(tool, dict):\n            return {\n                \"name\": tool.get(\"name\", \"unknown\"),\n                \"description\": tool.get(\"description\", \"\"),\n                \"input_schema\": tool.get(\"inputSchema\", tool.get(\"input_schema\", {})),\n            }\n        return {\n            \"name\": getattr(tool, \"name\", \"unknown\"),\n            \"description\": getattr(tool, \"description\", \"\"),\n            \"input_schema\": getattr(tool, \"inputSchema\", getattr(tool, \"input_schema\", {})),\n        }\n</code></pre>"},{"location":"reference/api/adapters/#agentprobe.adapters.mcp.MCPAdapter.__init__","title":"<code>__init__(server, *, name='mcp', transport='stdio', model_name=None)</code>","text":"<p>Initialize the MCP adapter.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>Any</code> <p>An MCP server client or connection object.</p> required <code>name</code> <code>str</code> <p>Adapter name for identification.</p> <code>'mcp'</code> <code>transport</code> <code>str</code> <p>Transport protocol ('stdio' or 'http').</p> <code>'stdio'</code> <code>model_name</code> <code>str | None</code> <p>Model name to record in traces.</p> <code>None</code> Source code in <code>src/agentprobe/adapters/mcp.py</code> <pre><code>def __init__(\n    self,\n    server: Any,\n    *,\n    name: str = \"mcp\",\n    transport: str = \"stdio\",\n    model_name: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the MCP adapter.\n\n    Args:\n        server: An MCP server client or connection object.\n        name: Adapter name for identification.\n        transport: Transport protocol ('stdio' or 'http').\n        model_name: Model name to record in traces.\n    \"\"\"\n    super().__init__(name)\n    self._server = server\n    self._transport = transport\n    self._model_name = model_name\n</code></pre>"},{"location":"reference/api/adapters/#agentprobe.adapters.mcp.MCPAdapter.list_tools","title":"<code>list_tools()</code>  <code>async</code>","text":"<p>List available tools on the MCP server.</p> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of tool descriptions.</p> <p>Raises:</p> Type Description <code>AdapterError</code> <p>If the server does not support listing tools.</p> Source code in <code>src/agentprobe/adapters/mcp.py</code> <pre><code>async def list_tools(self) -&gt; list[dict[str, Any]]:\n    \"\"\"List available tools on the MCP server.\n\n    Returns:\n        A list of tool descriptions.\n\n    Raises:\n        AdapterError: If the server does not support listing tools.\n    \"\"\"\n    if hasattr(self._server, \"list_tools\"):\n        if asyncio.iscoroutinefunction(self._server.list_tools):\n            result = await self._server.list_tools()\n        else:\n            loop = asyncio.get_running_loop()\n            result = await loop.run_in_executor(None, self._server.list_tools)\n        if isinstance(result, list):\n            return [self._normalize_tool_desc(t) for t in result]\n        tools = getattr(result, \"tools\", [])\n        return [self._normalize_tool_desc(t) for t in tools]\n\n    raise AdapterError(self.name, \"Server has no list_tools() method\")\n</code></pre>"},{"location":"reference/api/core/","title":"Core","text":"<p>Models, runner, assertions, protocols, configuration, and exception hierarchy.</p>"},{"location":"reference/api/core/#models","title":"Models","text":""},{"location":"reference/api/core/#agentprobe.core.models","title":"<code>agentprobe.core.models</code>","text":"<p>Core data models and enumerations for AgentProbe.</p> <p>This module defines all Pydantic models used throughout the framework, including traces, test cases, results, and cost summaries. Output types are frozen (immutable); input/configuration types are mutable.</p>"},{"location":"reference/api/core/#agentprobe.core.models.TestStatus","title":"<code>TestStatus</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Status of a single test case execution.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class TestStatus(StrEnum):\n    \"\"\"Status of a single test case execution.\"\"\"\n\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    PASSED = \"passed\"\n    FAILED = \"failed\"\n    ERROR = \"error\"\n    SKIPPED = \"skipped\"\n    TIMEOUT = \"timeout\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.RunStatus","title":"<code>RunStatus</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Status of an overall agent run or test suite execution.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class RunStatus(StrEnum):\n    \"\"\"Status of an overall agent run or test suite execution.\"\"\"\n\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.TurnType","title":"<code>TurnType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Type of event within a trace turn.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class TurnType(StrEnum):\n    \"\"\"Type of event within a trace turn.\"\"\"\n\n    LLM_CALL = \"llm_call\"\n    TOOL_CALL = \"tool_call\"\n    USER_MESSAGE = \"user_message\"\n    AGENT_MESSAGE = \"agent_message\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.EvalVerdict","title":"<code>EvalVerdict</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Verdict produced by an evaluator.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class EvalVerdict(StrEnum):\n    \"\"\"Verdict produced by an evaluator.\"\"\"\n\n    PASS = \"pass\"\n    FAIL = \"fail\"\n    PARTIAL = \"partial\"\n    ERROR = \"error\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.LLMCall","title":"<code>LLMCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single call to a language model within a trace.</p> <p>Attributes:</p> Name Type Description <code>call_id</code> <code>str</code> <p>Unique identifier for this call.</p> <code>model</code> <code>str</code> <p>Model identifier string (e.g. 'claude-sonnet-4-5-20250929').</p> <code>input_tokens</code> <code>int</code> <p>Number of input/prompt tokens consumed.</p> <code>output_tokens</code> <code>int</code> <p>Number of output/completion tokens produced.</p> <code>input_text</code> <code>str</code> <p>The prompt or input sent to the model.</p> <code>output_text</code> <code>str</code> <p>The response text from the model.</p> <code>latency_ms</code> <code>int</code> <p>Round-trip latency in milliseconds.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional provider-specific metadata.</p> <code>timestamp</code> <code>datetime</code> <p>When the call was made.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class LLMCall(BaseModel):\n    \"\"\"A single call to a language model within a trace.\n\n    Attributes:\n        call_id: Unique identifier for this call.\n        model: Model identifier string (e.g. 'claude-sonnet-4-5-20250929').\n        input_tokens: Number of input/prompt tokens consumed.\n        output_tokens: Number of output/completion tokens produced.\n        input_text: The prompt or input sent to the model.\n        output_text: The response text from the model.\n        latency_ms: Round-trip latency in milliseconds.\n        metadata: Additional provider-specific metadata.\n        timestamp: When the call was made.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    call_id: str = Field(default_factory=lambda: str(uuid4()))\n    model: str\n    input_tokens: int = Field(default=0, ge=0)\n    output_tokens: int = Field(default=0, ge=0)\n    input_text: str = \"\"\n    output_text: str = \"\"\n    latency_ms: int = Field(default=0, ge=0)\n    metadata: dict[str, Any] = Field(default_factory=dict)\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(UTC))\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.ToolCall","title":"<code>ToolCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single tool invocation within a trace.</p> <p>Attributes:</p> Name Type Description <code>call_id</code> <code>str</code> <p>Unique identifier for this call.</p> <code>tool_name</code> <code>str</code> <p>Name of the tool invoked.</p> <code>tool_input</code> <code>dict[str, Any]</code> <p>Arguments passed to the tool.</p> <code>tool_output</code> <code>Any</code> <p>Output returned by the tool.</p> <code>success</code> <code>bool</code> <p>Whether the tool call succeeded.</p> <code>error</code> <code>str | None</code> <p>Error message if the call failed.</p> <code>latency_ms</code> <code>int</code> <p>Round-trip latency in milliseconds.</p> <code>timestamp</code> <code>datetime</code> <p>When the call was made.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class ToolCall(BaseModel):\n    \"\"\"A single tool invocation within a trace.\n\n    Attributes:\n        call_id: Unique identifier for this call.\n        tool_name: Name of the tool invoked.\n        tool_input: Arguments passed to the tool.\n        tool_output: Output returned by the tool.\n        success: Whether the tool call succeeded.\n        error: Error message if the call failed.\n        latency_ms: Round-trip latency in milliseconds.\n        timestamp: When the call was made.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    call_id: str = Field(default_factory=lambda: str(uuid4()))\n    tool_name: str\n    tool_input: dict[str, Any] = Field(default_factory=dict)\n    tool_output: Any = None\n    success: bool = True\n    error: str | None = None\n    latency_ms: int = Field(default=0, ge=0)\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(UTC))\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.Turn","title":"<code>Turn</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single turn (event) within a trace timeline.</p> <p>Attributes:</p> Name Type Description <code>turn_id</code> <code>str</code> <p>Unique identifier for this turn.</p> <code>turn_type</code> <code>TurnType</code> <p>The type of event this turn represents.</p> <code>content</code> <code>str</code> <p>Text content of the turn.</p> <code>llm_call</code> <code>LLMCall | None</code> <p>Associated LLM call, if this is an LLM turn.</p> <code>tool_call</code> <code>ToolCall | None</code> <p>Associated tool call, if this is a tool turn.</p> <code>timestamp</code> <code>datetime</code> <p>When the turn occurred.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class Turn(BaseModel):\n    \"\"\"A single turn (event) within a trace timeline.\n\n    Attributes:\n        turn_id: Unique identifier for this turn.\n        turn_type: The type of event this turn represents.\n        content: Text content of the turn.\n        llm_call: Associated LLM call, if this is an LLM turn.\n        tool_call: Associated tool call, if this is a tool turn.\n        timestamp: When the turn occurred.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    turn_id: str = Field(default_factory=lambda: str(uuid4()))\n    turn_type: TurnType\n    content: str = \"\"\n    llm_call: LLMCall | None = None\n    tool_call: ToolCall | None = None\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(UTC))\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.Trace","title":"<code>Trace</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete execution trace of an agent run.</p> <p>A trace captures the full timeline of LLM calls, tool invocations, and message exchanges during a single agent execution. Once assembled by the TraceRecorder, traces are immutable.</p> <p>Attributes:</p> Name Type Description <code>trace_id</code> <code>str</code> <p>Unique identifier for this trace.</p> <code>agent_name</code> <code>str</code> <p>Name of the agent that produced this trace.</p> <code>model</code> <code>str | None</code> <p>Primary model used during the run.</p> <code>input_text</code> <code>str</code> <p>The input/prompt given to the agent.</p> <code>output_text</code> <code>str</code> <p>The final output produced by the agent.</p> <code>turns</code> <code>tuple[Turn, ...]</code> <p>Ordered list of turns in the execution timeline.</p> <code>llm_calls</code> <code>tuple[LLMCall, ...]</code> <p>All LLM calls made during the run.</p> <code>tool_calls</code> <code>tuple[ToolCall, ...]</code> <p>All tool calls made during the run.</p> <code>total_input_tokens</code> <code>int</code> <p>Aggregate input tokens across all LLM calls.</p> <code>total_output_tokens</code> <code>int</code> <p>Aggregate output tokens across all LLM calls.</p> <code>total_latency_ms</code> <code>int</code> <p>Total execution time in milliseconds.</p> <code>tags</code> <code>tuple[str, ...]</code> <p>Tags for filtering and grouping.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional run metadata.</p> <code>created_at</code> <code>datetime</code> <p>When the trace was created.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class Trace(BaseModel):\n    \"\"\"Complete execution trace of an agent run.\n\n    A trace captures the full timeline of LLM calls, tool invocations,\n    and message exchanges during a single agent execution. Once assembled\n    by the TraceRecorder, traces are immutable.\n\n    Attributes:\n        trace_id: Unique identifier for this trace.\n        agent_name: Name of the agent that produced this trace.\n        model: Primary model used during the run.\n        input_text: The input/prompt given to the agent.\n        output_text: The final output produced by the agent.\n        turns: Ordered list of turns in the execution timeline.\n        llm_calls: All LLM calls made during the run.\n        tool_calls: All tool calls made during the run.\n        total_input_tokens: Aggregate input tokens across all LLM calls.\n        total_output_tokens: Aggregate output tokens across all LLM calls.\n        total_latency_ms: Total execution time in milliseconds.\n        tags: Tags for filtering and grouping.\n        metadata: Additional run metadata.\n        created_at: When the trace was created.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    trace_id: str = Field(default_factory=lambda: str(uuid4()))\n    agent_name: str\n    model: str | None = None\n    input_text: str = \"\"\n    output_text: str = \"\"\n    turns: tuple[Turn, ...] = ()\n    llm_calls: tuple[LLMCall, ...] = ()\n    tool_calls: tuple[ToolCall, ...] = ()\n    total_input_tokens: int = Field(default=0, ge=0)\n    total_output_tokens: int = Field(default=0, ge=0)\n    total_latency_ms: int = Field(default=0, ge=0)\n    tags: tuple[str, ...] = ()\n    metadata: dict[str, Any] = Field(default_factory=dict)\n    created_at: datetime = Field(default_factory=lambda: datetime.now(UTC))\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.EvalResult","title":"<code>EvalResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result produced by an evaluator.</p> <p>Attributes:</p> Name Type Description <code>eval_id</code> <code>str</code> <p>Unique identifier for this evaluation.</p> <code>evaluator_name</code> <code>str</code> <p>Name of the evaluator that produced this result.</p> <code>verdict</code> <code>EvalVerdict</code> <p>Pass/fail/partial/error verdict.</p> <code>score</code> <code>float</code> <p>Numeric score between 0.0 and 1.0.</p> <code>reason</code> <code>str</code> <p>Human-readable explanation of the verdict.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional evaluator-specific data.</p> <code>created_at</code> <code>datetime</code> <p>When the evaluation was performed.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class EvalResult(BaseModel):\n    \"\"\"Result produced by an evaluator.\n\n    Attributes:\n        eval_id: Unique identifier for this evaluation.\n        evaluator_name: Name of the evaluator that produced this result.\n        verdict: Pass/fail/partial/error verdict.\n        score: Numeric score between 0.0 and 1.0.\n        reason: Human-readable explanation of the verdict.\n        metadata: Additional evaluator-specific data.\n        created_at: When the evaluation was performed.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    eval_id: str = Field(default_factory=lambda: str(uuid4()))\n    evaluator_name: str\n    verdict: EvalVerdict\n    score: float = Field(..., ge=0.0, le=1.0)\n    reason: str = \"\"\n    metadata: dict[str, Any] = Field(default_factory=dict)\n    created_at: datetime = Field(default_factory=lambda: datetime.now(UTC))\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.AssertionResult","title":"<code>AssertionResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result of a single test assertion.</p> <p>Attributes:</p> Name Type Description <code>assertion_type</code> <code>str</code> <p>Type of assertion (e.g. 'contain', 'match').</p> <code>passed</code> <code>bool</code> <p>Whether the assertion passed.</p> <code>expected</code> <code>Any</code> <p>The expected value.</p> <code>actual</code> <code>Any</code> <p>The actual value.</p> <code>message</code> <code>str</code> <p>Descriptive message about the result.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class AssertionResult(BaseModel):\n    \"\"\"Result of a single test assertion.\n\n    Attributes:\n        assertion_type: Type of assertion (e.g. 'contain', 'match').\n        passed: Whether the assertion passed.\n        expected: The expected value.\n        actual: The actual value.\n        message: Descriptive message about the result.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    assertion_type: str\n    passed: bool\n    expected: Any = None\n    actual: Any = None\n    message: str = \"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.TestCase","title":"<code>TestCase</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single test scenario to be executed against an agent.</p> <p>TestCase is mutable because the runner populates fields during execution (e.g. status transitions, attaching results).</p> <p>Attributes:</p> Name Type Description <code>test_id</code> <code>str</code> <p>Unique identifier for this test case.</p> <code>name</code> <code>str</code> <p>Human-readable name (usually from the @scenario decorator).</p> <code>description</code> <code>str</code> <p>Detailed description of what this test validates.</p> <code>input_text</code> <code>str</code> <p>The input prompt to send to the agent.</p> <code>expected_output</code> <code>str | None</code> <p>Optional expected output for comparison.</p> <code>tags</code> <code>list[str]</code> <p>Tags for filtering and grouping.</p> <code>timeout_seconds</code> <code>float</code> <p>Maximum allowed execution time.</p> <code>evaluators</code> <code>list[str]</code> <p>Names of evaluators to run on this test.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional test configuration.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class TestCase(BaseModel):\n    \"\"\"A single test scenario to be executed against an agent.\n\n    TestCase is mutable because the runner populates fields during execution\n    (e.g. status transitions, attaching results).\n\n    Attributes:\n        test_id: Unique identifier for this test case.\n        name: Human-readable name (usually from the @scenario decorator).\n        description: Detailed description of what this test validates.\n        input_text: The input prompt to send to the agent.\n        expected_output: Optional expected output for comparison.\n        tags: Tags for filtering and grouping.\n        timeout_seconds: Maximum allowed execution time.\n        evaluators: Names of evaluators to run on this test.\n        metadata: Additional test configuration.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, extra=\"forbid\")\n\n    test_id: str = Field(default_factory=lambda: str(uuid4()))\n    name: str = Field(..., min_length=1, max_length=200)\n    description: str = \"\"\n    input_text: str = \"\"\n    expected_output: str | None = None\n    tags: list[str] = Field(default_factory=list)\n    timeout_seconds: float = Field(default=30.0, gt=0)\n    evaluators: list[str] = Field(default_factory=list)\n    metadata: dict[str, Any] = Field(default_factory=dict)\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v: str) -&gt; str:\n        \"\"\"Ensure test name contains only valid characters.\"\"\"\n        cleaned = v.replace(\"_\", \"\").replace(\"-\", \"\").replace(\" \", \"\")\n        if not cleaned.replace(\".\", \"\").isalnum():\n            msg = \"Test name must be alphanumeric with underscores, hyphens, spaces, or dots\"\n            raise ValueError(msg)\n        return v\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.TestCase.validate_name","title":"<code>validate_name(v)</code>  <code>classmethod</code>","text":"<p>Ensure test name contains only valid characters.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>@field_validator(\"name\")\n@classmethod\ndef validate_name(cls, v: str) -&gt; str:\n    \"\"\"Ensure test name contains only valid characters.\"\"\"\n    cleaned = v.replace(\"_\", \"\").replace(\"-\", \"\").replace(\" \", \"\")\n    if not cleaned.replace(\".\", \"\").isalnum():\n        msg = \"Test name must be alphanumeric with underscores, hyphens, spaces, or dots\"\n        raise ValueError(msg)\n    return v\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.TestResult","title":"<code>TestResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete result of executing a single test case.</p> <p>Attributes:</p> Name Type Description <code>result_id</code> <code>str</code> <p>Unique identifier for this result.</p> <code>test_name</code> <code>str</code> <p>Name of the test that was executed.</p> <code>status</code> <code>TestStatus</code> <p>Final status of the test execution.</p> <code>score</code> <code>float</code> <p>Aggregate score from evaluators (0.0 to 1.0).</p> <code>duration_ms</code> <code>int</code> <p>Execution time in milliseconds.</p> <code>trace</code> <code>Trace | None</code> <p>The execution trace, if recording was enabled.</p> <code>eval_results</code> <code>tuple[EvalResult, ...]</code> <p>Results from all evaluators run on this test.</p> <code>assertion_results</code> <code>tuple[AssertionResult, ...]</code> <p>Results from all assertions.</p> <code>error_message</code> <code>str | None</code> <p>Error description if the test errored.</p> <code>created_at</code> <code>datetime</code> <p>When the result was recorded.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class TestResult(BaseModel):\n    \"\"\"Complete result of executing a single test case.\n\n    Attributes:\n        result_id: Unique identifier for this result.\n        test_name: Name of the test that was executed.\n        status: Final status of the test execution.\n        score: Aggregate score from evaluators (0.0 to 1.0).\n        duration_ms: Execution time in milliseconds.\n        trace: The execution trace, if recording was enabled.\n        eval_results: Results from all evaluators run on this test.\n        assertion_results: Results from all assertions.\n        error_message: Error description if the test errored.\n        created_at: When the result was recorded.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    result_id: str = Field(default_factory=lambda: str(uuid4()))\n    test_name: str = Field(..., min_length=1, max_length=200)\n    status: TestStatus\n    score: float = Field(default=0.0, ge=0.0, le=1.0)\n    duration_ms: int = Field(default=0, ge=0)\n    trace: Trace | None = None\n    eval_results: tuple[EvalResult, ...] = ()\n    assertion_results: tuple[AssertionResult, ...] = ()\n    error_message: str | None = None\n    created_at: datetime = Field(default_factory=lambda: datetime.now(UTC))\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.CostBreakdown","title":"<code>CostBreakdown</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Cost breakdown for a single model.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The model identifier.</p> <code>input_tokens</code> <code>int</code> <p>Total input tokens for this model.</p> <code>output_tokens</code> <code>int</code> <p>Total output tokens for this model.</p> <code>input_cost_usd</code> <code>float</code> <p>Cost for input tokens in USD.</p> <code>output_cost_usd</code> <code>float</code> <p>Cost for output tokens in USD.</p> <code>total_cost_usd</code> <code>float</code> <p>Total cost in USD.</p> <code>call_count</code> <code>int</code> <p>Number of calls to this model.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class CostBreakdown(BaseModel):\n    \"\"\"Cost breakdown for a single model.\n\n    Attributes:\n        model: The model identifier.\n        input_tokens: Total input tokens for this model.\n        output_tokens: Total output tokens for this model.\n        input_cost_usd: Cost for input tokens in USD.\n        output_cost_usd: Cost for output tokens in USD.\n        total_cost_usd: Total cost in USD.\n        call_count: Number of calls to this model.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    model: str\n    input_tokens: int = Field(default=0, ge=0)\n    output_tokens: int = Field(default=0, ge=0)\n    input_cost_usd: float = Field(default=0.0, ge=0.0)\n    output_cost_usd: float = Field(default=0.0, ge=0.0)\n    total_cost_usd: float = Field(default=0.0, ge=0.0)\n    call_count: int = Field(default=0, ge=0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.CostSummary","title":"<code>CostSummary</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Aggregate cost summary for a trace or test suite.</p> <p>Attributes:</p> Name Type Description <code>total_llm_cost_usd</code> <code>float</code> <p>Total cost of all LLM calls in USD.</p> <code>total_tool_cost_usd</code> <code>float</code> <p>Total cost of tool usage in USD.</p> <code>total_cost_usd</code> <code>float</code> <p>Grand total cost in USD.</p> <code>breakdown_by_model</code> <code>dict[str, CostBreakdown]</code> <p>Per-model cost breakdown.</p> <code>total_input_tokens</code> <code>int</code> <p>Aggregate input tokens.</p> <code>total_output_tokens</code> <code>int</code> <p>Aggregate output tokens.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class CostSummary(BaseModel):\n    \"\"\"Aggregate cost summary for a trace or test suite.\n\n    Attributes:\n        total_llm_cost_usd: Total cost of all LLM calls in USD.\n        total_tool_cost_usd: Total cost of tool usage in USD.\n        total_cost_usd: Grand total cost in USD.\n        breakdown_by_model: Per-model cost breakdown.\n        total_input_tokens: Aggregate input tokens.\n        total_output_tokens: Aggregate output tokens.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    total_llm_cost_usd: float = Field(default=0.0, ge=0.0)\n    total_tool_cost_usd: float = Field(default=0.0, ge=0.0)\n    total_cost_usd: float = Field(default=0.0, ge=0.0)\n    breakdown_by_model: dict[str, CostBreakdown] = Field(default_factory=dict)\n    total_input_tokens: int = Field(default=0, ge=0)\n    total_output_tokens: int = Field(default=0, ge=0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.MetricType","title":"<code>MetricType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Type of metric being measured.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class MetricType(StrEnum):\n    \"\"\"Type of metric being measured.\"\"\"\n\n    LATENCY = \"latency\"\n    COST = \"cost\"\n    TOKENS = \"tokens\"\n    SCORE = \"score\"\n    COUNT = \"count\"\n    RATE = \"rate\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.TrendDirection","title":"<code>TrendDirection</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Direction of a metric trend over time.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class TrendDirection(StrEnum):\n    \"\"\"Direction of a metric trend over time.\"\"\"\n\n    IMPROVING = \"improving\"\n    DEGRADING = \"degrading\"\n    STABLE = \"stable\"\n    INSUFFICIENT_DATA = \"insufficient_data\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.PluginType","title":"<code>PluginType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Type of plugin extension.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class PluginType(StrEnum):\n    \"\"\"Type of plugin extension.\"\"\"\n\n    EVALUATOR = \"evaluator\"\n    ADAPTER = \"adapter\"\n    REPORTER = \"reporter\"\n    STORAGE = \"storage\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.ChaosType","title":"<code>ChaosType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Type of chaos fault to inject during testing.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class ChaosType(StrEnum):\n    \"\"\"Type of chaos fault to inject during testing.\"\"\"\n\n    TIMEOUT = \"timeout\"\n    ERROR = \"error\"\n    MALFORMED = \"malformed\"\n    RATE_LIMIT = \"rate_limit\"\n    SLOW = \"slow\"\n    EMPTY = \"empty\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.ConversationTurn","title":"<code>ConversationTurn</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for a single turn in a multi-turn conversation test.</p> <p>Attributes:</p> Name Type Description <code>turn_id</code> <code>str</code> <p>Unique identifier for this turn.</p> <code>input_text</code> <code>str</code> <p>The input to send for this turn.</p> <code>expected_output</code> <code>str | None</code> <p>Optional expected output for this turn.</p> <code>evaluators</code> <code>tuple[str, ...]</code> <p>Evaluator names to run on this turn's result.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional turn-level configuration.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class ConversationTurn(BaseModel):\n    \"\"\"Specification for a single turn in a multi-turn conversation test.\n\n    Attributes:\n        turn_id: Unique identifier for this turn.\n        input_text: The input to send for this turn.\n        expected_output: Optional expected output for this turn.\n        evaluators: Evaluator names to run on this turn's result.\n        metadata: Additional turn-level configuration.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    turn_id: str = Field(default_factory=lambda: str(uuid4()))\n    input_text: str\n    expected_output: str | None = None\n    evaluators: tuple[str, ...] = ()\n    metadata: dict[str, Any] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.TurnResult","title":"<code>TurnResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result from executing a single conversation turn.</p> <p>Attributes:</p> Name Type Description <code>turn_index</code> <code>int</code> <p>Zero-based index of this turn.</p> <code>input_text</code> <code>str</code> <p>The input sent for this turn.</p> <code>trace</code> <code>Trace | None</code> <p>Execution trace from this turn.</p> <code>eval_results</code> <code>tuple[EvalResult, ...]</code> <p>Results from evaluators run on this turn.</p> <code>duration_ms</code> <code>int</code> <p>Execution time for this turn in milliseconds.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class TurnResult(BaseModel):\n    \"\"\"Result from executing a single conversation turn.\n\n    Attributes:\n        turn_index: Zero-based index of this turn.\n        input_text: The input sent for this turn.\n        trace: Execution trace from this turn.\n        eval_results: Results from evaluators run on this turn.\n        duration_ms: Execution time for this turn in milliseconds.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    turn_index: int = Field(ge=0)\n    input_text: str = \"\"\n    trace: Trace | None = None\n    eval_results: tuple[EvalResult, ...] = ()\n    duration_ms: int = Field(default=0, ge=0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.ConversationResult","title":"<code>ConversationResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Aggregate result from a multi-turn conversation test.</p> <p>Attributes:</p> Name Type Description <code>conversation_id</code> <code>str</code> <p>Unique identifier for this conversation.</p> <code>agent_name</code> <code>str</code> <p>Name of the agent tested.</p> <code>turn_results</code> <code>tuple[TurnResult, ...]</code> <p>Per-turn results in order.</p> <code>total_turns</code> <code>int</code> <p>Number of turns executed.</p> <code>passed_turns</code> <code>int</code> <p>Number of turns where all evaluators passed.</p> <code>aggregate_score</code> <code>float</code> <p>Mean score across all turns.</p> <code>total_duration_ms</code> <code>int</code> <p>Total execution time in milliseconds.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class ConversationResult(BaseModel):\n    \"\"\"Aggregate result from a multi-turn conversation test.\n\n    Attributes:\n        conversation_id: Unique identifier for this conversation.\n        agent_name: Name of the agent tested.\n        turn_results: Per-turn results in order.\n        total_turns: Number of turns executed.\n        passed_turns: Number of turns where all evaluators passed.\n        aggregate_score: Mean score across all turns.\n        total_duration_ms: Total execution time in milliseconds.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    conversation_id: str = Field(default_factory=lambda: str(uuid4()))\n    agent_name: str = \"\"\n    turn_results: tuple[TurnResult, ...] = ()\n    total_turns: int = Field(default=0, ge=0)\n    passed_turns: int = Field(default=0, ge=0)\n    aggregate_score: float = Field(default=0.0, ge=0.0, le=1.0)\n    total_duration_ms: int = Field(default=0, ge=0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.StatisticalSummary","title":"<code>StatisticalSummary</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Summary statistics from repeated evaluations.</p> <p>Attributes:</p> Name Type Description <code>evaluator_name</code> <code>str</code> <p>Name of the evaluator that produced these stats.</p> <code>sample_count</code> <code>int</code> <p>Number of evaluation runs.</p> <code>scores</code> <code>tuple[float, ...]</code> <p>Raw scores from each run (for reproducibility).</p> <code>mean</code> <code>float</code> <p>Arithmetic mean of scores.</p> <code>std_dev</code> <code>float</code> <p>Standard deviation of scores.</p> <code>median</code> <code>float</code> <p>Median score.</p> <code>p5</code> <code>float</code> <p>5th percentile score.</p> <code>p95</code> <code>float</code> <p>95th percentile score.</p> <code>ci_lower</code> <code>float</code> <p>Lower bound of 95% confidence interval.</p> <code>ci_upper</code> <code>float</code> <p>Upper bound of 95% confidence interval.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class StatisticalSummary(BaseModel):\n    \"\"\"Summary statistics from repeated evaluations.\n\n    Attributes:\n        evaluator_name: Name of the evaluator that produced these stats.\n        sample_count: Number of evaluation runs.\n        scores: Raw scores from each run (for reproducibility).\n        mean: Arithmetic mean of scores.\n        std_dev: Standard deviation of scores.\n        median: Median score.\n        p5: 5th percentile score.\n        p95: 95th percentile score.\n        ci_lower: Lower bound of 95% confidence interval.\n        ci_upper: Upper bound of 95% confidence interval.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    evaluator_name: str\n    sample_count: int = Field(ge=1)\n    scores: tuple[float, ...] = ()\n    mean: float = Field(default=0.0, ge=0.0, le=1.0)\n    std_dev: float = Field(default=0.0, ge=0.0)\n    median: float = Field(default=0.0, ge=0.0, le=1.0)\n    p5: float = Field(default=0.0, ge=0.0, le=1.0)\n    p95: float = Field(default=0.0, ge=0.0, le=1.0)\n    ci_lower: float = Field(default=0.0, ge=0.0, le=1.0)\n    ci_upper: float = Field(default=0.0, ge=0.0, le=1.0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.TestComparison","title":"<code>TestComparison</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Comparison of a single test between baseline and current results.</p> <p>Attributes:</p> Name Type Description <code>test_name</code> <code>str</code> <p>Name of the compared test.</p> <code>baseline_score</code> <code>float</code> <p>Score from the baseline run.</p> <code>current_score</code> <code>float</code> <p>Score from the current run.</p> <code>delta</code> <code>float</code> <p>Score change (current - baseline).</p> <code>is_regression</code> <code>bool</code> <p>Whether the change constitutes a regression.</p> <code>is_improvement</code> <code>bool</code> <p>Whether the change constitutes an improvement.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class TestComparison(BaseModel):\n    \"\"\"Comparison of a single test between baseline and current results.\n\n    Attributes:\n        test_name: Name of the compared test.\n        baseline_score: Score from the baseline run.\n        current_score: Score from the current run.\n        delta: Score change (current - baseline).\n        is_regression: Whether the change constitutes a regression.\n        is_improvement: Whether the change constitutes an improvement.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    test_name: str\n    baseline_score: float = Field(ge=0.0, le=1.0)\n    current_score: float = Field(ge=0.0, le=1.0)\n    delta: float = 0.0\n    is_regression: bool = False\n    is_improvement: bool = False\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.RegressionReport","title":"<code>RegressionReport</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Report from comparing current results against a baseline.</p> <p>Attributes:</p> Name Type Description <code>baseline_name</code> <code>str</code> <p>Name of the baseline used for comparison.</p> <code>comparisons</code> <code>tuple[TestComparison, ...]</code> <p>Per-test comparisons.</p> <code>total_tests</code> <code>int</code> <p>Number of tests compared.</p> <code>regressions</code> <code>int</code> <p>Number of tests that regressed.</p> <code>improvements</code> <code>int</code> <p>Number of tests that improved.</p> <code>unchanged</code> <code>int</code> <p>Number of tests with no significant change.</p> <code>threshold</code> <code>float</code> <p>Score delta threshold used for regression detection.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class RegressionReport(BaseModel):\n    \"\"\"Report from comparing current results against a baseline.\n\n    Attributes:\n        baseline_name: Name of the baseline used for comparison.\n        comparisons: Per-test comparisons.\n        total_tests: Number of tests compared.\n        regressions: Number of tests that regressed.\n        improvements: Number of tests that improved.\n        unchanged: Number of tests with no significant change.\n        threshold: Score delta threshold used for regression detection.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    baseline_name: str\n    comparisons: tuple[TestComparison, ...] = ()\n    total_tests: int = Field(default=0, ge=0)\n    regressions: int = Field(default=0, ge=0)\n    improvements: int = Field(default=0, ge=0)\n    unchanged: int = Field(default=0, ge=0)\n    threshold: float = Field(default=0.05, ge=0.0, le=1.0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.BudgetCheckResult","title":"<code>BudgetCheckResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result of checking a cost against a budget.</p> <p>Attributes:</p> Name Type Description <code>within_budget</code> <code>bool</code> <p>Whether the cost is within the budget.</p> <code>actual_cost_usd</code> <code>float</code> <p>The actual cost incurred.</p> <code>budget_limit_usd</code> <code>float</code> <p>The budget limit.</p> <code>remaining_usd</code> <code>float</code> <p>Budget remaining (may be negative if exceeded).</p> <code>utilization_pct</code> <code>float</code> <p>Percentage of budget used.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class BudgetCheckResult(BaseModel):\n    \"\"\"Result of checking a cost against a budget.\n\n    Attributes:\n        within_budget: Whether the cost is within the budget.\n        actual_cost_usd: The actual cost incurred.\n        budget_limit_usd: The budget limit.\n        remaining_usd: Budget remaining (may be negative if exceeded).\n        utilization_pct: Percentage of budget used.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    within_budget: bool\n    actual_cost_usd: float = Field(ge=0.0)\n    budget_limit_usd: float = Field(ge=0.0)\n    remaining_usd: float = 0.0\n    utilization_pct: float = Field(default=0.0, ge=0.0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.DiffItem","title":"<code>DiffItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single difference between two snapshots.</p> <p>Attributes:</p> Name Type Description <code>dimension</code> <code>str</code> <p>The dimension being compared (e.g. 'tool_calls', 'cost').</p> <code>expected</code> <code>Any</code> <p>The expected (baseline) value.</p> <code>actual</code> <code>Any</code> <p>The actual (current) value.</p> <code>similarity</code> <code>float</code> <p>Similarity score for this dimension (0.0 to 1.0).</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class DiffItem(BaseModel):\n    \"\"\"A single difference between two snapshots.\n\n    Attributes:\n        dimension: The dimension being compared (e.g. 'tool_calls', 'cost').\n        expected: The expected (baseline) value.\n        actual: The actual (current) value.\n        similarity: Similarity score for this dimension (0.0 to 1.0).\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    dimension: str\n    expected: Any = None\n    actual: Any = None\n    similarity: float = Field(default=0.0, ge=0.0, le=1.0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.SnapshotDiff","title":"<code>SnapshotDiff</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Comparison result between a snapshot and current output.</p> <p>Attributes:</p> Name Type Description <code>snapshot_name</code> <code>str</code> <p>Name of the snapshot being compared.</p> <code>overall_similarity</code> <code>float</code> <p>Weighted average similarity across dimensions.</p> <code>diffs</code> <code>tuple[DiffItem, ...]</code> <p>Per-dimension comparison details.</p> <code>is_match</code> <code>bool</code> <p>Whether the overall similarity meets the threshold.</p> <code>threshold</code> <code>float</code> <p>Similarity threshold used.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class SnapshotDiff(BaseModel):\n    \"\"\"Comparison result between a snapshot and current output.\n\n    Attributes:\n        snapshot_name: Name of the snapshot being compared.\n        overall_similarity: Weighted average similarity across dimensions.\n        diffs: Per-dimension comparison details.\n        is_match: Whether the overall similarity meets the threshold.\n        threshold: Similarity threshold used.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    snapshot_name: str\n    overall_similarity: float = Field(default=0.0, ge=0.0, le=1.0)\n    diffs: tuple[DiffItem, ...] = ()\n    is_match: bool = False\n    threshold: float = Field(default=0.8, ge=0.0, le=1.0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.TraceStep","title":"<code>TraceStep</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single step in a time-travel trace, with cumulative metrics.</p> <p>Attributes:</p> Name Type Description <code>step_index</code> <code>int</code> <p>Zero-based index of this step.</p> <code>turn</code> <code>Turn</code> <p>The trace turn at this step.</p> <code>cumulative_input_tokens</code> <code>int</code> <p>Total input tokens up to this step.</p> <code>cumulative_output_tokens</code> <code>int</code> <p>Total output tokens up to this step.</p> <code>cumulative_cost_usd</code> <code>float</code> <p>Estimated cumulative cost up to this step.</p> <code>cumulative_latency_ms</code> <code>int</code> <p>Total latency up to this step.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class TraceStep(BaseModel):\n    \"\"\"A single step in a time-travel trace, with cumulative metrics.\n\n    Attributes:\n        step_index: Zero-based index of this step.\n        turn: The trace turn at this step.\n        cumulative_input_tokens: Total input tokens up to this step.\n        cumulative_output_tokens: Total output tokens up to this step.\n        cumulative_cost_usd: Estimated cumulative cost up to this step.\n        cumulative_latency_ms: Total latency up to this step.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    step_index: int = Field(ge=0)\n    turn: Turn\n    cumulative_input_tokens: int = Field(default=0, ge=0)\n    cumulative_output_tokens: int = Field(default=0, ge=0)\n    cumulative_cost_usd: float = Field(default=0.0, ge=0.0)\n    cumulative_latency_ms: int = Field(default=0, ge=0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.ReplayDiff","title":"<code>ReplayDiff</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Diff between an original trace and a replay trace.</p> <p>Attributes:</p> Name Type Description <code>original_trace_id</code> <code>str</code> <p>ID of the original trace.</p> <code>replay_trace_id</code> <code>str</code> <p>ID of the replay trace.</p> <code>tool_call_diffs</code> <code>tuple[DiffItem, ...]</code> <p>Differences in tool calls.</p> <code>output_matches</code> <code>bool</code> <p>Whether the outputs match.</p> <code>original_output</code> <code>str</code> <p>Output from the original trace.</p> <code>replay_output</code> <code>str</code> <p>Output from the replay trace.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class ReplayDiff(BaseModel):\n    \"\"\"Diff between an original trace and a replay trace.\n\n    Attributes:\n        original_trace_id: ID of the original trace.\n        replay_trace_id: ID of the replay trace.\n        tool_call_diffs: Differences in tool calls.\n        output_matches: Whether the outputs match.\n        original_output: Output from the original trace.\n        replay_output: Output from the replay trace.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    original_trace_id: str = \"\"\n    replay_trace_id: str = \"\"\n    tool_call_diffs: tuple[DiffItem, ...] = ()\n    output_matches: bool = False\n    original_output: str = \"\"\n    replay_output: str = \"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.ChaosOverride","title":"<code>ChaosOverride</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a single chaos fault injection.</p> <p>Attributes:</p> Name Type Description <code>chaos_type</code> <code>ChaosType</code> <p>Type of fault to inject.</p> <code>probability</code> <code>float</code> <p>Probability of applying this fault (0.0 to 1.0).</p> <code>target_tool</code> <code>str | None</code> <p>If set, only apply to this specific tool.</p> <code>delay_ms</code> <code>int</code> <p>Delay in ms for SLOW type.</p> <code>error_message</code> <code>str</code> <p>Custom error message for ERROR type.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class ChaosOverride(BaseModel):\n    \"\"\"Configuration for a single chaos fault injection.\n\n    Attributes:\n        chaos_type: Type of fault to inject.\n        probability: Probability of applying this fault (0.0 to 1.0).\n        target_tool: If set, only apply to this specific tool.\n        delay_ms: Delay in ms for SLOW type.\n        error_message: Custom error message for ERROR type.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    chaos_type: ChaosType\n    probability: float = Field(default=1.0, ge=0.0, le=1.0)\n    target_tool: str | None = None\n    delay_ms: int = Field(default=5000, ge=0)\n    error_message: str = \"Chaos fault injected\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.AgentRun","title":"<code>AgentRun</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A complete agent test run encompassing multiple test results.</p> <p>Attributes:</p> Name Type Description <code>run_id</code> <code>str</code> <p>Unique identifier for this run.</p> <code>agent_name</code> <code>str</code> <p>Name of the agent tested.</p> <code>status</code> <code>RunStatus</code> <p>Overall run status.</p> <code>test_results</code> <code>tuple[TestResult, ...]</code> <p>All test results from this run.</p> <code>total_tests</code> <code>int</code> <p>Total number of tests.</p> <code>passed</code> <code>int</code> <p>Number of tests that passed.</p> <code>failed</code> <code>int</code> <p>Number of tests that failed.</p> <code>errors</code> <code>int</code> <p>Number of tests that errored.</p> <code>skipped</code> <code>int</code> <p>Number of tests skipped.</p> <code>cost_summary</code> <code>CostSummary | None</code> <p>Aggregate cost for the run.</p> <code>duration_ms</code> <code>int</code> <p>Total run duration in milliseconds.</p> <code>tags</code> <code>tuple[str, ...]</code> <p>Tags for filtering.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional run metadata.</p> <code>created_at</code> <code>datetime</code> <p>When the run started.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class AgentRun(BaseModel):\n    \"\"\"A complete agent test run encompassing multiple test results.\n\n    Attributes:\n        run_id: Unique identifier for this run.\n        agent_name: Name of the agent tested.\n        status: Overall run status.\n        test_results: All test results from this run.\n        total_tests: Total number of tests.\n        passed: Number of tests that passed.\n        failed: Number of tests that failed.\n        errors: Number of tests that errored.\n        skipped: Number of tests skipped.\n        cost_summary: Aggregate cost for the run.\n        duration_ms: Total run duration in milliseconds.\n        tags: Tags for filtering.\n        metadata: Additional run metadata.\n        created_at: When the run started.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    run_id: str = Field(default_factory=lambda: str(uuid4()))\n    agent_name: str\n    status: RunStatus\n    test_results: tuple[TestResult, ...] = ()\n    total_tests: int = Field(default=0, ge=0)\n    passed: int = Field(default=0, ge=0)\n    failed: int = Field(default=0, ge=0)\n    errors: int = Field(default=0, ge=0)\n    skipped: int = Field(default=0, ge=0)\n    cost_summary: CostSummary | None = None\n    duration_ms: int = Field(default=0, ge=0)\n    tags: tuple[str, ...] = ()\n    metadata: dict[str, Any] = Field(default_factory=dict)\n    created_at: datetime = Field(default_factory=lambda: datetime.now(UTC))\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.MetricDefinition","title":"<code>MetricDefinition</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Definition of a named metric that can be collected and tracked.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique metric identifier (e.g. 'latency_ms', 'token_cost_usd').</p> <code>metric_type</code> <code>MetricType</code> <p>Category of the metric.</p> <code>description</code> <code>str</code> <p>Human-readable description.</p> <code>unit</code> <code>str</code> <p>Unit of measurement (e.g. 'ms', 'usd', 'count').</p> <code>lower_is_better</code> <code>bool</code> <p>Whether lower values indicate better performance.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class MetricDefinition(BaseModel):\n    \"\"\"Definition of a named metric that can be collected and tracked.\n\n    Attributes:\n        name: Unique metric identifier (e.g. 'latency_ms', 'token_cost_usd').\n        metric_type: Category of the metric.\n        description: Human-readable description.\n        unit: Unit of measurement (e.g. 'ms', 'usd', 'count').\n        lower_is_better: Whether lower values indicate better performance.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    name: str = Field(..., min_length=1, max_length=200)\n    metric_type: MetricType\n    description: str = \"\"\n    unit: str = \"\"\n    lower_is_better: bool = True\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.MetricValue","title":"<code>MetricValue</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single metric measurement at a point in time.</p> <p>Attributes:</p> Name Type Description <code>metric_name</code> <code>str</code> <p>Name of the metric this value belongs to.</p> <code>value</code> <code>float</code> <p>The numeric measurement.</p> <code>tags</code> <code>tuple[str, ...]</code> <p>Tags for filtering and grouping.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional context about this measurement.</p> <code>timestamp</code> <code>datetime</code> <p>When the measurement was taken.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class MetricValue(BaseModel):\n    \"\"\"A single metric measurement at a point in time.\n\n    Attributes:\n        metric_name: Name of the metric this value belongs to.\n        value: The numeric measurement.\n        tags: Tags for filtering and grouping.\n        metadata: Additional context about this measurement.\n        timestamp: When the measurement was taken.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    metric_name: str = Field(..., min_length=1)\n    value: float\n    tags: tuple[str, ...] = ()\n    metadata: dict[str, Any] = Field(default_factory=dict)\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(UTC))\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.MetricAggregation","title":"<code>MetricAggregation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Aggregated statistics for a collection of metric values.</p> <p>Attributes:</p> Name Type Description <code>metric_name</code> <code>str</code> <p>Name of the metric.</p> <code>count</code> <code>int</code> <p>Number of values aggregated.</p> <code>mean</code> <code>float</code> <p>Arithmetic mean.</p> <code>median</code> <code>float</code> <p>Median value.</p> <code>min_value</code> <code>float</code> <p>Minimum value.</p> <code>max_value</code> <code>float</code> <p>Maximum value.</p> <code>p95</code> <code>float</code> <p>95th percentile.</p> <code>p99</code> <code>float</code> <p>99th percentile.</p> <code>std_dev</code> <code>float</code> <p>Standard deviation.</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class MetricAggregation(BaseModel):\n    \"\"\"Aggregated statistics for a collection of metric values.\n\n    Attributes:\n        metric_name: Name of the metric.\n        count: Number of values aggregated.\n        mean: Arithmetic mean.\n        median: Median value.\n        min_value: Minimum value.\n        max_value: Maximum value.\n        p95: 95th percentile.\n        p99: 99th percentile.\n        std_dev: Standard deviation.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    metric_name: str = Field(..., min_length=1)\n    count: int = Field(ge=1)\n    mean: float = 0.0\n    median: float = 0.0\n    min_value: float = 0.0\n    max_value: float = 0.0\n    p95: float = 0.0\n    p99: float = 0.0\n    std_dev: float = Field(default=0.0, ge=0.0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.models.TraceDiffReport","title":"<code>TraceDiffReport</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Report from comparing two independent traces.</p> <p>Compares output text, tool call sequences, model usage, token counts, and latency between any two traces.</p> <p>Attributes:</p> Name Type Description <code>trace_a_id</code> <code>str</code> <p>ID of the first trace.</p> <code>trace_b_id</code> <code>str</code> <p>ID of the second trace.</p> <code>tool_call_diffs</code> <code>tuple[DiffItem, ...]</code> <p>Per-tool-call comparison items.</p> <code>output_matches</code> <code>bool</code> <p>Whether the output texts match exactly.</p> <code>token_delta</code> <code>int</code> <p>Difference in total tokens (B - A).</p> <code>latency_delta_ms</code> <code>int</code> <p>Difference in total latency (B - A).</p> <code>overall_similarity</code> <code>float</code> <p>Weighted similarity score (0.0 to 1.0).</p> Source code in <code>src/agentprobe/core/models.py</code> <pre><code>class TraceDiffReport(BaseModel):\n    \"\"\"Report from comparing two independent traces.\n\n    Compares output text, tool call sequences, model usage,\n    token counts, and latency between any two traces.\n\n    Attributes:\n        trace_a_id: ID of the first trace.\n        trace_b_id: ID of the second trace.\n        tool_call_diffs: Per-tool-call comparison items.\n        output_matches: Whether the output texts match exactly.\n        token_delta: Difference in total tokens (B - A).\n        latency_delta_ms: Difference in total latency (B - A).\n        overall_similarity: Weighted similarity score (0.0 to 1.0).\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    trace_a_id: str = \"\"\n    trace_b_id: str = \"\"\n    tool_call_diffs: tuple[DiffItem, ...] = ()\n    output_matches: bool = False\n    token_delta: int = 0\n    latency_delta_ms: int = 0\n    overall_similarity: float = Field(default=0.0, ge=0.0, le=1.0)\n</code></pre>"},{"location":"reference/api/core/#runner","title":"Runner","text":""},{"location":"reference/api/core/#agentprobe.core.runner","title":"<code>agentprobe.core.runner</code>","text":"<p>Test runner: orchestrates test execution with optional parallelism.</p> <p>Discovers tests, invokes them against an adapter, runs evaluators, and assembles results into an AgentRun.</p>"},{"location":"reference/api/core/#agentprobe.core.runner.TestRunner","title":"<code>TestRunner</code>","text":"<p>Orchestrates test case execution against an agent adapter.</p> <p>Supports sequential and parallel execution modes, per-test timeouts, and evaluator orchestration.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>The runner configuration.</p> <code>evaluators</code> <p>Evaluators to run on each test result.</p> Source code in <code>src/agentprobe/core/runner.py</code> <pre><code>class TestRunner:\n    \"\"\"Orchestrates test case execution against an agent adapter.\n\n    Supports sequential and parallel execution modes, per-test timeouts,\n    and evaluator orchestration.\n\n    Attributes:\n        config: The runner configuration.\n        evaluators: Evaluators to run on each test result.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: AgentProbeConfig | None = None,\n        evaluators: list[EvaluatorProtocol] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the test runner.\n\n        Args:\n            config: AgentProbe configuration. Uses defaults if None.\n            evaluators: Evaluators to apply to test results.\n        \"\"\"\n        self._config = config or AgentProbeConfig()\n        self._evaluators = evaluators or []\n\n    async def run(\n        self,\n        test_cases: Sequence[TestCase],\n        adapter: AdapterProtocol,\n    ) -&gt; AgentRun:\n        \"\"\"Execute test cases against an adapter and collect results.\n\n        Args:\n            test_cases: The test cases to execute.\n            adapter: The agent adapter to test.\n\n        Returns:\n            An AgentRun with all results.\n        \"\"\"\n        start = time.monotonic()\n        results: list[TestResult] = []\n\n        if self._config.runner.parallel:\n            results = await self._run_parallel(list(test_cases), adapter)\n        else:\n            results = await self._run_sequential(list(test_cases), adapter)\n\n        elapsed_ms = int((time.monotonic() - start) * 1000)\n        passed = sum(1 for r in results if r.status == TestStatus.PASSED)\n        failed = sum(1 for r in results if r.status == TestStatus.FAILED)\n        errors = sum(1 for r in results if r.status == TestStatus.ERROR)\n        skipped = sum(1 for r in results if r.status == TestStatus.SKIPPED)\n\n        status = RunStatus.COMPLETED if errors == 0 else RunStatus.FAILED\n\n        return AgentRun(\n            agent_name=adapter.name,\n            status=status,\n            test_results=tuple(results),\n            total_tests=len(results),\n            passed=passed,\n            failed=failed,\n            errors=errors,\n            skipped=skipped,\n            duration_ms=elapsed_ms,\n        )\n\n    async def _run_sequential(\n        self,\n        test_cases: list[TestCase],\n        adapter: AdapterProtocol,\n    ) -&gt; list[TestResult]:\n        \"\"\"Execute tests one at a time.\"\"\"\n        results: list[TestResult] = []\n        for tc in test_cases:\n            result = await self._execute_single(tc, adapter)\n            results.append(result)\n        return results\n\n    async def _run_parallel(\n        self,\n        test_cases: list[TestCase],\n        adapter: AdapterProtocol,\n    ) -&gt; list[TestResult]:\n        \"\"\"Execute tests concurrently with a semaphore limit.\"\"\"\n        semaphore = asyncio.Semaphore(self._config.runner.max_workers)\n        results: list[TestResult] = [None] * len(test_cases)  # type: ignore[list-item]\n\n        async def _run_with_semaphore(idx: int, tc: TestCase) -&gt; None:\n            async with semaphore:\n                results[idx] = await self._execute_single(tc, adapter)\n\n        async with asyncio.TaskGroup() as tg:\n            for i, tc in enumerate(test_cases):\n                tg.create_task(_run_with_semaphore(i, tc))\n\n        return results\n\n    async def _execute_single(\n        self,\n        test_case: TestCase,\n        adapter: AdapterProtocol,\n    ) -&gt; TestResult:\n        \"\"\"Execute a single test case with timeout and error handling.\n\n        Args:\n            test_case: The test to execute.\n            adapter: The agent adapter.\n\n        Returns:\n            A TestResult reflecting the outcome.\n        \"\"\"\n        start = time.monotonic()\n        timeout = test_case.timeout_seconds or self._config.runner.default_timeout\n\n        try:\n            trace = await asyncio.wait_for(\n                adapter.invoke(test_case.input_text),\n                timeout=timeout,\n            )\n        except TimeoutError:\n            elapsed_ms = int((time.monotonic() - start) * 1000)\n            logger.warning(\"Test '%s' timed out after %.1fs\", test_case.name, timeout)\n            return TestResult(\n                test_name=test_case.name,\n                status=TestStatus.TIMEOUT,\n                duration_ms=elapsed_ms,\n                error_message=f\"Timed out after {timeout}s\",\n            )\n        except Exception as exc:\n            elapsed_ms = int((time.monotonic() - start) * 1000)\n            logger.error(\"Test '%s' errored: %s\", test_case.name, exc)\n            return TestResult(\n                test_name=test_case.name,\n                status=TestStatus.ERROR,\n                duration_ms=elapsed_ms,\n                error_message=str(exc),\n            )\n\n        eval_results = await self._run_evaluators(test_case, trace)\n        elapsed_ms = int((time.monotonic() - start) * 1000)\n\n        if eval_results:\n            avg_score = sum(r.score for r in eval_results) / len(eval_results)\n            all_passed = all(r.verdict.value in (\"pass\", \"partial\") for r in eval_results)\n        else:\n            avg_score = 1.0\n            all_passed = True\n\n        status = TestStatus.PASSED if all_passed else TestStatus.FAILED\n\n        return TestResult(\n            test_name=test_case.name,\n            status=status,\n            score=avg_score,\n            duration_ms=elapsed_ms,\n            trace=trace,\n            eval_results=tuple(eval_results),\n        )\n\n    async def _run_evaluators(\n        self,\n        test_case: TestCase,\n        trace: Trace,\n    ) -&gt; list[EvalResult]:\n        \"\"\"Run all evaluators against a test result.\n\n        Args:\n            test_case: The test case.\n            trace: The execution trace.\n\n        Returns:\n            List of evaluation results.\n        \"\"\"\n        results: list[EvalResult] = []\n        for evaluator in self._evaluators:\n            try:\n                result = await evaluator.evaluate(test_case, trace)\n                results.append(result)\n            except Exception:\n                logger.exception(\n                    \"Evaluator '%s' failed for test '%s'\",\n                    evaluator.name,\n                    test_case.name,\n                )\n        return results\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.runner.TestRunner.__init__","title":"<code>__init__(config=None, evaluators=None)</code>","text":"<p>Initialize the test runner.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AgentProbeConfig | None</code> <p>AgentProbe configuration. Uses defaults if None.</p> <code>None</code> <code>evaluators</code> <code>list[EvaluatorProtocol] | None</code> <p>Evaluators to apply to test results.</p> <code>None</code> Source code in <code>src/agentprobe/core/runner.py</code> <pre><code>def __init__(\n    self,\n    config: AgentProbeConfig | None = None,\n    evaluators: list[EvaluatorProtocol] | None = None,\n) -&gt; None:\n    \"\"\"Initialize the test runner.\n\n    Args:\n        config: AgentProbe configuration. Uses defaults if None.\n        evaluators: Evaluators to apply to test results.\n    \"\"\"\n    self._config = config or AgentProbeConfig()\n    self._evaluators = evaluators or []\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.runner.TestRunner.run","title":"<code>run(test_cases, adapter)</code>  <code>async</code>","text":"<p>Execute test cases against an adapter and collect results.</p> <p>Parameters:</p> Name Type Description Default <code>test_cases</code> <code>Sequence[TestCase]</code> <p>The test cases to execute.</p> required <code>adapter</code> <code>AdapterProtocol</code> <p>The agent adapter to test.</p> required <p>Returns:</p> Type Description <code>AgentRun</code> <p>An AgentRun with all results.</p> Source code in <code>src/agentprobe/core/runner.py</code> <pre><code>async def run(\n    self,\n    test_cases: Sequence[TestCase],\n    adapter: AdapterProtocol,\n) -&gt; AgentRun:\n    \"\"\"Execute test cases against an adapter and collect results.\n\n    Args:\n        test_cases: The test cases to execute.\n        adapter: The agent adapter to test.\n\n    Returns:\n        An AgentRun with all results.\n    \"\"\"\n    start = time.monotonic()\n    results: list[TestResult] = []\n\n    if self._config.runner.parallel:\n        results = await self._run_parallel(list(test_cases), adapter)\n    else:\n        results = await self._run_sequential(list(test_cases), adapter)\n\n    elapsed_ms = int((time.monotonic() - start) * 1000)\n    passed = sum(1 for r in results if r.status == TestStatus.PASSED)\n    failed = sum(1 for r in results if r.status == TestStatus.FAILED)\n    errors = sum(1 for r in results if r.status == TestStatus.ERROR)\n    skipped = sum(1 for r in results if r.status == TestStatus.SKIPPED)\n\n    status = RunStatus.COMPLETED if errors == 0 else RunStatus.FAILED\n\n    return AgentRun(\n        agent_name=adapter.name,\n        status=status,\n        test_results=tuple(results),\n        total_tests=len(results),\n        passed=passed,\n        failed=failed,\n        errors=errors,\n        skipped=skipped,\n        duration_ms=elapsed_ms,\n    )\n</code></pre>"},{"location":"reference/api/core/#assertions","title":"Assertions","text":""},{"location":"reference/api/core/#agentprobe.core.assertions","title":"<code>agentprobe.core.assertions</code>","text":"<p>Fluent assertion API for validating agent outputs and tool calls.</p> <p>Provides <code>expect()</code> and <code>expect_tool_calls()</code> entry points that return chainable expectation objects.</p>"},{"location":"reference/api/core/#agentprobe.core.assertions.OutputExpectation","title":"<code>OutputExpectation</code>","text":"<p>Fluent expectation chain for validating string output.</p> <p>Each assertion method returns <code>self</code> for chaining. Results accumulate in <code>results</code> and can be checked with <code>all_passed()</code>.</p> Example <pre><code>expect(output).to_contain(\"hello\").to_not_contain(\"error\")\n</code></pre> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>class OutputExpectation:\n    \"\"\"Fluent expectation chain for validating string output.\n\n    Each assertion method returns ``self`` for chaining. Results\n    accumulate in ``results`` and can be checked with ``all_passed()``.\n\n    Example:\n        ```python\n        expect(output).to_contain(\"hello\").to_not_contain(\"error\")\n        ```\n    \"\"\"\n\n    def __init__(self, output: str) -&gt; None:\n        self._output = output\n        self.results: list[AssertionResult] = []\n\n    def _record(\n        self,\n        assertion_type: str,\n        passed: bool,\n        expected: object,\n        actual: object,\n        message: str = \"\",\n    ) -&gt; OutputExpectation:\n        self.results.append(\n            AssertionResult(\n                assertion_type=assertion_type,\n                passed=passed,\n                expected=expected,\n                actual=actual,\n                message=message,\n            )\n        )\n        if not passed:\n            raise AssertionFailedError(\n                assertion_type=assertion_type,\n                expected=expected,\n                actual=actual,\n                message=message or None,\n            )\n        return self\n\n    def to_contain(self, substring: str) -&gt; OutputExpectation:\n        \"\"\"Assert that the output contains the given substring.\n\n        Args:\n            substring: The substring to search for.\n\n        Returns:\n            Self for chaining.\n        \"\"\"\n        found = substring in self._output\n        return self._record(\n            \"contain\",\n            found,\n            substring,\n            self._output[:200],\n            f\"Expected output to contain '{substring}'\" if not found else \"\",\n        )\n\n    def to_not_contain(self, substring: str) -&gt; OutputExpectation:\n        \"\"\"Assert that the output does NOT contain the given substring.\n\n        Args:\n            substring: The substring that should not appear.\n\n        Returns:\n            Self for chaining.\n        \"\"\"\n        found = substring not in self._output\n        return self._record(\n            \"not_contain\",\n            found,\n            substring,\n            self._output[:200],\n            f\"Expected output to not contain '{substring}'\" if not found else \"\",\n        )\n\n    def to_match(self, pattern: str) -&gt; OutputExpectation:\n        \"\"\"Assert that the output matches a regex pattern.\n\n        Args:\n            pattern: Regular expression pattern.\n\n        Returns:\n            Self for chaining.\n        \"\"\"\n        matched = re.search(pattern, self._output) is not None\n        return self._record(\n            \"match\",\n            matched,\n            pattern,\n            self._output[:200],\n            f\"Expected output to match pattern '{pattern}'\" if not matched else \"\",\n        )\n\n    def to_have_length_less_than(self, max_length: int) -&gt; OutputExpectation:\n        \"\"\"Assert that the output length is less than the given value.\n\n        Args:\n            max_length: Maximum allowed length.\n\n        Returns:\n            Self for chaining.\n        \"\"\"\n        actual_len = len(self._output)\n        passed = actual_len &lt; max_length\n        return self._record(\n            \"length_less_than\",\n            passed,\n            max_length,\n            actual_len,\n            f\"Expected length &lt; {max_length}, got {actual_len}\" if not passed else \"\",\n        )\n\n    def to_be_valid_json(self) -&gt; OutputExpectation:\n        \"\"\"Assert that the output is valid JSON.\n\n        Returns:\n            Self for chaining.\n        \"\"\"\n        try:\n            json.loads(self._output)\n            valid = True\n        except (json.JSONDecodeError, TypeError):\n            valid = False\n        return self._record(\n            \"valid_json\",\n            valid,\n            \"valid JSON\",\n            self._output[:200],\n            \"Expected output to be valid JSON\" if not valid else \"\",\n        )\n\n    def to_contain_any_of(self, substrings: Sequence[str]) -&gt; OutputExpectation:\n        \"\"\"Assert that the output contains at least one of the substrings.\n\n        Args:\n            substrings: Substrings to check for.\n\n        Returns:\n            Self for chaining.\n        \"\"\"\n        found = any(s in self._output for s in substrings)\n        return self._record(\n            \"contain_any_of\",\n            found,\n            list(substrings),\n            self._output[:200],\n            f\"Expected output to contain one of {list(substrings)}\" if not found else \"\",\n        )\n\n    def all_passed(self) -&gt; bool:\n        \"\"\"Return True if all recorded assertions passed.\"\"\"\n        return all(r.passed for r in self.results)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.OutputExpectation.to_contain","title":"<code>to_contain(substring)</code>","text":"<p>Assert that the output contains the given substring.</p> <p>Parameters:</p> Name Type Description Default <code>substring</code> <code>str</code> <p>The substring to search for.</p> required <p>Returns:</p> Type Description <code>OutputExpectation</code> <p>Self for chaining.</p> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>def to_contain(self, substring: str) -&gt; OutputExpectation:\n    \"\"\"Assert that the output contains the given substring.\n\n    Args:\n        substring: The substring to search for.\n\n    Returns:\n        Self for chaining.\n    \"\"\"\n    found = substring in self._output\n    return self._record(\n        \"contain\",\n        found,\n        substring,\n        self._output[:200],\n        f\"Expected output to contain '{substring}'\" if not found else \"\",\n    )\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.OutputExpectation.to_not_contain","title":"<code>to_not_contain(substring)</code>","text":"<p>Assert that the output does NOT contain the given substring.</p> <p>Parameters:</p> Name Type Description Default <code>substring</code> <code>str</code> <p>The substring that should not appear.</p> required <p>Returns:</p> Type Description <code>OutputExpectation</code> <p>Self for chaining.</p> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>def to_not_contain(self, substring: str) -&gt; OutputExpectation:\n    \"\"\"Assert that the output does NOT contain the given substring.\n\n    Args:\n        substring: The substring that should not appear.\n\n    Returns:\n        Self for chaining.\n    \"\"\"\n    found = substring not in self._output\n    return self._record(\n        \"not_contain\",\n        found,\n        substring,\n        self._output[:200],\n        f\"Expected output to not contain '{substring}'\" if not found else \"\",\n    )\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.OutputExpectation.to_match","title":"<code>to_match(pattern)</code>","text":"<p>Assert that the output matches a regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regular expression pattern.</p> required <p>Returns:</p> Type Description <code>OutputExpectation</code> <p>Self for chaining.</p> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>def to_match(self, pattern: str) -&gt; OutputExpectation:\n    \"\"\"Assert that the output matches a regex pattern.\n\n    Args:\n        pattern: Regular expression pattern.\n\n    Returns:\n        Self for chaining.\n    \"\"\"\n    matched = re.search(pattern, self._output) is not None\n    return self._record(\n        \"match\",\n        matched,\n        pattern,\n        self._output[:200],\n        f\"Expected output to match pattern '{pattern}'\" if not matched else \"\",\n    )\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.OutputExpectation.to_have_length_less_than","title":"<code>to_have_length_less_than(max_length)</code>","text":"<p>Assert that the output length is less than the given value.</p> <p>Parameters:</p> Name Type Description Default <code>max_length</code> <code>int</code> <p>Maximum allowed length.</p> required <p>Returns:</p> Type Description <code>OutputExpectation</code> <p>Self for chaining.</p> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>def to_have_length_less_than(self, max_length: int) -&gt; OutputExpectation:\n    \"\"\"Assert that the output length is less than the given value.\n\n    Args:\n        max_length: Maximum allowed length.\n\n    Returns:\n        Self for chaining.\n    \"\"\"\n    actual_len = len(self._output)\n    passed = actual_len &lt; max_length\n    return self._record(\n        \"length_less_than\",\n        passed,\n        max_length,\n        actual_len,\n        f\"Expected length &lt; {max_length}, got {actual_len}\" if not passed else \"\",\n    )\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.OutputExpectation.to_be_valid_json","title":"<code>to_be_valid_json()</code>","text":"<p>Assert that the output is valid JSON.</p> <p>Returns:</p> Type Description <code>OutputExpectation</code> <p>Self for chaining.</p> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>def to_be_valid_json(self) -&gt; OutputExpectation:\n    \"\"\"Assert that the output is valid JSON.\n\n    Returns:\n        Self for chaining.\n    \"\"\"\n    try:\n        json.loads(self._output)\n        valid = True\n    except (json.JSONDecodeError, TypeError):\n        valid = False\n    return self._record(\n        \"valid_json\",\n        valid,\n        \"valid JSON\",\n        self._output[:200],\n        \"Expected output to be valid JSON\" if not valid else \"\",\n    )\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.OutputExpectation.to_contain_any_of","title":"<code>to_contain_any_of(substrings)</code>","text":"<p>Assert that the output contains at least one of the substrings.</p> <p>Parameters:</p> Name Type Description Default <code>substrings</code> <code>Sequence[str]</code> <p>Substrings to check for.</p> required <p>Returns:</p> Type Description <code>OutputExpectation</code> <p>Self for chaining.</p> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>def to_contain_any_of(self, substrings: Sequence[str]) -&gt; OutputExpectation:\n    \"\"\"Assert that the output contains at least one of the substrings.\n\n    Args:\n        substrings: Substrings to check for.\n\n    Returns:\n        Self for chaining.\n    \"\"\"\n    found = any(s in self._output for s in substrings)\n    return self._record(\n        \"contain_any_of\",\n        found,\n        list(substrings),\n        self._output[:200],\n        f\"Expected output to contain one of {list(substrings)}\" if not found else \"\",\n    )\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.OutputExpectation.all_passed","title":"<code>all_passed()</code>","text":"<p>Return True if all recorded assertions passed.</p> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>def all_passed(self) -&gt; bool:\n    \"\"\"Return True if all recorded assertions passed.\"\"\"\n    return all(r.passed for r in self.results)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.ToolCallExpectation","title":"<code>ToolCallExpectation</code>","text":"<p>Fluent expectation chain for validating tool call sequences.</p> Example <pre><code>expect_tool_calls(trace.tool_calls).to_contain(\"search\").to_have_count(2)\n</code></pre> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>class ToolCallExpectation:\n    \"\"\"Fluent expectation chain for validating tool call sequences.\n\n    Example:\n        ```python\n        expect_tool_calls(trace.tool_calls).to_contain(\"search\").to_have_count(2)\n        ```\n    \"\"\"\n\n    def __init__(self, tool_calls: Sequence[ToolCall]) -&gt; None:\n        self._tool_calls = list(tool_calls)\n        self._names = [tc.tool_name for tc in self._tool_calls]\n        self.results: list[AssertionResult] = []\n\n    def _record(\n        self,\n        assertion_type: str,\n        passed: bool,\n        expected: object,\n        actual: object,\n        message: str = \"\",\n    ) -&gt; ToolCallExpectation:\n        self.results.append(\n            AssertionResult(\n                assertion_type=assertion_type,\n                passed=passed,\n                expected=expected,\n                actual=actual,\n                message=message,\n            )\n        )\n        if not passed:\n            raise AssertionFailedError(\n                assertion_type=assertion_type,\n                expected=expected,\n                actual=actual,\n                message=message or None,\n            )\n        return self\n\n    def to_contain(self, tool_name: str) -&gt; ToolCallExpectation:\n        \"\"\"Assert that a tool with the given name was called.\n\n        Args:\n            tool_name: The expected tool name.\n\n        Returns:\n            Self for chaining.\n        \"\"\"\n        found = tool_name in self._names\n        return self._record(\n            \"tool_contain\",\n            found,\n            tool_name,\n            self._names,\n            f\"Expected tool '{tool_name}' in calls {self._names}\" if not found else \"\",\n        )\n\n    def to_have_sequence(self, expected_sequence: Sequence[str]) -&gt; ToolCallExpectation:\n        \"\"\"Assert that tools were called in the given order.\n\n        The expected sequence must appear as a contiguous subsequence\n        in the actual tool call names.\n\n        Args:\n            expected_sequence: Ordered tool names to match.\n\n        Returns:\n            Self for chaining.\n        \"\"\"\n        expected = list(expected_sequence)\n        seq_len = len(expected)\n        found = (\n            any(\n                self._names[i : i + seq_len] == expected\n                for i in range(len(self._names) - seq_len + 1)\n            )\n            if seq_len &lt;= len(self._names)\n            else False\n        )\n        return self._record(\n            \"tool_sequence\",\n            found,\n            expected,\n            self._names,\n            f\"Expected sequence {expected} in calls {self._names}\" if not found else \"\",\n        )\n\n    def to_have_count(self, count: int) -&gt; ToolCallExpectation:\n        \"\"\"Assert the total number of tool calls.\n\n        Args:\n            count: Expected number of tool calls.\n\n        Returns:\n            Self for chaining.\n        \"\"\"\n        actual = len(self._tool_calls)\n        passed = actual == count\n        return self._record(\n            \"tool_count\",\n            passed,\n            count,\n            actual,\n            f\"Expected {count} tool calls, got {actual}\" if not passed else \"\",\n        )\n\n    def all_passed(self) -&gt; bool:\n        \"\"\"Return True if all recorded assertions passed.\"\"\"\n        return all(r.passed for r in self.results)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.ToolCallExpectation.to_contain","title":"<code>to_contain(tool_name)</code>","text":"<p>Assert that a tool with the given name was called.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The expected tool name.</p> required <p>Returns:</p> Type Description <code>ToolCallExpectation</code> <p>Self for chaining.</p> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>def to_contain(self, tool_name: str) -&gt; ToolCallExpectation:\n    \"\"\"Assert that a tool with the given name was called.\n\n    Args:\n        tool_name: The expected tool name.\n\n    Returns:\n        Self for chaining.\n    \"\"\"\n    found = tool_name in self._names\n    return self._record(\n        \"tool_contain\",\n        found,\n        tool_name,\n        self._names,\n        f\"Expected tool '{tool_name}' in calls {self._names}\" if not found else \"\",\n    )\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.ToolCallExpectation.to_have_sequence","title":"<code>to_have_sequence(expected_sequence)</code>","text":"<p>Assert that tools were called in the given order.</p> <p>The expected sequence must appear as a contiguous subsequence in the actual tool call names.</p> <p>Parameters:</p> Name Type Description Default <code>expected_sequence</code> <code>Sequence[str]</code> <p>Ordered tool names to match.</p> required <p>Returns:</p> Type Description <code>ToolCallExpectation</code> <p>Self for chaining.</p> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>def to_have_sequence(self, expected_sequence: Sequence[str]) -&gt; ToolCallExpectation:\n    \"\"\"Assert that tools were called in the given order.\n\n    The expected sequence must appear as a contiguous subsequence\n    in the actual tool call names.\n\n    Args:\n        expected_sequence: Ordered tool names to match.\n\n    Returns:\n        Self for chaining.\n    \"\"\"\n    expected = list(expected_sequence)\n    seq_len = len(expected)\n    found = (\n        any(\n            self._names[i : i + seq_len] == expected\n            for i in range(len(self._names) - seq_len + 1)\n        )\n        if seq_len &lt;= len(self._names)\n        else False\n    )\n    return self._record(\n        \"tool_sequence\",\n        found,\n        expected,\n        self._names,\n        f\"Expected sequence {expected} in calls {self._names}\" if not found else \"\",\n    )\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.ToolCallExpectation.to_have_count","title":"<code>to_have_count(count)</code>","text":"<p>Assert the total number of tool calls.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Expected number of tool calls.</p> required <p>Returns:</p> Type Description <code>ToolCallExpectation</code> <p>Self for chaining.</p> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>def to_have_count(self, count: int) -&gt; ToolCallExpectation:\n    \"\"\"Assert the total number of tool calls.\n\n    Args:\n        count: Expected number of tool calls.\n\n    Returns:\n        Self for chaining.\n    \"\"\"\n    actual = len(self._tool_calls)\n    passed = actual == count\n    return self._record(\n        \"tool_count\",\n        passed,\n        count,\n        actual,\n        f\"Expected {count} tool calls, got {actual}\" if not passed else \"\",\n    )\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.ToolCallExpectation.all_passed","title":"<code>all_passed()</code>","text":"<p>Return True if all recorded assertions passed.</p> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>def all_passed(self) -&gt; bool:\n    \"\"\"Return True if all recorded assertions passed.\"\"\"\n    return all(r.passed for r in self.results)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.expect","title":"<code>expect(output)</code>","text":"<p>Create a fluent output expectation.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The agent output string to validate.</p> required <p>Returns:</p> Type Description <code>OutputExpectation</code> <p>An OutputExpectation for chaining assertions.</p> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>def expect(output: str) -&gt; OutputExpectation:\n    \"\"\"Create a fluent output expectation.\n\n    Args:\n        output: The agent output string to validate.\n\n    Returns:\n        An OutputExpectation for chaining assertions.\n    \"\"\"\n    return OutputExpectation(output)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.assertions.expect_tool_calls","title":"<code>expect_tool_calls(tool_calls)</code>","text":"<p>Create a fluent tool call expectation.</p> <p>Parameters:</p> Name Type Description Default <code>tool_calls</code> <code>Sequence[ToolCall]</code> <p>The sequence of tool calls to validate.</p> required <p>Returns:</p> Type Description <code>ToolCallExpectation</code> <p>A ToolCallExpectation for chaining assertions.</p> Source code in <code>src/agentprobe/core/assertions.py</code> <pre><code>def expect_tool_calls(tool_calls: Sequence[ToolCall]) -&gt; ToolCallExpectation:\n    \"\"\"Create a fluent tool call expectation.\n\n    Args:\n        tool_calls: The sequence of tool calls to validate.\n\n    Returns:\n        A ToolCallExpectation for chaining assertions.\n    \"\"\"\n    return ToolCallExpectation(tool_calls)\n</code></pre>"},{"location":"reference/api/core/#scenario-decorator","title":"Scenario Decorator","text":""},{"location":"reference/api/core/#agentprobe.core.scenario","title":"<code>agentprobe.core.scenario</code>","text":"<p>Scenario decorator and registry for defining agent test cases.</p> <p>The <code>@scenario</code> decorator marks functions as test scenarios and registers them in a global registry for discovery by the test runner.</p>"},{"location":"reference/api/core/#agentprobe.core.scenario.scenario","title":"<code>scenario(name=None, *, input_text='', expected_output=None, tags=None, timeout=30.0, evaluators=None)</code>","text":"<p>Decorator that registers a function as a test scenario.</p> <p>The decorated function can optionally accept a <code>TestCase</code> argument and mutate it (e.g. setting dynamic input). If it returns a string, that string overrides <code>input_text</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Test name. Defaults to the function name.</p> <code>None</code> <code>input_text</code> <code>str</code> <p>The input prompt to send to the agent.</p> <code>''</code> <code>expected_output</code> <code>str | None</code> <p>Optional expected output for comparison.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Tags for filtering and grouping.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Maximum execution time in seconds.</p> <code>30.0</code> <code>evaluators</code> <code>list[str] | None</code> <p>Names of evaluators to run.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[Callable[..., Any]], Callable[..., Any]]</code> <p>A decorator that registers the function.</p> Example <pre><code>@scenario(name=\"greeting_test\", input_text=\"Hello!\")\ndef test_greeting():\n    pass\n</code></pre> Source code in <code>src/agentprobe/core/scenario.py</code> <pre><code>def scenario(\n    name: str | None = None,\n    *,\n    input_text: str = \"\",\n    expected_output: str | None = None,\n    tags: list[str] | None = None,\n    timeout: float = 30.0,\n    evaluators: list[str] | None = None,\n) -&gt; Callable[[Callable[..., Any]], Callable[..., Any]]:\n    \"\"\"Decorator that registers a function as a test scenario.\n\n    The decorated function can optionally accept a ``TestCase`` argument\n    and mutate it (e.g. setting dynamic input). If it returns a string,\n    that string overrides ``input_text``.\n\n    Args:\n        name: Test name. Defaults to the function name.\n        input_text: The input prompt to send to the agent.\n        expected_output: Optional expected output for comparison.\n        tags: Tags for filtering and grouping.\n        timeout: Maximum execution time in seconds.\n        evaluators: Names of evaluators to run.\n\n    Returns:\n        A decorator that registers the function.\n\n    Example:\n        ```python\n        @scenario(name=\"greeting_test\", input_text=\"Hello!\")\n        def test_greeting():\n            pass\n        ```\n    \"\"\"\n\n    def decorator(func: Callable[..., Any]) -&gt; Callable[..., Any]:\n        resolved_name = name or func.__name__\n        test_case = TestCase(\n            name=resolved_name,\n            input_text=input_text,\n            expected_output=expected_output,\n            tags=tags or [],\n            timeout_seconds=timeout,\n            evaluators=evaluators or [],\n            metadata={\"source_function\": func.__qualname__},\n        )\n\n        module = func.__module__\n        _scenario_registry.setdefault(module, []).append(test_case)\n\n        @functools.wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n            return func(*args, **kwargs)\n\n        wrapper._agentprobe_scenario = test_case  # type: ignore[attr-defined]\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.scenario.get_scenarios","title":"<code>get_scenarios(module_name=None)</code>","text":"<p>Retrieve registered scenarios.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str | None</code> <p>If provided, return scenarios from this module only. If None, return all registered scenarios.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TestCase]</code> <p>A list of TestCase objects.</p> Source code in <code>src/agentprobe/core/scenario.py</code> <pre><code>def get_scenarios(module_name: str | None = None) -&gt; list[TestCase]:\n    \"\"\"Retrieve registered scenarios.\n\n    Args:\n        module_name: If provided, return scenarios from this module only.\n            If None, return all registered scenarios.\n\n    Returns:\n        A list of TestCase objects.\n    \"\"\"\n    if module_name is not None:\n        return list(_scenario_registry.get(module_name, []))\n    return [tc for cases in _scenario_registry.values() for tc in cases]\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.scenario.clear_registry","title":"<code>clear_registry()</code>","text":"<p>Clear all registered scenarios. Primarily for testing.</p> Source code in <code>src/agentprobe/core/scenario.py</code> <pre><code>def clear_registry() -&gt; None:\n    \"\"\"Clear all registered scenarios. Primarily for testing.\"\"\"\n    _scenario_registry.clear()\n</code></pre>"},{"location":"reference/api/core/#configuration","title":"Configuration","text":""},{"location":"reference/api/core/#agentprobe.core.config","title":"<code>agentprobe.core.config</code>","text":"<p>Configuration loading and validation for AgentProbe.</p> <p>Loads configuration from <code>agentprobe.yaml</code> with support for <code>${ENV_VAR}</code> interpolation and sensible defaults.</p>"},{"location":"reference/api/core/#agentprobe.core.config.RunnerConfig","title":"<code>RunnerConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the test runner.</p> <p>Attributes:</p> Name Type Description <code>parallel</code> <code>bool</code> <p>Whether to run tests in parallel.</p> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent tests.</p> <code>default_timeout</code> <code>float</code> <p>Default test timeout in seconds.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class RunnerConfig(BaseModel):\n    \"\"\"Configuration for the test runner.\n\n    Attributes:\n        parallel: Whether to run tests in parallel.\n        max_workers: Maximum number of concurrent tests.\n        default_timeout: Default test timeout in seconds.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    parallel: bool = False\n    max_workers: int = Field(default=4, ge=1)\n    default_timeout: float = Field(default=30.0, gt=0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.EvalConfig","title":"<code>EvalConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for evaluators.</p> <p>Attributes:</p> Name Type Description <code>default_evaluators</code> <code>list[str]</code> <p>Evaluator names to apply to all tests.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class EvalConfig(BaseModel):\n    \"\"\"Configuration for evaluators.\n\n    Attributes:\n        default_evaluators: Evaluator names to apply to all tests.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    default_evaluators: list[str] = Field(default_factory=list)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.JudgeConfig","title":"<code>JudgeConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the judge evaluator.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>Model to use for judging.</p> <code>provider</code> <code>str</code> <p>API provider name.</p> <code>temperature</code> <code>float</code> <p>Sampling temperature.</p> <code>max_tokens</code> <code>int</code> <p>Maximum response tokens.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class JudgeConfig(BaseModel):\n    \"\"\"Configuration for the judge evaluator.\n\n    Attributes:\n        model: Model to use for judging.\n        provider: API provider name.\n        temperature: Sampling temperature.\n        max_tokens: Maximum response tokens.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    model: str = \"claude-sonnet-4-5-20250929\"\n    provider: str = \"anthropic\"\n    temperature: float = Field(default=0.0, ge=0.0, le=2.0)\n    max_tokens: int = Field(default=1024, ge=1)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.TraceConfig","title":"<code>TraceConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for trace recording and storage.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether to record traces.</p> <code>storage_backend</code> <code>str</code> <p>Storage backend type.</p> <code>database_path</code> <code>str</code> <p>Path to SQLite database file.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class TraceConfig(BaseModel):\n    \"\"\"Configuration for trace recording and storage.\n\n    Attributes:\n        enabled: Whether to record traces.\n        storage_backend: Storage backend type.\n        database_path: Path to SQLite database file.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    enabled: bool = True\n    storage_backend: str = \"sqlite\"\n    database_path: str = \".agentprobe/traces.db\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.CostConfig","title":"<code>CostConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for cost tracking.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether to track costs.</p> <code>budget_limit_usd</code> <code>float | None</code> <p>Maximum allowed cost per run.</p> <code>pricing_dir</code> <code>str | None</code> <p>Directory containing pricing YAML files.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class CostConfig(BaseModel):\n    \"\"\"Configuration for cost tracking.\n\n    Attributes:\n        enabled: Whether to track costs.\n        budget_limit_usd: Maximum allowed cost per run.\n        pricing_dir: Directory containing pricing YAML files.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    enabled: bool = True\n    budget_limit_usd: float | None = None\n    pricing_dir: str | None = None\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.SafetyConfig","title":"<code>SafetyConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for safety testing.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether to run safety tests.</p> <code>suites</code> <code>list[str]</code> <p>List of safety suite names to run.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class SafetyConfig(BaseModel):\n    \"\"\"Configuration for safety testing.\n\n    Attributes:\n        enabled: Whether to run safety tests.\n        suites: List of safety suite names to run.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    enabled: bool = False\n    suites: list[str] = Field(default_factory=list)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.ChaosConfig","title":"<code>ChaosConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for chaos fault injection testing.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether chaos testing is enabled.</p> <code>seed</code> <code>int</code> <p>Random seed for deterministic fault injection.</p> <code>default_probability</code> <code>float</code> <p>Default probability of applying a fault.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class ChaosConfig(BaseModel):\n    \"\"\"Configuration for chaos fault injection testing.\n\n    Attributes:\n        enabled: Whether chaos testing is enabled.\n        seed: Random seed for deterministic fault injection.\n        default_probability: Default probability of applying a fault.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    enabled: bool = False\n    seed: int = 42\n    default_probability: float = Field(default=0.5, ge=0.0, le=1.0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.SnapshotConfig","title":"<code>SnapshotConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for snapshot/golden file testing.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether snapshot testing is enabled.</p> <code>snapshot_dir</code> <code>str</code> <p>Directory for storing snapshot files.</p> <code>update_on_first_run</code> <code>bool</code> <p>Whether to create snapshots on first run.</p> <code>threshold</code> <code>float</code> <p>Similarity threshold for snapshot matching.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class SnapshotConfig(BaseModel):\n    \"\"\"Configuration for snapshot/golden file testing.\n\n    Attributes:\n        enabled: Whether snapshot testing is enabled.\n        snapshot_dir: Directory for storing snapshot files.\n        update_on_first_run: Whether to create snapshots on first run.\n        threshold: Similarity threshold for snapshot matching.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    enabled: bool = False\n    snapshot_dir: str = \".agentprobe/snapshots\"\n    update_on_first_run: bool = True\n    threshold: float = Field(default=0.8, ge=0.0, le=1.0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.BudgetConfig","title":"<code>BudgetConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for per-test and per-suite cost budgets.</p> <p>Attributes:</p> Name Type Description <code>test_budget_usd</code> <code>float | None</code> <p>Maximum cost per individual test.</p> <code>suite_budget_usd</code> <code>float | None</code> <p>Maximum cost per test suite run.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class BudgetConfig(BaseModel):\n    \"\"\"Configuration for per-test and per-suite cost budgets.\n\n    Attributes:\n        test_budget_usd: Maximum cost per individual test.\n        suite_budget_usd: Maximum cost per test suite run.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    test_budget_usd: float | None = None\n    suite_budget_usd: float | None = None\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.RegressionConfig","title":"<code>RegressionConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for regression detection.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether regression detection is enabled.</p> <code>baseline_dir</code> <code>str</code> <p>Directory for storing baseline files.</p> <code>threshold</code> <code>float</code> <p>Score delta threshold for flagging regressions.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class RegressionConfig(BaseModel):\n    \"\"\"Configuration for regression detection.\n\n    Attributes:\n        enabled: Whether regression detection is enabled.\n        baseline_dir: Directory for storing baseline files.\n        threshold: Score delta threshold for flagging regressions.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    enabled: bool = False\n    baseline_dir: str = \".agentprobe/baselines\"\n    threshold: float = Field(default=0.05, ge=0.0, le=1.0)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.MetricsConfig","title":"<code>MetricsConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for metric collection and trending.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether metric collection is enabled.</p> <code>builtin_metrics</code> <code>bool</code> <p>Whether to collect built-in metrics automatically.</p> <code>trend_window</code> <code>int</code> <p>Number of recent runs to use for trend analysis.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class MetricsConfig(BaseModel):\n    \"\"\"Configuration for metric collection and trending.\n\n    Attributes:\n        enabled: Whether metric collection is enabled.\n        builtin_metrics: Whether to collect built-in metrics automatically.\n        trend_window: Number of recent runs to use for trend analysis.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    enabled: bool = True\n    builtin_metrics: bool = True\n    trend_window: int = Field(default=10, ge=2)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.PluginConfig","title":"<code>PluginConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the plugin system.</p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <p>Whether the plugin system is enabled.</p> <code>directories</code> <code>list[str]</code> <p>Additional directories to scan for plugins.</p> <code>entry_point_group</code> <code>str</code> <p>Entry point group name for plugin discovery.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class PluginConfig(BaseModel):\n    \"\"\"Configuration for the plugin system.\n\n    Attributes:\n        enabled: Whether the plugin system is enabled.\n        directories: Additional directories to scan for plugins.\n        entry_point_group: Entry point group name for plugin discovery.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    enabled: bool = True\n    directories: list[str] = Field(default_factory=list)\n    entry_point_group: str = \"agentprobe.plugins\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.ReportingConfig","title":"<code>ReportingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for result reporting.</p> <p>Attributes:</p> Name Type Description <code>formats</code> <code>list[str]</code> <p>Output format names.</p> <code>output_dir</code> <code>str</code> <p>Directory for report files.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class ReportingConfig(BaseModel):\n    \"\"\"Configuration for result reporting.\n\n    Attributes:\n        formats: Output format names.\n        output_dir: Directory for report files.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    formats: list[str] = Field(default_factory=lambda: [\"terminal\"])\n    output_dir: str = \"agentprobe-report\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.AgentProbeConfig","title":"<code>AgentProbeConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level AgentProbe configuration.</p> <p>Attributes:</p> Name Type Description <code>project_name</code> <code>str</code> <p>Name of the project being tested.</p> <code>test_dir</code> <code>str</code> <p>Directory containing test files.</p> <code>runner</code> <code>RunnerConfig</code> <p>Test runner configuration.</p> <code>eval</code> <code>EvalConfig</code> <p>Evaluator configuration.</p> <code>judge</code> <code>JudgeConfig</code> <p>Judge evaluator configuration.</p> <code>trace</code> <code>TraceConfig</code> <p>Trace recording configuration.</p> <code>cost</code> <code>CostConfig</code> <p>Cost tracking configuration.</p> <code>safety</code> <code>SafetyConfig</code> <p>Safety testing configuration.</p> <code>reporting</code> <code>ReportingConfig</code> <p>Reporting configuration.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>class AgentProbeConfig(BaseModel):\n    \"\"\"Top-level AgentProbe configuration.\n\n    Attributes:\n        project_name: Name of the project being tested.\n        test_dir: Directory containing test files.\n        runner: Test runner configuration.\n        eval: Evaluator configuration.\n        judge: Judge evaluator configuration.\n        trace: Trace recording configuration.\n        cost: Cost tracking configuration.\n        safety: Safety testing configuration.\n        reporting: Reporting configuration.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    project_name: str = \"agentprobe\"\n    test_dir: str = \"tests\"\n    runner: RunnerConfig = Field(default_factory=RunnerConfig)\n    eval: EvalConfig = Field(default_factory=EvalConfig)\n    judge: JudgeConfig = Field(default_factory=JudgeConfig)\n    trace: TraceConfig = Field(default_factory=TraceConfig)\n    cost: CostConfig = Field(default_factory=CostConfig)\n    safety: SafetyConfig = Field(default_factory=SafetyConfig)\n    reporting: ReportingConfig = Field(default_factory=ReportingConfig)\n    chaos: ChaosConfig = Field(default_factory=ChaosConfig)\n    snapshot: SnapshotConfig = Field(default_factory=SnapshotConfig)\n    budget: BudgetConfig = Field(default_factory=BudgetConfig)\n    regression: RegressionConfig = Field(default_factory=RegressionConfig)\n    metrics: MetricsConfig = Field(default_factory=MetricsConfig)\n    plugins: PluginConfig = Field(default_factory=PluginConfig)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.config.load_config","title":"<code>load_config(path=None)</code>","text":"<p>Load configuration from a YAML file.</p> <p>Searches for <code>agentprobe.yaml</code> or <code>agentprobe.yml</code> in the current directory if no path is provided. Returns default config if no file is found.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path | None</code> <p>Explicit path to a config file.</p> <code>None</code> <p>Returns:</p> Type Description <code>AgentProbeConfig</code> <p>A validated AgentProbeConfig instance.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the file exists but is invalid.</p> Source code in <code>src/agentprobe/core/config.py</code> <pre><code>def load_config(\n    path: str | Path | None = None,\n) -&gt; AgentProbeConfig:\n    \"\"\"Load configuration from a YAML file.\n\n    Searches for ``agentprobe.yaml`` or ``agentprobe.yml`` in the\n    current directory if no path is provided. Returns default config\n    if no file is found.\n\n    Args:\n        path: Explicit path to a config file.\n\n    Returns:\n        A validated AgentProbeConfig instance.\n\n    Raises:\n        ConfigError: If the file exists but is invalid.\n    \"\"\"\n    if path is not None:\n        config_path = Path(path)\n        if not config_path.exists():\n            raise ConfigError(f\"Config file not found: {config_path}\")\n    else:\n        for candidate in [\"agentprobe.yaml\", \"agentprobe.yml\"]:\n            config_path = Path(candidate)\n            if config_path.exists():\n                break\n        else:\n            logger.debug(\"No config file found, using defaults\")\n            return AgentProbeConfig()\n\n    logger.info(\"Loading config from %s\", config_path)\n    try:\n        raw = yaml.safe_load(config_path.read_text(encoding=\"utf-8\"))\n    except yaml.YAMLError as exc:\n        raise ConfigError(f\"Invalid YAML in {config_path}: {exc}\") from exc\n\n    if raw is None:\n        return AgentProbeConfig()\n\n    if not isinstance(raw, dict):\n        raise ConfigError(f\"Config file must be a YAML mapping, got {type(raw).__name__}\")\n\n    interpolated = _interpolate_recursive(raw)\n\n    try:\n        return AgentProbeConfig.model_validate(interpolated)\n    except Exception as exc:\n        raise ConfigError(f\"Invalid configuration: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/core/#protocols","title":"Protocols","text":""},{"location":"reference/api/core/#agentprobe.core.protocols","title":"<code>agentprobe.core.protocols</code>","text":"<p>Protocol definitions for AgentProbe's pluggable architecture.</p> <p>All protocols are runtime-checkable, allowing isinstance() verification of structural subtyping. Implementors do not need to inherit from these protocols \u2014 they only need to provide the required methods.</p>"},{"location":"reference/api/core/#agentprobe.core.protocols.AdapterProtocol","title":"<code>AdapterProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface for agent framework adapters.</p> <p>Adapters wrap specific agent frameworks (LangChain, CrewAI, etc.) and translate their execution into AgentProbe's Trace format.</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>@runtime_checkable\nclass AdapterProtocol(Protocol):\n    \"\"\"Interface for agent framework adapters.\n\n    Adapters wrap specific agent frameworks (LangChain, CrewAI, etc.)\n    and translate their execution into AgentProbe's Trace format.\n    \"\"\"\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the adapter name.\"\"\"\n        ...\n\n    async def invoke(\n        self,\n        input_text: str,\n        **kwargs: Any,\n    ) -&gt; Trace:\n        \"\"\"Invoke the agent with the given input and return a trace.\n\n        Args:\n            input_text: The input prompt to send to the agent.\n            **kwargs: Additional adapter-specific arguments.\n\n        Returns:\n            A complete execution trace.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.AdapterProtocol.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the adapter name.</p>"},{"location":"reference/api/core/#agentprobe.core.protocols.AdapterProtocol.invoke","title":"<code>invoke(input_text, **kwargs)</code>  <code>async</code>","text":"<p>Invoke the agent with the given input and return a trace.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The input prompt to send to the agent.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter-specific arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Trace</code> <p>A complete execution trace.</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>async def invoke(\n    self,\n    input_text: str,\n    **kwargs: Any,\n) -&gt; Trace:\n    \"\"\"Invoke the agent with the given input and return a trace.\n\n    Args:\n        input_text: The input prompt to send to the agent.\n        **kwargs: Additional adapter-specific arguments.\n\n    Returns:\n        A complete execution trace.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.EvaluatorProtocol","title":"<code>EvaluatorProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface for test result evaluators.</p> <p>Evaluators assess agent outputs against expectations, producing scored results with pass/fail verdicts.</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>@runtime_checkable\nclass EvaluatorProtocol(Protocol):\n    \"\"\"Interface for test result evaluators.\n\n    Evaluators assess agent outputs against expectations, producing\n    scored results with pass/fail verdicts.\n    \"\"\"\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the evaluator name.\"\"\"\n        ...\n\n    async def evaluate(\n        self,\n        test_case: TestCase,\n        trace: Trace,\n    ) -&gt; EvalResult:\n        \"\"\"Evaluate an agent's output for a given test case.\n\n        Args:\n            test_case: The test case that was executed.\n            trace: The execution trace to evaluate.\n\n        Returns:\n            An evaluation result with score and verdict.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.EvaluatorProtocol.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the evaluator name.</p>"},{"location":"reference/api/core/#agentprobe.core.protocols.EvaluatorProtocol.evaluate","title":"<code>evaluate(test_case, trace)</code>  <code>async</code>","text":"<p>Evaluate an agent's output for a given test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case that was executed.</p> required <code>trace</code> <code>Trace</code> <p>The execution trace to evaluate.</p> required <p>Returns:</p> Type Description <code>EvalResult</code> <p>An evaluation result with score and verdict.</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>async def evaluate(\n    self,\n    test_case: TestCase,\n    trace: Trace,\n) -&gt; EvalResult:\n    \"\"\"Evaluate an agent's output for a given test case.\n\n    Args:\n        test_case: The test case that was executed.\n        trace: The execution trace to evaluate.\n\n    Returns:\n        An evaluation result with score and verdict.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.StorageProtocol","title":"<code>StorageProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface for persistence backends.</p> <p>Storage implementations handle saving and loading traces, test results, and agent runs.</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>@runtime_checkable\nclass StorageProtocol(Protocol):\n    \"\"\"Interface for persistence backends.\n\n    Storage implementations handle saving and loading traces,\n    test results, and agent runs.\n    \"\"\"\n\n    async def setup(self) -&gt; None:\n        \"\"\"Initialize the storage backend (create tables, etc.).\"\"\"\n        ...\n\n    async def save_trace(self, trace: Trace) -&gt; None:\n        \"\"\"Persist a trace.\n\n        Args:\n            trace: The trace to save.\n        \"\"\"\n        ...\n\n    async def load_trace(self, trace_id: str) -&gt; Trace | None:\n        \"\"\"Load a trace by ID.\n\n        Args:\n            trace_id: The unique identifier of the trace.\n\n        Returns:\n            The trace if found, otherwise None.\n        \"\"\"\n        ...\n\n    async def list_traces(\n        self,\n        agent_name: str | None = None,\n        limit: int = 100,\n    ) -&gt; Sequence[Trace]:\n        \"\"\"List traces with optional filtering.\n\n        Args:\n            agent_name: Filter by agent name if provided.\n            limit: Maximum number of traces to return.\n\n        Returns:\n            A sequence of matching traces.\n        \"\"\"\n        ...\n\n    async def save_result(self, result: TestResult) -&gt; None:\n        \"\"\"Persist a test result.\n\n        Args:\n            result: The test result to save.\n        \"\"\"\n        ...\n\n    async def load_results(\n        self,\n        test_name: str | None = None,\n        limit: int = 100,\n    ) -&gt; Sequence[TestResult]:\n        \"\"\"Load test results with optional filtering.\n\n        Args:\n            test_name: Filter by test name if provided.\n            limit: Maximum number of results to return.\n\n        Returns:\n            A sequence of matching test results.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.StorageProtocol.setup","title":"<code>setup()</code>  <code>async</code>","text":"<p>Initialize the storage backend (create tables, etc.).</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>async def setup(self) -&gt; None:\n    \"\"\"Initialize the storage backend (create tables, etc.).\"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.StorageProtocol.save_trace","title":"<code>save_trace(trace)</code>  <code>async</code>","text":"<p>Persist a trace.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The trace to save.</p> required Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>async def save_trace(self, trace: Trace) -&gt; None:\n    \"\"\"Persist a trace.\n\n    Args:\n        trace: The trace to save.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.StorageProtocol.load_trace","title":"<code>load_trace(trace_id)</code>  <code>async</code>","text":"<p>Load a trace by ID.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>The unique identifier of the trace.</p> required <p>Returns:</p> Type Description <code>Trace | None</code> <p>The trace if found, otherwise None.</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>async def load_trace(self, trace_id: str) -&gt; Trace | None:\n    \"\"\"Load a trace by ID.\n\n    Args:\n        trace_id: The unique identifier of the trace.\n\n    Returns:\n        The trace if found, otherwise None.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.StorageProtocol.list_traces","title":"<code>list_traces(agent_name=None, limit=100)</code>  <code>async</code>","text":"<p>List traces with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str | None</code> <p>Filter by agent name if provided.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of traces to return.</p> <code>100</code> <p>Returns:</p> Type Description <code>Sequence[Trace]</code> <p>A sequence of matching traces.</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>async def list_traces(\n    self,\n    agent_name: str | None = None,\n    limit: int = 100,\n) -&gt; Sequence[Trace]:\n    \"\"\"List traces with optional filtering.\n\n    Args:\n        agent_name: Filter by agent name if provided.\n        limit: Maximum number of traces to return.\n\n    Returns:\n        A sequence of matching traces.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.StorageProtocol.save_result","title":"<code>save_result(result)</code>  <code>async</code>","text":"<p>Persist a test result.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>TestResult</code> <p>The test result to save.</p> required Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>async def save_result(self, result: TestResult) -&gt; None:\n    \"\"\"Persist a test result.\n\n    Args:\n        result: The test result to save.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.StorageProtocol.load_results","title":"<code>load_results(test_name=None, limit=100)</code>  <code>async</code>","text":"<p>Load test results with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>str | None</code> <p>Filter by test name if provided.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of results to return.</p> <code>100</code> <p>Returns:</p> Type Description <code>Sequence[TestResult]</code> <p>A sequence of matching test results.</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>async def load_results(\n    self,\n    test_name: str | None = None,\n    limit: int = 100,\n) -&gt; Sequence[TestResult]:\n    \"\"\"Load test results with optional filtering.\n\n    Args:\n        test_name: Filter by test name if provided.\n        limit: Maximum number of results to return.\n\n    Returns:\n        A sequence of matching test results.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.MetricStoreProtocol","title":"<code>MetricStoreProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface for metric persistence backends.</p> <p>Metric storage is optional and separate from the main StorageProtocol, allowing implementations to opt in to metric tracking independently.</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>@runtime_checkable\nclass MetricStoreProtocol(Protocol):\n    \"\"\"Interface for metric persistence backends.\n\n    Metric storage is optional and separate from the main StorageProtocol,\n    allowing implementations to opt in to metric tracking independently.\n    \"\"\"\n\n    async def save_metrics(self, metrics: Sequence[MetricValue]) -&gt; None:\n        \"\"\"Persist a batch of metric values.\n\n        Args:\n            metrics: The metric values to save.\n        \"\"\"\n        ...\n\n    async def load_metrics(\n        self,\n        metric_name: str | None = None,\n        limit: int = 1000,\n    ) -&gt; Sequence[MetricValue]:\n        \"\"\"Load metric values with optional filtering.\n\n        Args:\n            metric_name: Filter by metric name if provided.\n            limit: Maximum number of values to return.\n\n        Returns:\n            A sequence of matching metric values.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.MetricStoreProtocol.save_metrics","title":"<code>save_metrics(metrics)</code>  <code>async</code>","text":"<p>Persist a batch of metric values.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Sequence[MetricValue]</code> <p>The metric values to save.</p> required Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>async def save_metrics(self, metrics: Sequence[MetricValue]) -&gt; None:\n    \"\"\"Persist a batch of metric values.\n\n    Args:\n        metrics: The metric values to save.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.MetricStoreProtocol.load_metrics","title":"<code>load_metrics(metric_name=None, limit=1000)</code>  <code>async</code>","text":"<p>Load metric values with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str | None</code> <p>Filter by metric name if provided.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of values to return.</p> <code>1000</code> <p>Returns:</p> Type Description <code>Sequence[MetricValue]</code> <p>A sequence of matching metric values.</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>async def load_metrics(\n    self,\n    metric_name: str | None = None,\n    limit: int = 1000,\n) -&gt; Sequence[MetricValue]:\n    \"\"\"Load metric values with optional filtering.\n\n    Args:\n        metric_name: Filter by metric name if provided.\n        limit: Maximum number of values to return.\n\n    Returns:\n        A sequence of matching metric values.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.RunnerProtocol","title":"<code>RunnerProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface for test execution engines.</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>@runtime_checkable\nclass RunnerProtocol(Protocol):\n    \"\"\"Interface for test execution engines.\"\"\"\n\n    async def run(\n        self,\n        test_cases: Sequence[TestCase],\n        adapter: AdapterProtocol,\n    ) -&gt; AgentRun:\n        \"\"\"Execute a batch of test cases against an agent adapter.\n\n        Args:\n            test_cases: The test cases to execute.\n            adapter: The agent adapter to test against.\n\n        Returns:\n            An AgentRun containing all results.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.RunnerProtocol.run","title":"<code>run(test_cases, adapter)</code>  <code>async</code>","text":"<p>Execute a batch of test cases against an agent adapter.</p> <p>Parameters:</p> Name Type Description Default <code>test_cases</code> <code>Sequence[TestCase]</code> <p>The test cases to execute.</p> required <code>adapter</code> <code>AdapterProtocol</code> <p>The agent adapter to test against.</p> required <p>Returns:</p> Type Description <code>AgentRun</code> <p>An AgentRun containing all results.</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>async def run(\n    self,\n    test_cases: Sequence[TestCase],\n    adapter: AdapterProtocol,\n) -&gt; AgentRun:\n    \"\"\"Execute a batch of test cases against an agent adapter.\n\n    Args:\n        test_cases: The test cases to execute.\n        adapter: The agent adapter to test against.\n\n    Returns:\n        An AgentRun containing all results.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.ReporterProtocol","title":"<code>ReporterProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface for test result reporters.</p> Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>@runtime_checkable\nclass ReporterProtocol(Protocol):\n    \"\"\"Interface for test result reporters.\"\"\"\n\n    async def report(self, run: AgentRun) -&gt; None:\n        \"\"\"Generate and output a report for an agent run.\n\n        Args:\n            run: The completed agent run to report on.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.protocols.ReporterProtocol.report","title":"<code>report(run)</code>  <code>async</code>","text":"<p>Generate and output a report for an agent run.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>AgentRun</code> <p>The completed agent run to report on.</p> required Source code in <code>src/agentprobe/core/protocols.py</code> <pre><code>async def report(self, run: AgentRun) -&gt; None:\n    \"\"\"Generate and output a report for an agent run.\n\n    Args:\n        run: The completed agent run to report on.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/core/#exceptions","title":"Exceptions","text":""},{"location":"reference/api/core/#agentprobe.core.exceptions","title":"<code>agentprobe.core.exceptions</code>","text":"<p>Exception hierarchy for the AgentProbe framework.</p> <p>All exceptions inherit from AgentProbeError, allowing callers to catch the base type for generic error handling or specific subclasses for targeted recovery.</p>"},{"location":"reference/api/core/#agentprobe.core.exceptions.AgentProbeError","title":"<code>AgentProbeError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all AgentProbe errors.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class AgentProbeError(Exception):\n    \"\"\"Base exception for all AgentProbe errors.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.ConfigError","title":"<code>ConfigError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when configuration is invalid or missing.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class ConfigError(AgentProbeError):\n    \"\"\"Raised when configuration is invalid or missing.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.RunnerError","title":"<code>RunnerError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when the test runner encounters an execution failure.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class RunnerError(AgentProbeError):\n    \"\"\"Raised when the test runner encounters an execution failure.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.TestTimeoutError","title":"<code>TestTimeoutError</code>","text":"<p>               Bases: <code>RunnerError</code></p> <p>Raised when a test exceeds its configured timeout.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class TestTimeoutError(RunnerError):\n    \"\"\"Raised when a test exceeds its configured timeout.\"\"\"\n\n    def __init__(self, test_name: str, timeout_seconds: float) -&gt; None:\n        self.test_name = test_name\n        self.timeout_seconds = timeout_seconds\n        super().__init__(f\"Test '{test_name}' exceeded {timeout_seconds}s timeout\")\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.AdapterError","title":"<code>AdapterError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when an agent adapter fails during invocation.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class AdapterError(AgentProbeError):\n    \"\"\"Raised when an agent adapter fails during invocation.\"\"\"\n\n    def __init__(self, adapter_name: str, message: str) -&gt; None:\n        self.adapter_name = adapter_name\n        super().__init__(f\"Adapter '{adapter_name}' error: {message}\")\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.EvaluatorError","title":"<code>EvaluatorError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Base exception for evaluation errors.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class EvaluatorError(AgentProbeError):\n    \"\"\"Base exception for evaluation errors.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.JudgeAPIError","title":"<code>JudgeAPIError</code>","text":"<p>               Bases: <code>EvaluatorError</code></p> <p>Raised when the judge model API call fails.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class JudgeAPIError(EvaluatorError):\n    \"\"\"Raised when the judge model API call fails.\"\"\"\n\n    def __init__(self, model: str, status_code: int, message: str) -&gt; None:\n        self.model = model\n        self.status_code = status_code\n        super().__init__(f\"Judge API error ({model}): {status_code} \u2014 {message}\")\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.StorageError","title":"<code>StorageError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when a storage backend operation fails.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class StorageError(AgentProbeError):\n    \"\"\"Raised when a storage backend operation fails.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.TraceError","title":"<code>TraceError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when trace recording or processing fails.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class TraceError(AgentProbeError):\n    \"\"\"Raised when trace recording or processing fails.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.CostError","title":"<code>CostError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when cost calculation encounters an error.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class CostError(AgentProbeError):\n    \"\"\"Raised when cost calculation encounters an error.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.BudgetExceededError","title":"<code>BudgetExceededError</code>","text":"<p>               Bases: <code>CostError</code></p> <p>Raised when a cost budget limit is exceeded.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class BudgetExceededError(CostError):\n    \"\"\"Raised when a cost budget limit is exceeded.\"\"\"\n\n    def __init__(self, actual: float, limit: float, currency: str = \"USD\") -&gt; None:\n        self.actual = actual\n        self.limit = limit\n        self.currency = currency\n        super().__init__(f\"Budget exceeded: ${actual:.4f} &gt; ${limit:.4f} {currency} limit\")\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.SafetyError","title":"<code>SafetyError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when a safety check fails or encounters an error.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class SafetyError(AgentProbeError):\n    \"\"\"Raised when a safety check fails or encounters an error.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.SecurityError","title":"<code>SecurityError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when a security violation is detected.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class SecurityError(AgentProbeError):\n    \"\"\"Raised when a security violation is detected.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.MetricsError","title":"<code>MetricsError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when metric collection, aggregation, or trending fails.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class MetricsError(AgentProbeError):\n    \"\"\"Raised when metric collection, aggregation, or trending fails.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.PluginError","title":"<code>PluginError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when a plugin fails to load or execute.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class PluginError(AgentProbeError):\n    \"\"\"Raised when a plugin fails to load or execute.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.ChaosError","title":"<code>ChaosError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when a chaos fault injection causes a failure.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class ChaosError(AgentProbeError):\n    \"\"\"Raised when a chaos fault injection causes a failure.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.SnapshotError","title":"<code>SnapshotError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when a snapshot operation fails.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class SnapshotError(AgentProbeError):\n    \"\"\"Raised when a snapshot operation fails.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.ReplayError","title":"<code>ReplayError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when trace replay encounters an error.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class ReplayError(AgentProbeError):\n    \"\"\"Raised when trace replay encounters an error.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.RegressionError","title":"<code>RegressionError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when regression detection encounters an error.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class RegressionError(AgentProbeError):\n    \"\"\"Raised when regression detection encounters an error.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.ConversationError","title":"<code>ConversationError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when a multi-turn conversation test fails.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class ConversationError(AgentProbeError):\n    \"\"\"Raised when a multi-turn conversation test fails.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.DashboardError","title":"<code>DashboardError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when dashboard operations fail.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class DashboardError(AgentProbeError):\n    \"\"\"Raised when dashboard operations fail.\"\"\"\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.exceptions.AssertionFailedError","title":"<code>AssertionFailedError</code>","text":"<p>               Bases: <code>AgentProbeError</code></p> <p>Raised when a test assertion fails.</p> <p>Attributes:</p> Name Type Description <code>assertion_type</code> <p>The type of assertion that failed (e.g. 'contain', 'match').</p> <code>expected</code> <p>The expected value or pattern.</p> <code>actual</code> <p>The actual value received.</p> Source code in <code>src/agentprobe/core/exceptions.py</code> <pre><code>class AssertionFailedError(AgentProbeError):\n    \"\"\"Raised when a test assertion fails.\n\n    Attributes:\n        assertion_type: The type of assertion that failed (e.g. 'contain', 'match').\n        expected: The expected value or pattern.\n        actual: The actual value received.\n    \"\"\"\n\n    def __init__(\n        self,\n        assertion_type: str,\n        expected: object,\n        actual: object,\n        message: str | None = None,\n    ) -&gt; None:\n        self.assertion_type = assertion_type\n        self.expected = expected\n        self.actual = actual\n        msg = message or (\n            f\"Assertion '{assertion_type}' failed: expected {expected!r}, got {actual!r}\"\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"reference/api/core/#discovery","title":"Discovery","text":""},{"location":"reference/api/core/#agentprobe.core.discovery","title":"<code>agentprobe.core.discovery</code>","text":"<p>Test discovery: finds and loads test modules with @scenario decorators.</p> <p>Scans directories for Python files matching test patterns, imports them, and extracts registered test cases.</p>"},{"location":"reference/api/core/#agentprobe.core.discovery.discover_test_files","title":"<code>discover_test_files(test_dir, pattern='test_*.py')</code>","text":"<p>Find test files matching a pattern in the given directory.</p> <p>Parameters:</p> Name Type Description Default <code>test_dir</code> <code>str | Path</code> <p>Root directory to search.</p> required <code>pattern</code> <code>str</code> <p>Glob pattern for test files.</p> <code>'test_*.py'</code> <p>Returns:</p> Type Description <code>list[Path]</code> <p>Sorted list of matching file paths.</p> Source code in <code>src/agentprobe/core/discovery.py</code> <pre><code>def discover_test_files(\n    test_dir: str | Path,\n    pattern: str = \"test_*.py\",\n) -&gt; list[Path]:\n    \"\"\"Find test files matching a pattern in the given directory.\n\n    Args:\n        test_dir: Root directory to search.\n        pattern: Glob pattern for test files.\n\n    Returns:\n        Sorted list of matching file paths.\n    \"\"\"\n    test_path = Path(test_dir)\n    if not test_path.is_dir():\n        logger.warning(\"Test directory does not exist: %s\", test_path)\n        return []\n\n    files = sorted(test_path.rglob(pattern))\n    logger.info(\"Discovered %d test files in %s\", len(files), test_path)\n    return files\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.discovery.load_test_module","title":"<code>load_test_module(file_path)</code>","text":"<p>Import a test module from a file path.</p> <p>Uses importlib to load the module with a unique name derived from the file path. The module is registered in <code>sys.modules</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the Python test file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The module name used for registration.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module cannot be loaded.</p> Source code in <code>src/agentprobe/core/discovery.py</code> <pre><code>def load_test_module(file_path: Path) -&gt; str:\n    \"\"\"Import a test module from a file path.\n\n    Uses importlib to load the module with a unique name derived\n    from the file path. The module is registered in ``sys.modules``.\n\n    Args:\n        file_path: Path to the Python test file.\n\n    Returns:\n        The module name used for registration.\n\n    Raises:\n        ImportError: If the module cannot be loaded.\n    \"\"\"\n    module_name = f\"agentprobe_tests.{file_path.stem}_{id(file_path)}\"\n    spec = importlib.util.spec_from_file_location(module_name, file_path)\n    if spec is None or spec.loader is None:\n        msg = f\"Cannot create module spec for {file_path}\"\n        raise ImportError(msg)\n\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[module_name] = module\n\n    try:\n        spec.loader.exec_module(module)\n    except Exception as exc:\n        del sys.modules[module_name]\n        raise ImportError(f\"Failed to load {file_path}: {exc}\") from exc\n\n    logger.debug(\"Loaded test module: %s from %s\", module_name, file_path)\n    return module_name\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.discovery.extract_test_cases","title":"<code>extract_test_cases(test_dir, pattern='test_*.py')</code>","text":"<p>Discover and extract all test cases from a directory.</p> <p>Finds test files, imports them (triggering @scenario registration), then collects all registered test cases.</p> <p>Parameters:</p> Name Type Description Default <code>test_dir</code> <code>str | Path</code> <p>Root directory to search.</p> required <code>pattern</code> <code>str</code> <p>Glob pattern for test files.</p> <code>'test_*.py'</code> <p>Returns:</p> Type Description <code>list[TestCase]</code> <p>List of all discovered TestCase objects.</p> Source code in <code>src/agentprobe/core/discovery.py</code> <pre><code>def extract_test_cases(\n    test_dir: str | Path,\n    pattern: str = \"test_*.py\",\n) -&gt; list[TestCase]:\n    \"\"\"Discover and extract all test cases from a directory.\n\n    Finds test files, imports them (triggering @scenario registration),\n    then collects all registered test cases.\n\n    Args:\n        test_dir: Root directory to search.\n        pattern: Glob pattern for test files.\n\n    Returns:\n        List of all discovered TestCase objects.\n    \"\"\"\n    files = discover_test_files(test_dir, pattern)\n    module_names: list[str] = []\n\n    for file_path in files:\n        try:\n            name = load_test_module(file_path)\n            module_names.append(name)\n        except ImportError:\n            logger.exception(\"Skipping unloadable file: %s\", file_path)\n\n    all_cases: list[TestCase] = []\n    for module_name in module_names:\n        cases = get_scenarios(module_name)\n        all_cases.extend(cases)\n\n    logger.info(\"Extracted %d test cases from %d modules\", len(all_cases), len(module_names))\n    return all_cases\n</code></pre>"},{"location":"reference/api/core/#conversation-runner","title":"Conversation Runner","text":""},{"location":"reference/api/core/#agentprobe.core.conversation","title":"<code>agentprobe.core.conversation</code>","text":"<p>Multi-turn conversation runner for sequential dialogue testing.</p> <p>Executes a series of conversation turns against an agent adapter, collecting per-turn traces and evaluation results, then aggregates into a ConversationResult.</p>"},{"location":"reference/api/core/#agentprobe.core.conversation.ConversationRunner","title":"<code>ConversationRunner</code>","text":"<p>Runs multi-turn conversation tests against an agent.</p> <p>Executes each turn sequentially, optionally passing the previous output as context to the next turn's input. Collects per-turn evaluation results and aggregates into a final ConversationResult.</p> <p>Attributes:</p> Name Type Description <code>evaluators</code> <p>Mapping of evaluator names to instances.</p> Source code in <code>src/agentprobe/core/conversation.py</code> <pre><code>class ConversationRunner:\n    \"\"\"Runs multi-turn conversation tests against an agent.\n\n    Executes each turn sequentially, optionally passing the previous\n    output as context to the next turn's input. Collects per-turn\n    evaluation results and aggregates into a final ConversationResult.\n\n    Attributes:\n        evaluators: Mapping of evaluator names to instances.\n    \"\"\"\n\n    def __init__(\n        self,\n        evaluators: dict[str, EvaluatorProtocol] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the conversation runner.\n\n        Args:\n            evaluators: Named evaluator instances for per-turn evaluation.\n        \"\"\"\n        self._evaluators = evaluators or {}\n\n    async def run(\n        self,\n        adapter: AdapterProtocol,\n        turns: Sequence[ConversationTurn],\n        *,\n        pass_context: bool = True,\n    ) -&gt; ConversationResult:\n        \"\"\"Execute a multi-turn conversation.\n\n        Args:\n            adapter: The agent adapter to invoke for each turn.\n            turns: The conversation turns to execute in order.\n            pass_context: If True, prepend previous output to next turn's input.\n\n        Returns:\n            A ConversationResult with per-turn details and aggregate metrics.\n\n        Raises:\n            ConversationError: If a critical error occurs during execution.\n        \"\"\"\n        turn_results: list[TurnResult] = []\n        previous_output = \"\"\n        total_start = time.monotonic()\n\n        for i, turn in enumerate(turns):\n            input_text = turn.input_text\n            if pass_context and previous_output:\n                input_text = f\"{previous_output}\\n\\n{turn.input_text}\"\n\n            turn_start = time.monotonic()\n            try:\n                trace = await adapter.invoke(input_text)\n                previous_output = trace.output_text\n            except Exception as exc:\n                logger.error(\"Turn %d failed: %s\", i, exc)\n                turn_results.append(\n                    TurnResult(\n                        turn_index=i,\n                        input_text=turn.input_text,\n                        trace=None,\n                        eval_results=(),\n                        duration_ms=int((time.monotonic() - turn_start) * 1000),\n                    )\n                )\n                continue\n\n            # Run per-turn evaluators\n            eval_results: list[EvalResult] = []\n            if turn.evaluators:\n                test_case = TestCase(\n                    name=f\"turn_{i}\",\n                    input_text=turn.input_text,\n                    expected_output=turn.expected_output,\n                )\n                for eval_name in turn.evaluators:\n                    evaluator = self._evaluators.get(eval_name)\n                    if evaluator is None:\n                        logger.warning(\"Turn %d: evaluator '%s' not found\", i, eval_name)\n                        continue\n                    result = await evaluator.evaluate(test_case, trace)\n                    eval_results.append(result)\n\n            duration_ms = int((time.monotonic() - turn_start) * 1000)\n            turn_results.append(\n                TurnResult(\n                    turn_index=i,\n                    input_text=turn.input_text,\n                    trace=trace,\n                    eval_results=tuple(eval_results),\n                    duration_ms=duration_ms,\n                )\n            )\n\n        total_duration = int((time.monotonic() - total_start) * 1000)\n        return self._build_result(adapter.name, turn_results, total_duration)\n\n    @staticmethod\n    def _build_result(\n        agent_name: str,\n        turn_results: list[TurnResult],\n        total_duration_ms: int,\n    ) -&gt; ConversationResult:\n        \"\"\"Aggregate per-turn results into a ConversationResult.\"\"\"\n        passed = 0\n        scores: list[float] = []\n\n        for tr in turn_results:\n            if tr.eval_results:\n                turn_passed = all(er.verdict == EvalVerdict.PASS for er in tr.eval_results)\n                if turn_passed:\n                    passed += 1\n                turn_score = sum(er.score for er in tr.eval_results) / len(tr.eval_results)\n                scores.append(turn_score)\n            elif tr.trace is not None:\n                # No evaluators but trace exists = pass\n                passed += 1\n                scores.append(1.0)\n            else:\n                scores.append(0.0)\n\n        aggregate_score = sum(scores) / len(scores) if scores else 0.0\n\n        return ConversationResult(\n            agent_name=agent_name,\n            turn_results=tuple(turn_results),\n            total_turns=len(turn_results),\n            passed_turns=passed,\n            aggregate_score=round(min(aggregate_score, 1.0), 6),\n            total_duration_ms=total_duration_ms,\n        )\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.conversation.ConversationRunner.__init__","title":"<code>__init__(evaluators=None)</code>","text":"<p>Initialize the conversation runner.</p> <p>Parameters:</p> Name Type Description Default <code>evaluators</code> <code>dict[str, EvaluatorProtocol] | None</code> <p>Named evaluator instances for per-turn evaluation.</p> <code>None</code> Source code in <code>src/agentprobe/core/conversation.py</code> <pre><code>def __init__(\n    self,\n    evaluators: dict[str, EvaluatorProtocol] | None = None,\n) -&gt; None:\n    \"\"\"Initialize the conversation runner.\n\n    Args:\n        evaluators: Named evaluator instances for per-turn evaluation.\n    \"\"\"\n    self._evaluators = evaluators or {}\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.conversation.ConversationRunner.run","title":"<code>run(adapter, turns, *, pass_context=True)</code>  <code>async</code>","text":"<p>Execute a multi-turn conversation.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>AdapterProtocol</code> <p>The agent adapter to invoke for each turn.</p> required <code>turns</code> <code>Sequence[ConversationTurn]</code> <p>The conversation turns to execute in order.</p> required <code>pass_context</code> <code>bool</code> <p>If True, prepend previous output to next turn's input.</p> <code>True</code> <p>Returns:</p> Type Description <code>ConversationResult</code> <p>A ConversationResult with per-turn details and aggregate metrics.</p> <p>Raises:</p> Type Description <code>ConversationError</code> <p>If a critical error occurs during execution.</p> Source code in <code>src/agentprobe/core/conversation.py</code> <pre><code>async def run(\n    self,\n    adapter: AdapterProtocol,\n    turns: Sequence[ConversationTurn],\n    *,\n    pass_context: bool = True,\n) -&gt; ConversationResult:\n    \"\"\"Execute a multi-turn conversation.\n\n    Args:\n        adapter: The agent adapter to invoke for each turn.\n        turns: The conversation turns to execute in order.\n        pass_context: If True, prepend previous output to next turn's input.\n\n    Returns:\n        A ConversationResult with per-turn details and aggregate metrics.\n\n    Raises:\n        ConversationError: If a critical error occurs during execution.\n    \"\"\"\n    turn_results: list[TurnResult] = []\n    previous_output = \"\"\n    total_start = time.monotonic()\n\n    for i, turn in enumerate(turns):\n        input_text = turn.input_text\n        if pass_context and previous_output:\n            input_text = f\"{previous_output}\\n\\n{turn.input_text}\"\n\n        turn_start = time.monotonic()\n        try:\n            trace = await adapter.invoke(input_text)\n            previous_output = trace.output_text\n        except Exception as exc:\n            logger.error(\"Turn %d failed: %s\", i, exc)\n            turn_results.append(\n                TurnResult(\n                    turn_index=i,\n                    input_text=turn.input_text,\n                    trace=None,\n                    eval_results=(),\n                    duration_ms=int((time.monotonic() - turn_start) * 1000),\n                )\n            )\n            continue\n\n        # Run per-turn evaluators\n        eval_results: list[EvalResult] = []\n        if turn.evaluators:\n            test_case = TestCase(\n                name=f\"turn_{i}\",\n                input_text=turn.input_text,\n                expected_output=turn.expected_output,\n            )\n            for eval_name in turn.evaluators:\n                evaluator = self._evaluators.get(eval_name)\n                if evaluator is None:\n                    logger.warning(\"Turn %d: evaluator '%s' not found\", i, eval_name)\n                    continue\n                result = await evaluator.evaluate(test_case, trace)\n                eval_results.append(result)\n\n        duration_ms = int((time.monotonic() - turn_start) * 1000)\n        turn_results.append(\n            TurnResult(\n                turn_index=i,\n                input_text=turn.input_text,\n                trace=trace,\n                eval_results=tuple(eval_results),\n                duration_ms=duration_ms,\n            )\n        )\n\n    total_duration = int((time.monotonic() - total_start) * 1000)\n    return self._build_result(adapter.name, turn_results, total_duration)\n</code></pre>"},{"location":"reference/api/core/#chaos-proxy","title":"Chaos Proxy","text":""},{"location":"reference/api/core/#agentprobe.core.chaos","title":"<code>agentprobe.core.chaos</code>","text":"<p>Chaos fault injection proxy for testing agent resilience.</p> <p>Wraps an adapter and modifies tool call results in the resulting trace to simulate failures, timeouts, malformed data, rate limits, slow responses, and empty responses.</p>"},{"location":"reference/api/core/#agentprobe.core.chaos.ChaosProxy","title":"<code>ChaosProxy</code>","text":"<p>Wraps an adapter and injects chaos faults into tool call results.</p> <p>After the real adapter produces a trace, ChaosProxy scans tool calls and probabilistically replaces their outputs with fault-injected variants. The modified trace is returned as a frozen copy.</p> <p>Attributes:</p> Name Type Description <code>overrides</code> <p>Configured fault injection rules.</p> Source code in <code>src/agentprobe/core/chaos.py</code> <pre><code>class ChaosProxy:\n    \"\"\"Wraps an adapter and injects chaos faults into tool call results.\n\n    After the real adapter produces a trace, ChaosProxy scans tool calls\n    and probabilistically replaces their outputs with fault-injected\n    variants. The modified trace is returned as a frozen copy.\n\n    Attributes:\n        overrides: Configured fault injection rules.\n    \"\"\"\n\n    def __init__(\n        self,\n        adapter: AdapterProtocol,\n        overrides: list[ChaosOverride],\n        *,\n        seed: int = 42,\n    ) -&gt; None:\n        \"\"\"Initialize the chaos proxy.\n\n        Args:\n            adapter: The real adapter to wrap.\n            overrides: Fault injection rules to apply.\n            seed: Random seed for deterministic fault injection.\n        \"\"\"\n        self._adapter = adapter\n        self._overrides = overrides\n        self._rng = random.Random(seed)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the adapter name with chaos prefix.\"\"\"\n        return f\"chaos-{self._adapter.name}\"\n\n    async def invoke(self, input_text: str, **kwargs: Any) -&gt; Trace:\n        \"\"\"Invoke the wrapped adapter and inject faults.\n\n        Args:\n            input_text: Input text to send to the adapter.\n            **kwargs: Additional adapter arguments.\n\n        Returns:\n            A modified trace with chaos faults injected.\n        \"\"\"\n        trace = await self._adapter.invoke(input_text, **kwargs)\n        return self._apply_chaos(trace)\n\n    def _apply_chaos(self, trace: Trace) -&gt; Trace:\n        \"\"\"Apply chaos overrides to tool calls in the trace.\"\"\"\n        if not trace.tool_calls or not self._overrides:\n            return trace\n\n        modified_calls: list[ToolCall] = []\n        any_modified = False\n\n        for tc in trace.tool_calls:\n            override = self._match_override(tc)\n            if override is not None and self._rng.random() &lt; override.probability:\n                modified_calls.append(self._inject_fault(tc, override))\n                any_modified = True\n            else:\n                modified_calls.append(tc)\n\n        if not any_modified:\n            return trace\n\n        return trace.model_copy(\n            update={\"tool_calls\": tuple(modified_calls)},\n        )\n\n    def _match_override(self, tool_call: ToolCall) -&gt; ChaosOverride | None:\n        \"\"\"Find the first matching override for a tool call.\"\"\"\n        for override in self._overrides:\n            if override.target_tool is None or override.target_tool == tool_call.tool_name:\n                return override\n        return None\n\n    def _inject_fault(self, tool_call: ToolCall, override: ChaosOverride) -&gt; ToolCall:\n        \"\"\"Create a fault-injected copy of a tool call.\"\"\"\n        logger.debug(\n            \"Injecting %s fault into tool '%s'\",\n            override.chaos_type.value,\n            tool_call.tool_name,\n        )\n        fault_map: dict[ChaosType, dict[str, Any]] = {\n            ChaosType.TIMEOUT: {\n                \"success\": False,\n                \"error\": \"Chaos: operation timed out\",\n                \"tool_output\": None,\n            },\n            ChaosType.ERROR: {\n                \"success\": False,\n                \"error\": f\"Chaos: {override.error_message}\",\n                \"tool_output\": None,\n            },\n            ChaosType.MALFORMED: {\n                \"success\": True,\n                \"tool_output\": \"{malformed: data, &lt;&lt;invalid&gt;&gt;}\",\n            },\n            ChaosType.RATE_LIMIT: {\n                \"success\": False,\n                \"error\": \"Chaos: rate limit exceeded (429)\",\n                \"tool_output\": None,\n            },\n            ChaosType.SLOW: {\n                \"success\": True,\n                \"tool_output\": tool_call.tool_output,\n                \"latency_ms\": tool_call.latency_ms + override.delay_ms,\n            },\n            ChaosType.EMPTY: {\n                \"success\": True,\n                \"tool_output\": \"\",\n            },\n        }\n        updates = fault_map.get(\n            override.chaos_type,\n            {\"success\": False, \"error\": f\"Chaos: unknown type {override.chaos_type}\"},\n        )\n        return tool_call.model_copy(update=updates)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.chaos.ChaosProxy.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the adapter name with chaos prefix.</p>"},{"location":"reference/api/core/#agentprobe.core.chaos.ChaosProxy.__init__","title":"<code>__init__(adapter, overrides, *, seed=42)</code>","text":"<p>Initialize the chaos proxy.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>AdapterProtocol</code> <p>The real adapter to wrap.</p> required <code>overrides</code> <code>list[ChaosOverride]</code> <p>Fault injection rules to apply.</p> required <code>seed</code> <code>int</code> <p>Random seed for deterministic fault injection.</p> <code>42</code> Source code in <code>src/agentprobe/core/chaos.py</code> <pre><code>def __init__(\n    self,\n    adapter: AdapterProtocol,\n    overrides: list[ChaosOverride],\n    *,\n    seed: int = 42,\n) -&gt; None:\n    \"\"\"Initialize the chaos proxy.\n\n    Args:\n        adapter: The real adapter to wrap.\n        overrides: Fault injection rules to apply.\n        seed: Random seed for deterministic fault injection.\n    \"\"\"\n    self._adapter = adapter\n    self._overrides = overrides\n    self._rng = random.Random(seed)\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.chaos.ChaosProxy.invoke","title":"<code>invoke(input_text, **kwargs)</code>  <code>async</code>","text":"<p>Invoke the wrapped adapter and inject faults.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>Input text to send to the adapter.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional adapter arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Trace</code> <p>A modified trace with chaos faults injected.</p> Source code in <code>src/agentprobe/core/chaos.py</code> <pre><code>async def invoke(self, input_text: str, **kwargs: Any) -&gt; Trace:\n    \"\"\"Invoke the wrapped adapter and inject faults.\n\n    Args:\n        input_text: Input text to send to the adapter.\n        **kwargs: Additional adapter arguments.\n\n    Returns:\n        A modified trace with chaos faults injected.\n    \"\"\"\n    trace = await self._adapter.invoke(input_text, **kwargs)\n    return self._apply_chaos(trace)\n</code></pre>"},{"location":"reference/api/core/#snapshot-manager","title":"Snapshot Manager","text":""},{"location":"reference/api/core/#agentprobe.core.snapshot","title":"<code>agentprobe.core.snapshot</code>","text":"<p>Snapshot (golden file) management for output comparison testing.</p> <p>Saves, loads, compares, and updates agent output snapshots stored as JSON files. Supports multi-dimension comparison including tool calls, response structure, key facts, cost, and latency.</p>"},{"location":"reference/api/core/#agentprobe.core.snapshot.SnapshotManager","title":"<code>SnapshotManager</code>","text":"<p>Manages snapshot files for golden-file testing.</p> <p>Saves traces as JSON snapshots and compares current traces against saved snapshots across multiple dimensions.</p> <p>Attributes:</p> Name Type Description <code>snapshot_dir</code> <p>Directory where snapshot files are stored.</p> <code>threshold</code> <p>Similarity threshold for matching.</p> Source code in <code>src/agentprobe/core/snapshot.py</code> <pre><code>class SnapshotManager:\n    \"\"\"Manages snapshot files for golden-file testing.\n\n    Saves traces as JSON snapshots and compares current traces against\n    saved snapshots across multiple dimensions.\n\n    Attributes:\n        snapshot_dir: Directory where snapshot files are stored.\n        threshold: Similarity threshold for matching.\n    \"\"\"\n\n    def __init__(\n        self,\n        snapshot_dir: str | Path = \".agentprobe/snapshots\",\n        *,\n        threshold: float = 0.8,\n    ) -&gt; None:\n        \"\"\"Initialize the snapshot manager.\n\n        Args:\n            snapshot_dir: Directory for snapshot storage.\n            threshold: Similarity threshold for a match.\n        \"\"\"\n        self._dir = Path(snapshot_dir)\n        self._threshold = threshold\n\n    def _snapshot_path(self, name: str) -&gt; Path:\n        \"\"\"Get the file path for a named snapshot.\"\"\"\n        return self._dir / f\"{name}.json\"\n\n    def save(self, name: str, trace: Trace) -&gt; Path:\n        \"\"\"Save a trace as a named snapshot.\n\n        Args:\n            name: Snapshot name.\n            trace: Trace to save.\n\n        Returns:\n            Path to the saved snapshot file.\n        \"\"\"\n        self._dir.mkdir(parents=True, exist_ok=True)\n        path = self._snapshot_path(name)\n        path.write_text(trace.model_dump_json(indent=2), encoding=\"utf-8\")\n        logger.info(\"Snapshot saved: %s\", path)\n        return path\n\n    def load(self, name: str) -&gt; Trace:\n        \"\"\"Load a named snapshot.\n\n        Args:\n            name: Snapshot name.\n\n        Returns:\n            The saved Trace.\n\n        Raises:\n            SnapshotError: If the snapshot does not exist.\n        \"\"\"\n        path = self._snapshot_path(name)\n        if not path.exists():\n            raise SnapshotError(f\"Snapshot not found: {name}\")\n        return Trace.model_validate_json(path.read_text(encoding=\"utf-8\"))\n\n    def exists(self, name: str) -&gt; bool:\n        \"\"\"Check if a named snapshot exists.\"\"\"\n        return self._snapshot_path(name).exists()\n\n    def list_snapshots(self) -&gt; list[str]:\n        \"\"\"List all snapshot names.\"\"\"\n        if not self._dir.is_dir():\n            return []\n        return sorted(p.stem for p in self._dir.glob(\"*.json\"))\n\n    def delete(self, name: str) -&gt; bool:\n        \"\"\"Delete a named snapshot.\n\n        Args:\n            name: Snapshot name.\n\n        Returns:\n            True if deleted, False if not found.\n        \"\"\"\n        path = self._snapshot_path(name)\n        if path.exists():\n            path.unlink()\n            logger.info(\"Snapshot deleted: %s\", name)\n            return True\n        return False\n\n    def compare(self, name: str, current: Trace) -&gt; SnapshotDiff:\n        \"\"\"Compare a current trace against a saved snapshot.\n\n        Compares across dimensions: tool_calls, output, token_usage,\n        latency, and metadata.\n\n        Args:\n            name: Snapshot name.\n            current: Current trace to compare.\n\n        Returns:\n            A SnapshotDiff with per-dimension similarity scores.\n\n        Raises:\n            SnapshotError: If the snapshot does not exist.\n        \"\"\"\n        baseline = self.load(name)\n        diffs: list[DiffItem] = []\n\n        # Tool call sequence similarity\n        baseline_tools = [tc.tool_name for tc in baseline.tool_calls]\n        current_tools = [tc.tool_name for tc in current.tool_calls]\n        tool_sim = _sequence_similarity(baseline_tools, current_tools)\n        diffs.append(\n            DiffItem(\n                dimension=\"tool_calls\",\n                expected=baseline_tools,\n                actual=current_tools,\n                similarity=round(tool_sim, 4),\n            )\n        )\n\n        # Output text similarity\n        output_sim = _keyword_overlap(baseline.output_text, current.output_text)\n        diffs.append(\n            DiffItem(\n                dimension=\"output\",\n                expected=baseline.output_text[:200],\n                actual=current.output_text[:200],\n                similarity=round(output_sim, 4),\n            )\n        )\n\n        # Token usage similarity\n        baseline_tokens = baseline.total_input_tokens + baseline.total_output_tokens\n        current_tokens = current.total_input_tokens + current.total_output_tokens\n        if baseline_tokens &gt; 0:\n            token_ratio = min(current_tokens, baseline_tokens) / max(\n                current_tokens, baseline_tokens\n            )\n        elif current_tokens == 0:\n            token_ratio = 1.0\n        else:\n            token_ratio = 0.0\n        diffs.append(\n            DiffItem(\n                dimension=\"token_usage\",\n                expected=baseline_tokens,\n                actual=current_tokens,\n                similarity=round(token_ratio, 4),\n            )\n        )\n\n        # Latency similarity\n        if baseline.total_latency_ms &gt; 0:\n            latency_ratio = min(current.total_latency_ms, baseline.total_latency_ms) / max(\n                current.total_latency_ms, baseline.total_latency_ms\n            )\n        elif current.total_latency_ms == 0:\n            latency_ratio = 1.0\n        else:\n            latency_ratio = 0.0\n        diffs.append(\n            DiffItem(\n                dimension=\"latency\",\n                expected=baseline.total_latency_ms,\n                actual=current.total_latency_ms,\n                similarity=round(latency_ratio, 4),\n            )\n        )\n\n        # Overall weighted average\n        weights = {\"tool_calls\": 0.35, \"output\": 0.35, \"token_usage\": 0.15, \"latency\": 0.15}\n        overall = sum(d.similarity * weights.get(d.dimension, 0.0) for d in diffs)\n\n        is_match = overall &gt;= self._threshold\n\n        return SnapshotDiff(\n            snapshot_name=name,\n            overall_similarity=round(overall, 4),\n            diffs=tuple(diffs),\n            is_match=is_match,\n            threshold=self._threshold,\n        )\n\n    def update_all(self, snapshots: dict[str, Trace]) -&gt; int:\n        \"\"\"Update multiple snapshots at once.\n\n        Args:\n            snapshots: Mapping of snapshot names to traces.\n\n        Returns:\n            Number of snapshots updated.\n        \"\"\"\n        count = 0\n        for name, trace in snapshots.items():\n            self.save(name, trace)\n            count += 1\n        return count\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.snapshot.SnapshotManager.__init__","title":"<code>__init__(snapshot_dir='.agentprobe/snapshots', *, threshold=0.8)</code>","text":"<p>Initialize the snapshot manager.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot_dir</code> <code>str | Path</code> <p>Directory for snapshot storage.</p> <code>'.agentprobe/snapshots'</code> <code>threshold</code> <code>float</code> <p>Similarity threshold for a match.</p> <code>0.8</code> Source code in <code>src/agentprobe/core/snapshot.py</code> <pre><code>def __init__(\n    self,\n    snapshot_dir: str | Path = \".agentprobe/snapshots\",\n    *,\n    threshold: float = 0.8,\n) -&gt; None:\n    \"\"\"Initialize the snapshot manager.\n\n    Args:\n        snapshot_dir: Directory for snapshot storage.\n        threshold: Similarity threshold for a match.\n    \"\"\"\n    self._dir = Path(snapshot_dir)\n    self._threshold = threshold\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.snapshot.SnapshotManager.save","title":"<code>save(name, trace)</code>","text":"<p>Save a trace as a named snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Snapshot name.</p> required <code>trace</code> <code>Trace</code> <p>Trace to save.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved snapshot file.</p> Source code in <code>src/agentprobe/core/snapshot.py</code> <pre><code>def save(self, name: str, trace: Trace) -&gt; Path:\n    \"\"\"Save a trace as a named snapshot.\n\n    Args:\n        name: Snapshot name.\n        trace: Trace to save.\n\n    Returns:\n        Path to the saved snapshot file.\n    \"\"\"\n    self._dir.mkdir(parents=True, exist_ok=True)\n    path = self._snapshot_path(name)\n    path.write_text(trace.model_dump_json(indent=2), encoding=\"utf-8\")\n    logger.info(\"Snapshot saved: %s\", path)\n    return path\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.snapshot.SnapshotManager.load","title":"<code>load(name)</code>","text":"<p>Load a named snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Snapshot name.</p> required <p>Returns:</p> Type Description <code>Trace</code> <p>The saved Trace.</p> <p>Raises:</p> Type Description <code>SnapshotError</code> <p>If the snapshot does not exist.</p> Source code in <code>src/agentprobe/core/snapshot.py</code> <pre><code>def load(self, name: str) -&gt; Trace:\n    \"\"\"Load a named snapshot.\n\n    Args:\n        name: Snapshot name.\n\n    Returns:\n        The saved Trace.\n\n    Raises:\n        SnapshotError: If the snapshot does not exist.\n    \"\"\"\n    path = self._snapshot_path(name)\n    if not path.exists():\n        raise SnapshotError(f\"Snapshot not found: {name}\")\n    return Trace.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.snapshot.SnapshotManager.exists","title":"<code>exists(name)</code>","text":"<p>Check if a named snapshot exists.</p> Source code in <code>src/agentprobe/core/snapshot.py</code> <pre><code>def exists(self, name: str) -&gt; bool:\n    \"\"\"Check if a named snapshot exists.\"\"\"\n    return self._snapshot_path(name).exists()\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.snapshot.SnapshotManager.list_snapshots","title":"<code>list_snapshots()</code>","text":"<p>List all snapshot names.</p> Source code in <code>src/agentprobe/core/snapshot.py</code> <pre><code>def list_snapshots(self) -&gt; list[str]:\n    \"\"\"List all snapshot names.\"\"\"\n    if not self._dir.is_dir():\n        return []\n    return sorted(p.stem for p in self._dir.glob(\"*.json\"))\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.snapshot.SnapshotManager.delete","title":"<code>delete(name)</code>","text":"<p>Delete a named snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Snapshot name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted, False if not found.</p> Source code in <code>src/agentprobe/core/snapshot.py</code> <pre><code>def delete(self, name: str) -&gt; bool:\n    \"\"\"Delete a named snapshot.\n\n    Args:\n        name: Snapshot name.\n\n    Returns:\n        True if deleted, False if not found.\n    \"\"\"\n    path = self._snapshot_path(name)\n    if path.exists():\n        path.unlink()\n        logger.info(\"Snapshot deleted: %s\", name)\n        return True\n    return False\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.snapshot.SnapshotManager.compare","title":"<code>compare(name, current)</code>","text":"<p>Compare a current trace against a saved snapshot.</p> <p>Compares across dimensions: tool_calls, output, token_usage, latency, and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Snapshot name.</p> required <code>current</code> <code>Trace</code> <p>Current trace to compare.</p> required <p>Returns:</p> Type Description <code>SnapshotDiff</code> <p>A SnapshotDiff with per-dimension similarity scores.</p> <p>Raises:</p> Type Description <code>SnapshotError</code> <p>If the snapshot does not exist.</p> Source code in <code>src/agentprobe/core/snapshot.py</code> <pre><code>def compare(self, name: str, current: Trace) -&gt; SnapshotDiff:\n    \"\"\"Compare a current trace against a saved snapshot.\n\n    Compares across dimensions: tool_calls, output, token_usage,\n    latency, and metadata.\n\n    Args:\n        name: Snapshot name.\n        current: Current trace to compare.\n\n    Returns:\n        A SnapshotDiff with per-dimension similarity scores.\n\n    Raises:\n        SnapshotError: If the snapshot does not exist.\n    \"\"\"\n    baseline = self.load(name)\n    diffs: list[DiffItem] = []\n\n    # Tool call sequence similarity\n    baseline_tools = [tc.tool_name for tc in baseline.tool_calls]\n    current_tools = [tc.tool_name for tc in current.tool_calls]\n    tool_sim = _sequence_similarity(baseline_tools, current_tools)\n    diffs.append(\n        DiffItem(\n            dimension=\"tool_calls\",\n            expected=baseline_tools,\n            actual=current_tools,\n            similarity=round(tool_sim, 4),\n        )\n    )\n\n    # Output text similarity\n    output_sim = _keyword_overlap(baseline.output_text, current.output_text)\n    diffs.append(\n        DiffItem(\n            dimension=\"output\",\n            expected=baseline.output_text[:200],\n            actual=current.output_text[:200],\n            similarity=round(output_sim, 4),\n        )\n    )\n\n    # Token usage similarity\n    baseline_tokens = baseline.total_input_tokens + baseline.total_output_tokens\n    current_tokens = current.total_input_tokens + current.total_output_tokens\n    if baseline_tokens &gt; 0:\n        token_ratio = min(current_tokens, baseline_tokens) / max(\n            current_tokens, baseline_tokens\n        )\n    elif current_tokens == 0:\n        token_ratio = 1.0\n    else:\n        token_ratio = 0.0\n    diffs.append(\n        DiffItem(\n            dimension=\"token_usage\",\n            expected=baseline_tokens,\n            actual=current_tokens,\n            similarity=round(token_ratio, 4),\n        )\n    )\n\n    # Latency similarity\n    if baseline.total_latency_ms &gt; 0:\n        latency_ratio = min(current.total_latency_ms, baseline.total_latency_ms) / max(\n            current.total_latency_ms, baseline.total_latency_ms\n        )\n    elif current.total_latency_ms == 0:\n        latency_ratio = 1.0\n    else:\n        latency_ratio = 0.0\n    diffs.append(\n        DiffItem(\n            dimension=\"latency\",\n            expected=baseline.total_latency_ms,\n            actual=current.total_latency_ms,\n            similarity=round(latency_ratio, 4),\n        )\n    )\n\n    # Overall weighted average\n    weights = {\"tool_calls\": 0.35, \"output\": 0.35, \"token_usage\": 0.15, \"latency\": 0.15}\n    overall = sum(d.similarity * weights.get(d.dimension, 0.0) for d in diffs)\n\n    is_match = overall &gt;= self._threshold\n\n    return SnapshotDiff(\n        snapshot_name=name,\n        overall_similarity=round(overall, 4),\n        diffs=tuple(diffs),\n        is_match=is_match,\n        threshold=self._threshold,\n    )\n</code></pre>"},{"location":"reference/api/core/#agentprobe.core.snapshot.SnapshotManager.update_all","title":"<code>update_all(snapshots)</code>","text":"<p>Update multiple snapshots at once.</p> <p>Parameters:</p> Name Type Description Default <code>snapshots</code> <code>dict[str, Trace]</code> <p>Mapping of snapshot names to traces.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of snapshots updated.</p> Source code in <code>src/agentprobe/core/snapshot.py</code> <pre><code>def update_all(self, snapshots: dict[str, Trace]) -&gt; int:\n    \"\"\"Update multiple snapshots at once.\n\n    Args:\n        snapshots: Mapping of snapshot names to traces.\n\n    Returns:\n        Number of snapshots updated.\n    \"\"\"\n    count = 0\n    for name, trace in snapshots.items():\n        self.save(name, trace)\n        count += 1\n    return count\n</code></pre>"},{"location":"reference/api/cost/","title":"Cost","text":"<p>Cost calculation and budget enforcement.</p>"},{"location":"reference/api/cost/#calculator","title":"Calculator","text":""},{"location":"reference/api/cost/#agentprobe.cost.calculator","title":"<code>agentprobe.cost.calculator</code>","text":"<p>Cost calculator for agent execution traces.</p> <p>Loads pricing data from YAML files and computes per-call and per-trace costs based on token usage.</p>"},{"location":"reference/api/cost/#agentprobe.cost.calculator.PricingEntry","title":"<code>PricingEntry</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pricing for a single model.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>Model identifier.</p> <code>input_cost_per_1k</code> <code>float</code> <p>Cost per 1,000 input tokens in USD.</p> <code>output_cost_per_1k</code> <code>float</code> <p>Cost per 1,000 output tokens in USD.</p> Source code in <code>src/agentprobe/cost/calculator.py</code> <pre><code>class PricingEntry(BaseModel):\n    \"\"\"Pricing for a single model.\n\n    Attributes:\n        model: Model identifier.\n        input_cost_per_1k: Cost per 1,000 input tokens in USD.\n        output_cost_per_1k: Cost per 1,000 output tokens in USD.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, extra=\"forbid\")\n\n    model: str\n    input_cost_per_1k: float = Field(ge=0.0)\n    output_cost_per_1k: float = Field(ge=0.0)\n</code></pre>"},{"location":"reference/api/cost/#agentprobe.cost.calculator.PricingConfig","title":"<code>PricingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Collection of pricing entries.</p> <p>Attributes:</p> Name Type Description <code>entries</code> <code>dict[str, PricingEntry]</code> <p>Mapping of model name to pricing entry.</p> Source code in <code>src/agentprobe/cost/calculator.py</code> <pre><code>class PricingConfig(BaseModel):\n    \"\"\"Collection of pricing entries.\n\n    Attributes:\n        entries: Mapping of model name to pricing entry.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    entries: dict[str, PricingEntry] = Field(default_factory=dict)\n\n    @classmethod\n    def load_from_dir(cls, pricing_dir: str | Path | None = None) -&gt; PricingConfig:\n        \"\"\"Load pricing data from all YAML files in a directory.\n\n        Args:\n            pricing_dir: Directory containing pricing YAML files.\n                Defaults to the bundled pricing_data directory.\n\n        Returns:\n            A PricingConfig with all entries loaded.\n        \"\"\"\n        directory = Path(pricing_dir) if pricing_dir else _DEFAULT_PRICING_DIR\n        entries: dict[str, PricingEntry] = {}\n\n        if not directory.is_dir():\n            logger.warning(\"Pricing directory not found: %s\", directory)\n            return cls(entries=entries)\n\n        for yaml_file in sorted(directory.glob(\"*.yaml\")):\n            try:\n                raw = yaml.safe_load(yaml_file.read_text(encoding=\"utf-8\"))\n                if not isinstance(raw, dict):\n                    continue\n                models = raw.get(\"models\", [])\n                for model_data in models:\n                    if isinstance(model_data, dict) and \"model\" in model_data:\n                        entry = PricingEntry.model_validate(model_data)\n                        entries[entry.model] = entry\n            except Exception:\n                logger.exception(\"Failed to load pricing from %s\", yaml_file)\n\n        logger.info(\"Loaded pricing for %d models\", len(entries))\n        return cls(entries=entries)\n</code></pre>"},{"location":"reference/api/cost/#agentprobe.cost.calculator.PricingConfig.load_from_dir","title":"<code>load_from_dir(pricing_dir=None)</code>  <code>classmethod</code>","text":"<p>Load pricing data from all YAML files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>pricing_dir</code> <code>str | Path | None</code> <p>Directory containing pricing YAML files. Defaults to the bundled pricing_data directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>PricingConfig</code> <p>A PricingConfig with all entries loaded.</p> Source code in <code>src/agentprobe/cost/calculator.py</code> <pre><code>@classmethod\ndef load_from_dir(cls, pricing_dir: str | Path | None = None) -&gt; PricingConfig:\n    \"\"\"Load pricing data from all YAML files in a directory.\n\n    Args:\n        pricing_dir: Directory containing pricing YAML files.\n            Defaults to the bundled pricing_data directory.\n\n    Returns:\n        A PricingConfig with all entries loaded.\n    \"\"\"\n    directory = Path(pricing_dir) if pricing_dir else _DEFAULT_PRICING_DIR\n    entries: dict[str, PricingEntry] = {}\n\n    if not directory.is_dir():\n        logger.warning(\"Pricing directory not found: %s\", directory)\n        return cls(entries=entries)\n\n    for yaml_file in sorted(directory.glob(\"*.yaml\")):\n        try:\n            raw = yaml.safe_load(yaml_file.read_text(encoding=\"utf-8\"))\n            if not isinstance(raw, dict):\n                continue\n            models = raw.get(\"models\", [])\n            for model_data in models:\n                if isinstance(model_data, dict) and \"model\" in model_data:\n                    entry = PricingEntry.model_validate(model_data)\n                    entries[entry.model] = entry\n        except Exception:\n            logger.exception(\"Failed to load pricing from %s\", yaml_file)\n\n    logger.info(\"Loaded pricing for %d models\", len(entries))\n    return cls(entries=entries)\n</code></pre>"},{"location":"reference/api/cost/#agentprobe.cost.calculator.CostCalculator","title":"<code>CostCalculator</code>","text":"<p>Calculates costs for agent execution traces.</p> <p>Uses pricing data to compute per-call costs, aggregates by model, and optionally enforces budget limits.</p> <p>Attributes:</p> Name Type Description <code>pricing</code> <p>The pricing configuration.</p> <code>budget_limit_usd</code> <p>Optional maximum cost per trace.</p> Source code in <code>src/agentprobe/cost/calculator.py</code> <pre><code>class CostCalculator:\n    \"\"\"Calculates costs for agent execution traces.\n\n    Uses pricing data to compute per-call costs, aggregates by model,\n    and optionally enforces budget limits.\n\n    Attributes:\n        pricing: The pricing configuration.\n        budget_limit_usd: Optional maximum cost per trace.\n    \"\"\"\n\n    def __init__(\n        self,\n        pricing: PricingConfig | None = None,\n        budget_limit_usd: float | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the cost calculator.\n\n        Args:\n            pricing: Pricing configuration. Loads defaults if None.\n            budget_limit_usd: Optional budget limit in USD.\n        \"\"\"\n        self._pricing = pricing or PricingConfig.load_from_dir()\n        self._budget_limit = budget_limit_usd\n\n    def calculate_llm_cost(self, call: LLMCall) -&gt; float:\n        \"\"\"Calculate the cost of a single LLM call.\n\n        Args:\n            call: The LLM call to price.\n\n        Returns:\n            Cost in USD.\n        \"\"\"\n        entry = self._pricing.entries.get(call.model)\n        if entry is None:\n            logger.warning(\"No pricing found for model: %s\", call.model)\n            return 0.0\n\n        input_cost = (call.input_tokens / 1000.0) * entry.input_cost_per_1k\n        output_cost = (call.output_tokens / 1000.0) * entry.output_cost_per_1k\n        return input_cost + output_cost\n\n    def calculate_trace_cost(self, trace: Trace) -&gt; CostSummary:\n        \"\"\"Calculate the total cost for a trace.\n\n        Args:\n            trace: The execution trace to price.\n\n        Returns:\n            A CostSummary with per-model breakdown.\n\n        Raises:\n            BudgetExceededError: If budget_limit_usd is set and exceeded.\n        \"\"\"\n        breakdowns: dict[str, dict[str, Any]] = {}\n\n        for call in trace.llm_calls:\n            cost = self.calculate_llm_cost(call)\n            entry = self._pricing.entries.get(call.model)\n            input_cost = 0.0\n            output_cost = 0.0\n            if entry is not None:\n                input_cost = (call.input_tokens / 1000.0) * entry.input_cost_per_1k\n                output_cost = (call.output_tokens / 1000.0) * entry.output_cost_per_1k\n\n            if call.model not in breakdowns:\n                breakdowns[call.model] = {\n                    \"input_tokens\": 0,\n                    \"output_tokens\": 0,\n                    \"input_cost_usd\": 0.0,\n                    \"output_cost_usd\": 0.0,\n                    \"total_cost_usd\": 0.0,\n                    \"call_count\": 0,\n                }\n\n            bd = breakdowns[call.model]\n            bd[\"input_tokens\"] += call.input_tokens\n            bd[\"output_tokens\"] += call.output_tokens\n            bd[\"input_cost_usd\"] += input_cost\n            bd[\"output_cost_usd\"] += output_cost\n            bd[\"total_cost_usd\"] += cost\n            bd[\"call_count\"] += 1\n\n        model_breakdowns = {\n            model: CostBreakdown(model=model, **data) for model, data in breakdowns.items()\n        }\n\n        total_llm = sum(bd.total_cost_usd for bd in model_breakdowns.values())\n        total_input = sum(bd.input_tokens for bd in model_breakdowns.values())\n        total_output = sum(bd.output_tokens for bd in model_breakdowns.values())\n\n        summary = CostSummary(\n            total_llm_cost_usd=total_llm,\n            total_tool_cost_usd=0.0,\n            total_cost_usd=total_llm,\n            breakdown_by_model=model_breakdowns,\n            total_input_tokens=total_input,\n            total_output_tokens=total_output,\n        )\n\n        if self._budget_limit is not None and total_llm &gt; self._budget_limit:\n            raise BudgetExceededError(total_llm, self._budget_limit)\n\n        return summary\n</code></pre>"},{"location":"reference/api/cost/#agentprobe.cost.calculator.CostCalculator.__init__","title":"<code>__init__(pricing=None, budget_limit_usd=None)</code>","text":"<p>Initialize the cost calculator.</p> <p>Parameters:</p> Name Type Description Default <code>pricing</code> <code>PricingConfig | None</code> <p>Pricing configuration. Loads defaults if None.</p> <code>None</code> <code>budget_limit_usd</code> <code>float | None</code> <p>Optional budget limit in USD.</p> <code>None</code> Source code in <code>src/agentprobe/cost/calculator.py</code> <pre><code>def __init__(\n    self,\n    pricing: PricingConfig | None = None,\n    budget_limit_usd: float | None = None,\n) -&gt; None:\n    \"\"\"Initialize the cost calculator.\n\n    Args:\n        pricing: Pricing configuration. Loads defaults if None.\n        budget_limit_usd: Optional budget limit in USD.\n    \"\"\"\n    self._pricing = pricing or PricingConfig.load_from_dir()\n    self._budget_limit = budget_limit_usd\n</code></pre>"},{"location":"reference/api/cost/#agentprobe.cost.calculator.CostCalculator.calculate_llm_cost","title":"<code>calculate_llm_cost(call)</code>","text":"<p>Calculate the cost of a single LLM call.</p> <p>Parameters:</p> Name Type Description Default <code>call</code> <code>LLMCall</code> <p>The LLM call to price.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Cost in USD.</p> Source code in <code>src/agentprobe/cost/calculator.py</code> <pre><code>def calculate_llm_cost(self, call: LLMCall) -&gt; float:\n    \"\"\"Calculate the cost of a single LLM call.\n\n    Args:\n        call: The LLM call to price.\n\n    Returns:\n        Cost in USD.\n    \"\"\"\n    entry = self._pricing.entries.get(call.model)\n    if entry is None:\n        logger.warning(\"No pricing found for model: %s\", call.model)\n        return 0.0\n\n    input_cost = (call.input_tokens / 1000.0) * entry.input_cost_per_1k\n    output_cost = (call.output_tokens / 1000.0) * entry.output_cost_per_1k\n    return input_cost + output_cost\n</code></pre>"},{"location":"reference/api/cost/#agentprobe.cost.calculator.CostCalculator.calculate_trace_cost","title":"<code>calculate_trace_cost(trace)</code>","text":"<p>Calculate the total cost for a trace.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The execution trace to price.</p> required <p>Returns:</p> Type Description <code>CostSummary</code> <p>A CostSummary with per-model breakdown.</p> <p>Raises:</p> Type Description <code>BudgetExceededError</code> <p>If budget_limit_usd is set and exceeded.</p> Source code in <code>src/agentprobe/cost/calculator.py</code> <pre><code>def calculate_trace_cost(self, trace: Trace) -&gt; CostSummary:\n    \"\"\"Calculate the total cost for a trace.\n\n    Args:\n        trace: The execution trace to price.\n\n    Returns:\n        A CostSummary with per-model breakdown.\n\n    Raises:\n        BudgetExceededError: If budget_limit_usd is set and exceeded.\n    \"\"\"\n    breakdowns: dict[str, dict[str, Any]] = {}\n\n    for call in trace.llm_calls:\n        cost = self.calculate_llm_cost(call)\n        entry = self._pricing.entries.get(call.model)\n        input_cost = 0.0\n        output_cost = 0.0\n        if entry is not None:\n            input_cost = (call.input_tokens / 1000.0) * entry.input_cost_per_1k\n            output_cost = (call.output_tokens / 1000.0) * entry.output_cost_per_1k\n\n        if call.model not in breakdowns:\n            breakdowns[call.model] = {\n                \"input_tokens\": 0,\n                \"output_tokens\": 0,\n                \"input_cost_usd\": 0.0,\n                \"output_cost_usd\": 0.0,\n                \"total_cost_usd\": 0.0,\n                \"call_count\": 0,\n            }\n\n        bd = breakdowns[call.model]\n        bd[\"input_tokens\"] += call.input_tokens\n        bd[\"output_tokens\"] += call.output_tokens\n        bd[\"input_cost_usd\"] += input_cost\n        bd[\"output_cost_usd\"] += output_cost\n        bd[\"total_cost_usd\"] += cost\n        bd[\"call_count\"] += 1\n\n    model_breakdowns = {\n        model: CostBreakdown(model=model, **data) for model, data in breakdowns.items()\n    }\n\n    total_llm = sum(bd.total_cost_usd for bd in model_breakdowns.values())\n    total_input = sum(bd.input_tokens for bd in model_breakdowns.values())\n    total_output = sum(bd.output_tokens for bd in model_breakdowns.values())\n\n    summary = CostSummary(\n        total_llm_cost_usd=total_llm,\n        total_tool_cost_usd=0.0,\n        total_cost_usd=total_llm,\n        breakdown_by_model=model_breakdowns,\n        total_input_tokens=total_input,\n        total_output_tokens=total_output,\n    )\n\n    if self._budget_limit is not None and total_llm &gt; self._budget_limit:\n        raise BudgetExceededError(total_llm, self._budget_limit)\n\n    return summary\n</code></pre>"},{"location":"reference/api/cost/#budget-enforcer","title":"Budget Enforcer","text":""},{"location":"reference/api/cost/#agentprobe.cost.budget","title":"<code>agentprobe.cost.budget</code>","text":"<p>Budget enforcement for test execution cost management.</p> <p>Provides the BudgetEnforcer class for checking individual test and suite-level costs against configured budget limits.</p>"},{"location":"reference/api/cost/#agentprobe.cost.budget.BudgetEnforcer","title":"<code>BudgetEnforcer</code>","text":"<p>Enforces cost budgets for tests and suites.</p> <p>Checks actual costs against configured limits and returns verdict objects indicating whether budgets were exceeded.</p> <p>Attributes:</p> Name Type Description <code>test_budget_usd</code> <p>Maximum cost per individual test.</p> <code>suite_budget_usd</code> <p>Maximum cost per test suite run.</p> Source code in <code>src/agentprobe/cost/budget.py</code> <pre><code>class BudgetEnforcer:\n    \"\"\"Enforces cost budgets for tests and suites.\n\n    Checks actual costs against configured limits and returns\n    verdict objects indicating whether budgets were exceeded.\n\n    Attributes:\n        test_budget_usd: Maximum cost per individual test.\n        suite_budget_usd: Maximum cost per test suite run.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        test_budget_usd: float | None = None,\n        suite_budget_usd: float | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the budget enforcer.\n\n        Args:\n            test_budget_usd: Maximum cost per test in USD.\n            suite_budget_usd: Maximum cost per suite in USD.\n        \"\"\"\n        self._test_budget = test_budget_usd\n        self._suite_budget = suite_budget_usd\n\n    @staticmethod\n    def _check(actual: float, limit: float) -&gt; BudgetCheckResult:\n        \"\"\"Compare actual cost against a budget limit.\n\n        Args:\n            actual: Actual cost in USD.\n            limit: Budget limit in USD.\n\n        Returns:\n            A BudgetCheckResult with within_budget verdict.\n        \"\"\"\n        remaining = limit - actual\n        utilization = (actual / limit * 100.0) if limit &gt; 0 else 0.0\n        within = actual &lt;= limit\n        return BudgetCheckResult(\n            within_budget=within,\n            actual_cost_usd=actual,\n            budget_limit_usd=limit,\n            remaining_usd=remaining,\n            utilization_pct=round(utilization, 2),\n        )\n\n    def check_test(self, cost_summary: CostSummary) -&gt; BudgetCheckResult | None:\n        \"\"\"Check a single test's cost against the test budget.\n\n        Args:\n            cost_summary: Cost summary for the test.\n\n        Returns:\n            A BudgetCheckResult if a test budget is configured, else None.\n        \"\"\"\n        if self._test_budget is None:\n            return None\n        result = self._check(cost_summary.total_cost_usd, self._test_budget)\n        if not result.within_budget:\n            logger.warning(\n                \"Test budget exceeded: $%.4f &gt; $%.4f limit\",\n                cost_summary.total_cost_usd,\n                self._test_budget,\n            )\n        return result\n\n    def check_suite(self, cost_summaries: list[CostSummary]) -&gt; BudgetCheckResult | None:\n        \"\"\"Check a suite's total cost against the suite budget.\n\n        Args:\n            cost_summaries: Cost summaries for all tests in the suite.\n\n        Returns:\n            A BudgetCheckResult if a suite budget is configured, else None.\n        \"\"\"\n        if self._suite_budget is None:\n            return None\n        total = sum(cs.total_cost_usd for cs in cost_summaries)\n        result = self._check(total, self._suite_budget)\n        if not result.within_budget:\n            logger.warning(\n                \"Suite budget exceeded: $%.4f &gt; $%.4f limit\",\n                total,\n                self._suite_budget,\n            )\n        return result\n</code></pre>"},{"location":"reference/api/cost/#agentprobe.cost.budget.BudgetEnforcer.__init__","title":"<code>__init__(*, test_budget_usd=None, suite_budget_usd=None)</code>","text":"<p>Initialize the budget enforcer.</p> <p>Parameters:</p> Name Type Description Default <code>test_budget_usd</code> <code>float | None</code> <p>Maximum cost per test in USD.</p> <code>None</code> <code>suite_budget_usd</code> <code>float | None</code> <p>Maximum cost per suite in USD.</p> <code>None</code> Source code in <code>src/agentprobe/cost/budget.py</code> <pre><code>def __init__(\n    self,\n    *,\n    test_budget_usd: float | None = None,\n    suite_budget_usd: float | None = None,\n) -&gt; None:\n    \"\"\"Initialize the budget enforcer.\n\n    Args:\n        test_budget_usd: Maximum cost per test in USD.\n        suite_budget_usd: Maximum cost per suite in USD.\n    \"\"\"\n    self._test_budget = test_budget_usd\n    self._suite_budget = suite_budget_usd\n</code></pre>"},{"location":"reference/api/cost/#agentprobe.cost.budget.BudgetEnforcer.check_test","title":"<code>check_test(cost_summary)</code>","text":"<p>Check a single test's cost against the test budget.</p> <p>Parameters:</p> Name Type Description Default <code>cost_summary</code> <code>CostSummary</code> <p>Cost summary for the test.</p> required <p>Returns:</p> Type Description <code>BudgetCheckResult | None</code> <p>A BudgetCheckResult if a test budget is configured, else None.</p> Source code in <code>src/agentprobe/cost/budget.py</code> <pre><code>def check_test(self, cost_summary: CostSummary) -&gt; BudgetCheckResult | None:\n    \"\"\"Check a single test's cost against the test budget.\n\n    Args:\n        cost_summary: Cost summary for the test.\n\n    Returns:\n        A BudgetCheckResult if a test budget is configured, else None.\n    \"\"\"\n    if self._test_budget is None:\n        return None\n    result = self._check(cost_summary.total_cost_usd, self._test_budget)\n    if not result.within_budget:\n        logger.warning(\n            \"Test budget exceeded: $%.4f &gt; $%.4f limit\",\n            cost_summary.total_cost_usd,\n            self._test_budget,\n        )\n    return result\n</code></pre>"},{"location":"reference/api/cost/#agentprobe.cost.budget.BudgetEnforcer.check_suite","title":"<code>check_suite(cost_summaries)</code>","text":"<p>Check a suite's total cost against the suite budget.</p> <p>Parameters:</p> Name Type Description Default <code>cost_summaries</code> <code>list[CostSummary]</code> <p>Cost summaries for all tests in the suite.</p> required <p>Returns:</p> Type Description <code>BudgetCheckResult | None</code> <p>A BudgetCheckResult if a suite budget is configured, else None.</p> Source code in <code>src/agentprobe/cost/budget.py</code> <pre><code>def check_suite(self, cost_summaries: list[CostSummary]) -&gt; BudgetCheckResult | None:\n    \"\"\"Check a suite's total cost against the suite budget.\n\n    Args:\n        cost_summaries: Cost summaries for all tests in the suite.\n\n    Returns:\n        A BudgetCheckResult if a suite budget is configured, else None.\n    \"\"\"\n    if self._suite_budget is None:\n        return None\n    total = sum(cs.total_cost_usd for cs in cost_summaries)\n    result = self._check(total, self._suite_budget)\n    if not result.within_budget:\n        logger.warning(\n            \"Suite budget exceeded: $%.4f &gt; $%.4f limit\",\n            total,\n            self._suite_budget,\n        )\n    return result\n</code></pre>"},{"location":"reference/api/eval/","title":"Evaluation","text":"<p>Evaluators for assessing agent output quality.</p>"},{"location":"reference/api/eval/#rules-based-evaluator","title":"Rules-Based Evaluator","text":""},{"location":"reference/api/eval/#agentprobe.eval.rules","title":"<code>agentprobe.eval.rules</code>","text":"<p>Rule-based evaluator with configurable rules and weighted scoring.</p> <p>Provides a declarative evaluation approach using built-in rule handlers like <code>contains_any</code>, <code>not_contains</code>, <code>max_length</code>, <code>regex</code>, and <code>json_valid</code>.</p>"},{"location":"reference/api/eval/#agentprobe.eval.rules.RuleSpec","title":"<code>RuleSpec</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Specification for a single evaluation rule.</p> <p>Attributes:</p> Name Type Description <code>rule_type</code> <code>str</code> <p>The type of rule (e.g. 'contains_any', 'regex').</p> <code>params</code> <code>dict[str, Any]</code> <p>Parameters for the rule handler.</p> <code>weight</code> <code>float</code> <p>Relative weight of this rule in the overall score.</p> <code>description</code> <code>str</code> <p>Human-readable description of what this rule checks.</p> Source code in <code>src/agentprobe/eval/rules.py</code> <pre><code>class RuleSpec(BaseModel):\n    \"\"\"Specification for a single evaluation rule.\n\n    Attributes:\n        rule_type: The type of rule (e.g. 'contains_any', 'regex').\n        params: Parameters for the rule handler.\n        weight: Relative weight of this rule in the overall score.\n        description: Human-readable description of what this rule checks.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, extra=\"forbid\")\n\n    rule_type: str\n    params: dict[str, Any] = Field(default_factory=dict)\n    weight: float = Field(default=1.0, gt=0)\n    description: str = \"\"\n</code></pre>"},{"location":"reference/api/eval/#agentprobe.eval.rules.RuleBasedEvaluator","title":"<code>RuleBasedEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Evaluator that applies a set of declarative rules with weighted scoring.</p> <p>Each rule is checked against the agent output. The final score is the weighted average of passing rules.</p> <p>Attributes:</p> Name Type Description <code>rules</code> <p>List of rule specifications to evaluate.</p> Source code in <code>src/agentprobe/eval/rules.py</code> <pre><code>class RuleBasedEvaluator(BaseEvaluator):\n    \"\"\"Evaluator that applies a set of declarative rules with weighted scoring.\n\n    Each rule is checked against the agent output. The final score is\n    the weighted average of passing rules.\n\n    Attributes:\n        rules: List of rule specifications to evaluate.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"rule-based\",\n        rules: list[RuleSpec] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the rule-based evaluator.\n\n        Args:\n            name: Evaluator name.\n            rules: List of rule specifications. Defaults to empty.\n        \"\"\"\n        super().__init__(name)\n        self.rules = rules or []\n\n    async def _evaluate(self, test_case: TestCase, trace: Trace) -&gt; EvalResult:\n        \"\"\"Evaluate the trace output against all configured rules.\n\n        Args:\n            test_case: The test case that was executed.\n            trace: The execution trace to evaluate.\n\n        Returns:\n            An evaluation result with weighted score.\n        \"\"\"\n        if not self.rules:\n            return EvalResult(\n                evaluator_name=self.name,\n                verdict=EvalVerdict.PASS,\n                score=1.0,\n                reason=\"No rules configured \u2014 pass by default\",\n            )\n\n        output = trace.output_text\n        total_weight = 0.0\n        weighted_score = 0.0\n        results: list[dict[str, Any]] = []\n\n        for rule in self.rules:\n            handler = _RULE_HANDLERS.get(rule.rule_type)\n            if handler is None:\n                logger.warning(\"Unknown rule type: %s\", rule.rule_type)\n                results.append(\n                    {\n                        \"rule\": rule.rule_type,\n                        \"passed\": False,\n                        \"error\": \"unknown rule type\",\n                    }\n                )\n                total_weight += rule.weight\n                continue\n\n            passed = handler(output, rule.params)\n            total_weight += rule.weight\n            if passed:\n                weighted_score += rule.weight\n\n            results.append(\n                {\n                    \"rule\": rule.rule_type,\n                    \"description\": rule.description,\n                    \"passed\": passed,\n                    \"weight\": rule.weight,\n                }\n            )\n\n        score = weighted_score / total_weight if total_weight &gt; 0 else 0.0\n        all_passed = all(r[\"passed\"] for r in results)\n\n        _partial_threshold = 0.5\n        if all_passed:\n            verdict = EvalVerdict.PASS\n        elif score &gt;= _partial_threshold:\n            verdict = EvalVerdict.PARTIAL\n        else:\n            verdict = EvalVerdict.FAIL\n\n        return EvalResult(\n            evaluator_name=self.name,\n            verdict=verdict,\n            score=score,\n            reason=f\"{int(weighted_score)}/{int(total_weight)} rules passed (weighted)\",\n            metadata={\"rule_results\": results},\n        )\n</code></pre>"},{"location":"reference/api/eval/#agentprobe.eval.rules.RuleBasedEvaluator.__init__","title":"<code>__init__(name='rule-based', rules=None)</code>","text":"<p>Initialize the rule-based evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Evaluator name.</p> <code>'rule-based'</code> <code>rules</code> <code>list[RuleSpec] | None</code> <p>List of rule specifications. Defaults to empty.</p> <code>None</code> Source code in <code>src/agentprobe/eval/rules.py</code> <pre><code>def __init__(\n    self,\n    name: str = \"rule-based\",\n    rules: list[RuleSpec] | None = None,\n) -&gt; None:\n    \"\"\"Initialize the rule-based evaluator.\n\n    Args:\n        name: Evaluator name.\n        rules: List of rule specifications. Defaults to empty.\n    \"\"\"\n    super().__init__(name)\n    self.rules = rules or []\n</code></pre>"},{"location":"reference/api/eval/#embedding-evaluator","title":"Embedding Evaluator","text":""},{"location":"reference/api/eval/#agentprobe.eval.embedding","title":"<code>agentprobe.eval.embedding</code>","text":"<p>Embedding similarity evaluator using cosine similarity.</p> <p>Compares agent output embeddings against expected output embeddings to produce a similarity score.</p>"},{"location":"reference/api/eval/#agentprobe.eval.embedding.EmbeddingSimilarityEvaluator","title":"<code>EmbeddingSimilarityEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Evaluator that compares embeddings via cosine similarity.</p> <p>Obtains embeddings for expected and actual outputs from an embedding API, then computes cosine similarity. A threshold determines pass/fail.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>Embedding model identifier.</p> <code>provider</code> <p>API provider ('openai').</p> <code>threshold</code> <p>Minimum similarity score to pass.</p> Source code in <code>src/agentprobe/eval/embedding.py</code> <pre><code>class EmbeddingSimilarityEvaluator(BaseEvaluator):\n    \"\"\"Evaluator that compares embeddings via cosine similarity.\n\n    Obtains embeddings for expected and actual outputs from an\n    embedding API, then computes cosine similarity. A threshold\n    determines pass/fail.\n\n    Attributes:\n        model: Embedding model identifier.\n        provider: API provider ('openai').\n        threshold: Minimum similarity score to pass.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        model: str = \"text-embedding-3-small\",\n        provider: str = \"openai\",\n        api_key: str | None = None,\n        threshold: float = 0.8,\n        name: str = \"embedding-similarity\",\n    ) -&gt; None:\n        \"\"\"Initialize the embedding similarity evaluator.\n\n        Args:\n            model: Embedding model name.\n            provider: API provider.\n            api_key: API key. Read from environment if None.\n            threshold: Minimum similarity to pass.\n            name: Evaluator name.\n        \"\"\"\n        super().__init__(name)\n        self.model = model\n        self.provider = provider\n        self._api_key = api_key\n        self.threshold = threshold\n        self._cache: dict[str, list[float]] = {}\n\n    async def _evaluate(self, test_case: TestCase, trace: Trace) -&gt; EvalResult:\n        \"\"\"Compare embeddings of expected and actual output.\n\n        Args:\n            test_case: Test case with expected output.\n            trace: Execution trace with actual output.\n\n        Returns:\n            Evaluation result based on cosine similarity.\n        \"\"\"\n        if not test_case.expected_output:\n            return EvalResult(\n                evaluator_name=self.name,\n                verdict=EvalVerdict.PASS,\n                score=1.0,\n                reason=\"No expected output \u2014 skip embedding comparison\",\n            )\n\n        expected_emb = await self._get_embedding(test_case.expected_output)\n        actual_emb = await self._get_embedding(trace.output_text)\n\n        similarity = cosine_similarity(expected_emb, actual_emb)\n        score = max(0.0, min(1.0, similarity))\n\n        if score &gt;= self.threshold:\n            verdict = EvalVerdict.PASS\n        elif score &gt;= self.threshold * 0.75:\n            verdict = EvalVerdict.PARTIAL\n        else:\n            verdict = EvalVerdict.FAIL\n\n        return EvalResult(\n            evaluator_name=self.name,\n            verdict=verdict,\n            score=score,\n            reason=f\"Cosine similarity: {similarity:.4f} (threshold: {self.threshold})\",\n            metadata={\"similarity\": similarity, \"threshold\": self.threshold},\n        )\n\n    async def _get_embedding(self, text: str) -&gt; list[float]:\n        \"\"\"Get the embedding for a text string, using cache.\n\n        Args:\n            text: The text to embed.\n\n        Returns:\n            Embedding vector.\n        \"\"\"\n        if text in self._cache:\n            return self._cache[text]\n\n        embedding = await self._call_embedding_api(text)\n        self._cache[text] = embedding\n        return embedding\n\n    async def _call_embedding_api(self, text: str) -&gt; list[float]:  # pragma: no cover\n        \"\"\"Call the embedding API.\n\n        Args:\n            text: Text to embed.\n\n        Returns:\n            Embedding vector.\n\n        Raises:\n            EvaluatorError: If the API call fails.\n        \"\"\"\n        import os\n\n        api_key = self._api_key or os.environ.get(\"OPENAI_API_KEY\")\n        if not api_key:\n            raise EvaluatorError(\"OPENAI_API_KEY not set for embedding API\")\n\n        url = \"https://api.openai.com/v1/embeddings\"\n        headers = {\n            \"Authorization\": f\"Bearer {api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n        payload = {\"model\": self.model, \"input\": text}\n\n        _http_ok = 200\n        async with (\n            aiohttp.ClientSession() as session,\n            session.post(url, json=payload, headers=headers) as resp,\n        ):\n            if resp.status != _http_ok:\n                body = await resp.text()\n                raise EvaluatorError(f\"Embedding API error: {resp.status} \u2014 {body}\")\n            data = await resp.json()\n            embedding: list[float] = data[\"data\"][0][\"embedding\"]\n            return embedding\n</code></pre>"},{"location":"reference/api/eval/#agentprobe.eval.embedding.EmbeddingSimilarityEvaluator.__init__","title":"<code>__init__(*, model='text-embedding-3-small', provider='openai', api_key=None, threshold=0.8, name='embedding-similarity')</code>","text":"<p>Initialize the embedding similarity evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Embedding model name.</p> <code>'text-embedding-3-small'</code> <code>provider</code> <code>str</code> <p>API provider.</p> <code>'openai'</code> <code>api_key</code> <code>str | None</code> <p>API key. Read from environment if None.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Minimum similarity to pass.</p> <code>0.8</code> <code>name</code> <code>str</code> <p>Evaluator name.</p> <code>'embedding-similarity'</code> Source code in <code>src/agentprobe/eval/embedding.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model: str = \"text-embedding-3-small\",\n    provider: str = \"openai\",\n    api_key: str | None = None,\n    threshold: float = 0.8,\n    name: str = \"embedding-similarity\",\n) -&gt; None:\n    \"\"\"Initialize the embedding similarity evaluator.\n\n    Args:\n        model: Embedding model name.\n        provider: API provider.\n        api_key: API key. Read from environment if None.\n        threshold: Minimum similarity to pass.\n        name: Evaluator name.\n    \"\"\"\n    super().__init__(name)\n    self.model = model\n    self.provider = provider\n    self._api_key = api_key\n    self.threshold = threshold\n    self._cache: dict[str, list[float]] = {}\n</code></pre>"},{"location":"reference/api/eval/#agentprobe.eval.embedding.cosine_similarity","title":"<code>cosine_similarity(vec_a, vec_b)</code>","text":"<p>Compute cosine similarity between two vectors.</p> <p>Parameters:</p> Name Type Description Default <code>vec_a</code> <code>list[float]</code> <p>First vector.</p> required <code>vec_b</code> <code>list[float]</code> <p>Second vector.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Cosine similarity score in [-1.0, 1.0].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If vectors have different lengths or are empty.</p> Source code in <code>src/agentprobe/eval/embedding.py</code> <pre><code>def cosine_similarity(vec_a: list[float], vec_b: list[float]) -&gt; float:\n    \"\"\"Compute cosine similarity between two vectors.\n\n    Args:\n        vec_a: First vector.\n        vec_b: Second vector.\n\n    Returns:\n        Cosine similarity score in [-1.0, 1.0].\n\n    Raises:\n        ValueError: If vectors have different lengths or are empty.\n    \"\"\"\n    if len(vec_a) != len(vec_b):\n        msg = f\"Vector length mismatch: {len(vec_a)} vs {len(vec_b)}\"\n        raise ValueError(msg)\n\n    if len(vec_a) == 0:\n        msg = \"Cannot compute similarity of empty vectors\"\n        raise ValueError(msg)\n\n    dot = sum(a * b for a, b in zip(vec_a, vec_b, strict=True))\n    norm_a = math.sqrt(sum(a * a for a in vec_a))\n    norm_b = math.sqrt(sum(b * b for b in vec_b))\n\n    if norm_a == 0.0 or norm_b == 0.0:\n        return 0.0\n\n    return dot / (norm_a * norm_b)\n</code></pre>"},{"location":"reference/api/eval/#judge-evaluator","title":"Judge Evaluator","text":""},{"location":"reference/api/eval/#agentprobe.eval.llm_judge","title":"<code>agentprobe.eval.llm_judge</code>","text":"<p>Judge evaluator that uses a language model to assess agent outputs.</p> <p>Sends the agent's output along with a rubric to a judge model and parses the structured JSON response into an EvalResult.</p>"},{"location":"reference/api/eval/#agentprobe.eval.llm_judge.LLMJudge","title":"<code>LLMJudge</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Evaluator that uses a language model as a judge.</p> <p>Calls an external model API (Anthropic or OpenAI) with the agent's output and a rubric, then parses the JSON verdict response.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The judge model identifier.</p> <code>provider</code> <p>API provider ('anthropic' or 'openai').</p> <code>temperature</code> <p>Sampling temperature for the judge.</p> <code>max_tokens</code> <p>Maximum response tokens.</p> <code>rubric</code> <p>Evaluation rubric/criteria text.</p> Source code in <code>src/agentprobe/eval/llm_judge.py</code> <pre><code>class LLMJudge(BaseEvaluator):\n    \"\"\"Evaluator that uses a language model as a judge.\n\n    Calls an external model API (Anthropic or OpenAI) with the agent's\n    output and a rubric, then parses the JSON verdict response.\n\n    Attributes:\n        model: The judge model identifier.\n        provider: API provider ('anthropic' or 'openai').\n        temperature: Sampling temperature for the judge.\n        max_tokens: Maximum response tokens.\n        rubric: Evaluation rubric/criteria text.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        model: str = \"claude-sonnet-4-5-20250929\",\n        provider: str = \"anthropic\",\n        api_key: str | None = None,\n        temperature: float = 0.0,\n        max_tokens: int = 1024,\n        rubric: str = \"\",\n        name: str = \"llm-judge\",\n    ) -&gt; None:\n        \"\"\"Initialize the judge evaluator.\n\n        Args:\n            model: Judge model identifier.\n            provider: API provider name.\n            api_key: API key. Read from environment if None.\n            temperature: Sampling temperature.\n            max_tokens: Max response tokens.\n            rubric: Evaluation criteria text.\n            name: Evaluator name.\n        \"\"\"\n        super().__init__(name)\n        self.model = model\n        self.provider = provider\n        self._api_key = api_key\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.rubric = rubric\n\n    async def _evaluate(self, test_case: TestCase, trace: Trace) -&gt; EvalResult:\n        \"\"\"Send the output to the judge model and parse the verdict.\n\n        Args:\n            test_case: The test case.\n            trace: The execution trace.\n\n        Returns:\n            Parsed evaluation result from the judge.\n        \"\"\"\n        prompt = self._build_prompt(test_case, trace)\n        response_text = await self._call_api(prompt)\n        return self._parse_response(response_text)\n\n    def _build_prompt(self, test_case: TestCase, trace: Trace) -&gt; str:\n        \"\"\"Build the evaluation prompt for the judge.\n\n        Args:\n            test_case: The test case with expectations.\n            trace: The execution trace with the output.\n\n        Returns:\n            Formatted prompt string.\n        \"\"\"\n        parts = [f\"## Agent Input\\n{test_case.input_text}\"]\n\n        if test_case.expected_output:\n            parts.append(f\"## Expected Output\\n{test_case.expected_output}\")\n\n        parts.append(f\"## Actual Output\\n{trace.output_text}\")\n\n        if self.rubric:\n            parts.append(f\"## Evaluation Criteria\\n{self.rubric}\")\n\n        return \"\\n\\n\".join(parts)\n\n    async def _call_api(self, prompt: str) -&gt; str:\n        \"\"\"Call the judge model API.\n\n        Args:\n            prompt: The evaluation prompt.\n\n        Returns:\n            Raw response text from the judge.\n\n        Raises:\n            JudgeAPIError: If the API call fails.\n        \"\"\"\n        if self.provider == \"anthropic\":\n            return await self._call_anthropic(prompt)\n        elif self.provider == \"openai\":\n            return await self._call_openai(prompt)\n        else:\n            raise JudgeAPIError(self.model, 0, f\"Unknown provider: {self.provider}\")\n\n    async def _call_anthropic(self, prompt: str) -&gt; str:  # pragma: no cover\n        \"\"\"Call the Anthropic Messages API.\n\n        Args:\n            prompt: The evaluation prompt.\n\n        Returns:\n            Response text.\n        \"\"\"\n        import os\n\n        api_key = self._api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n        if not api_key:\n            raise JudgeAPIError(self.model, 0, \"ANTHROPIC_API_KEY not set\")\n\n        url = \"https://api.anthropic.com/v1/messages\"\n        headers = {\n            \"x-api-key\": api_key,\n            \"anthropic-version\": \"2023-06-01\",\n            \"content-type\": \"application/json\",\n        }\n        payload = {\n            \"model\": self.model,\n            \"max_tokens\": self.max_tokens,\n            \"temperature\": self.temperature,\n            \"system\": _DEFAULT_SYSTEM_PROMPT,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        }\n\n        _http_ok = 200\n        async with (\n            aiohttp.ClientSession() as session,\n            session.post(url, json=payload, headers=headers) as resp,\n        ):\n            if resp.status != _http_ok:\n                body = await resp.text()\n                raise JudgeAPIError(self.model, resp.status, body)\n            data = await resp.json()\n            return str(data[\"content\"][0][\"text\"])\n\n    async def _call_openai(self, prompt: str) -&gt; str:  # pragma: no cover\n        \"\"\"Call the OpenAI Chat Completions API.\n\n        Args:\n            prompt: The evaluation prompt.\n\n        Returns:\n            Response text.\n        \"\"\"\n        import os\n\n        api_key = self._api_key or os.environ.get(\"OPENAI_API_KEY\")\n        if not api_key:\n            raise JudgeAPIError(self.model, 0, \"OPENAI_API_KEY not set\")\n\n        url = \"https://api.openai.com/v1/chat/completions\"\n        headers = {\n            \"Authorization\": f\"Bearer {api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n        payload = {\n            \"model\": self.model,\n            \"max_tokens\": self.max_tokens,\n            \"temperature\": self.temperature,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": _DEFAULT_SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n        }\n\n        _http_ok = 200\n        async with (\n            aiohttp.ClientSession() as session,\n            session.post(url, json=payload, headers=headers) as resp,\n        ):\n            if resp.status != _http_ok:\n                body = await resp.text()\n                raise JudgeAPIError(self.model, resp.status, body)\n            data = await resp.json()\n            return str(data[\"choices\"][0][\"message\"][\"content\"])\n\n    def _parse_response(self, response_text: str) -&gt; EvalResult:\n        \"\"\"Parse the judge's JSON response into an EvalResult.\n\n        Args:\n            response_text: Raw response text.\n\n        Returns:\n            Parsed EvalResult.\n        \"\"\"\n        try:\n            data = json.loads(response_text)\n        except json.JSONDecodeError:\n            cleaned = response_text.strip()\n            start = cleaned.find(\"{\")\n            end = cleaned.rfind(\"}\") + 1\n            if start &gt;= 0 and end &gt; start:\n                try:\n                    data = json.loads(cleaned[start:end])\n                except json.JSONDecodeError:\n                    return EvalResult(\n                        evaluator_name=self.name,\n                        verdict=EvalVerdict.ERROR,\n                        score=0.0,\n                        reason=f\"Failed to parse judge response: {response_text[:200]}\",\n                    )\n            else:\n                return EvalResult(\n                    evaluator_name=self.name,\n                    verdict=EvalVerdict.ERROR,\n                    score=0.0,\n                    reason=f\"No JSON found in judge response: {response_text[:200]}\",\n                )\n\n        verdict_str = str(data.get(\"verdict\", \"error\")).lower()\n        verdict_map = {\n            \"pass\": EvalVerdict.PASS,\n            \"fail\": EvalVerdict.FAIL,\n            \"partial\": EvalVerdict.PARTIAL,\n        }\n        verdict = verdict_map.get(verdict_str, EvalVerdict.ERROR)\n\n        score = float(data.get(\"score\", 0.0))\n        score = max(0.0, min(1.0, score))\n\n        reason = str(data.get(\"reason\", \"\"))\n\n        return EvalResult(\n            evaluator_name=self.name,\n            verdict=verdict,\n            score=score,\n            reason=reason,\n        )\n</code></pre>"},{"location":"reference/api/eval/#agentprobe.eval.llm_judge.LLMJudge.__init__","title":"<code>__init__(*, model='claude-sonnet-4-5-20250929', provider='anthropic', api_key=None, temperature=0.0, max_tokens=1024, rubric='', name='llm-judge')</code>","text":"<p>Initialize the judge evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Judge model identifier.</p> <code>'claude-sonnet-4-5-20250929'</code> <code>provider</code> <code>str</code> <p>API provider name.</p> <code>'anthropic'</code> <code>api_key</code> <code>str | None</code> <p>API key. Read from environment if None.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature.</p> <code>0.0</code> <code>max_tokens</code> <code>int</code> <p>Max response tokens.</p> <code>1024</code> <code>rubric</code> <code>str</code> <p>Evaluation criteria text.</p> <code>''</code> <code>name</code> <code>str</code> <p>Evaluator name.</p> <code>'llm-judge'</code> Source code in <code>src/agentprobe/eval/llm_judge.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model: str = \"claude-sonnet-4-5-20250929\",\n    provider: str = \"anthropic\",\n    api_key: str | None = None,\n    temperature: float = 0.0,\n    max_tokens: int = 1024,\n    rubric: str = \"\",\n    name: str = \"llm-judge\",\n) -&gt; None:\n    \"\"\"Initialize the judge evaluator.\n\n    Args:\n        model: Judge model identifier.\n        provider: API provider name.\n        api_key: API key. Read from environment if None.\n        temperature: Sampling temperature.\n        max_tokens: Max response tokens.\n        rubric: Evaluation criteria text.\n        name: Evaluator name.\n    \"\"\"\n    super().__init__(name)\n    self.model = model\n    self.provider = provider\n    self._api_key = api_key\n    self.temperature = temperature\n    self.max_tokens = max_tokens\n    self.rubric = rubric\n</code></pre>"},{"location":"reference/api/eval/#statistical-evaluator","title":"Statistical Evaluator","text":""},{"location":"reference/api/eval/#agentprobe.eval.statistical","title":"<code>agentprobe.eval.statistical</code>","text":"<p>Statistical evaluator for repeated evaluation with aggregated metrics.</p> <p>Wraps an inner evaluator and runs it multiple times across pre-collected traces, computing mean, standard deviation, percentiles, and confidence intervals from the score distribution.</p>"},{"location":"reference/api/eval/#agentprobe.eval.statistical.StatisticalEvaluator","title":"<code>StatisticalEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Evaluator that runs an inner evaluator multiple times and aggregates stats.</p> <p>Wraps another evaluator and runs it against multiple traces for the same test case, computing distributional statistics on the resulting scores.</p> <p>Attributes:</p> Name Type Description <code>inner</code> <code>BaseEvaluator</code> <p>The wrapped evaluator instance.</p> <code>pass_threshold</code> <code>BaseEvaluator</code> <p>Minimum mean score to consider a pass.</p> Source code in <code>src/agentprobe/eval/statistical.py</code> <pre><code>class StatisticalEvaluator(BaseEvaluator):\n    \"\"\"Evaluator that runs an inner evaluator multiple times and aggregates stats.\n\n    Wraps another evaluator and runs it against multiple traces for the same\n    test case, computing distributional statistics on the resulting scores.\n\n    Attributes:\n        inner: The wrapped evaluator instance.\n        pass_threshold: Minimum mean score to consider a pass.\n    \"\"\"\n\n    def __init__(\n        self,\n        inner: BaseEvaluator,\n        *,\n        name: str | None = None,\n        pass_threshold: float = 0.7,\n    ) -&gt; None:\n        \"\"\"Initialize the statistical evaluator.\n\n        Args:\n            inner: The evaluator to wrap and run repeatedly.\n            name: Optional name override. Defaults to 'statistical-{inner.name}'.\n            pass_threshold: Minimum mean score for a pass verdict.\n        \"\"\"\n        resolved_name = name or f\"statistical-{inner.name}\"\n        super().__init__(resolved_name)\n        self._inner = inner\n        self._pass_threshold = pass_threshold\n\n    @property\n    def inner(self) -&gt; BaseEvaluator:\n        \"\"\"Return the wrapped evaluator.\"\"\"\n        return self._inner\n\n    async def _evaluate(self, test_case: TestCase, trace: Trace) -&gt; EvalResult:\n        \"\"\"Run the inner evaluator once (single-trace mode).\n\n        For statistical analysis, use ``evaluate_multiple()`` instead.\n\n        Args:\n            test_case: The test case.\n            trace: A single trace to evaluate.\n\n        Returns:\n            The inner evaluator's result.\n        \"\"\"\n        return await self._inner.evaluate(test_case, trace)\n\n    async def evaluate_multiple(\n        self,\n        test_case: TestCase,\n        traces: Sequence[Trace],\n    ) -&gt; StatisticalSummary:\n        \"\"\"Evaluate multiple traces and compute aggregate statistics.\n\n        Runs the inner evaluator on each trace, collects scores, and\n        computes mean, standard deviation, median, percentiles, and\n        a 95% confidence interval.\n\n        Args:\n            test_case: The test case specification.\n            traces: Pre-collected traces to evaluate.\n\n        Returns:\n            A statistical summary of the score distribution.\n        \"\"\"\n        scores: list[float] = []\n        for trace in traces:\n            result = await self._inner.evaluate(test_case, trace)\n            scores.append(result.score)\n\n        if not scores:\n            return StatisticalSummary(\n                evaluator_name=self.name,\n                sample_count=1,\n                scores=(0.0,),\n                mean=0.0,\n                std_dev=0.0,\n                median=0.0,\n                p5=0.0,\n                p95=0.0,\n                ci_lower=0.0,\n                ci_upper=0.0,\n            )\n\n        n = len(scores)\n        mean = statistics.mean(scores)\n        std_dev = statistics.stdev(scores) if n &gt; 1 else 0.0\n\n        sorted_scores = sorted(scores)\n        median = statistics.median(sorted_scores)\n        p5 = _percentile(sorted_scores, 5)\n        p95 = _percentile(sorted_scores, 95)\n\n        # 95% confidence interval using t-distribution approximation\n        if n &gt; 1:\n            se = std_dev / math.sqrt(n)\n            # Approximate t-value for 95% CI (use 1.96 for large n)\n            t_val = 1.96\n            ci_lower = max(0.0, mean - t_val * se)\n            ci_upper = min(1.0, mean + t_val * se)\n        else:\n            ci_lower = mean\n            ci_upper = mean\n\n        return StatisticalSummary(\n            evaluator_name=self.name,\n            sample_count=n,\n            scores=tuple(scores),\n            mean=round(mean, 6),\n            std_dev=round(std_dev, 6),\n            median=round(median, 6),\n            p5=round(p5, 6),\n            p95=round(p95, 6),\n            ci_lower=round(ci_lower, 6),\n            ci_upper=round(ci_upper, 6),\n        )\n\n    def summary_to_eval_result(self, summary: StatisticalSummary) -&gt; EvalResult:\n        \"\"\"Convert a statistical summary into a standard EvalResult.\n\n        Args:\n            summary: The summary to convert.\n\n        Returns:\n            An EvalResult with the mean score and appropriate verdict.\n        \"\"\"\n        _partial_threshold = 0.5\n        if summary.mean &gt;= self._pass_threshold:\n            verdict = EvalVerdict.PASS\n        elif summary.mean &gt;= _partial_threshold:\n            verdict = EvalVerdict.PARTIAL\n        else:\n            verdict = EvalVerdict.FAIL\n\n        return EvalResult(\n            evaluator_name=self.name,\n            verdict=verdict,\n            score=summary.mean,\n            reason=(\n                f\"Statistical: mean={summary.mean:.3f}, \"\n                f\"std={summary.std_dev:.3f}, n={summary.sample_count}\"\n            ),\n            metadata={\n                \"std_dev\": summary.std_dev,\n                \"median\": summary.median,\n                \"p5\": summary.p5,\n                \"p95\": summary.p95,\n                \"ci_lower\": summary.ci_lower,\n                \"ci_upper\": summary.ci_upper,\n                \"sample_count\": summary.sample_count,\n            },\n        )\n</code></pre>"},{"location":"reference/api/eval/#agentprobe.eval.statistical.StatisticalEvaluator.inner","title":"<code>inner</code>  <code>property</code>","text":"<p>Return the wrapped evaluator.</p>"},{"location":"reference/api/eval/#agentprobe.eval.statistical.StatisticalEvaluator.__init__","title":"<code>__init__(inner, *, name=None, pass_threshold=0.7)</code>","text":"<p>Initialize the statistical evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>inner</code> <code>BaseEvaluator</code> <p>The evaluator to wrap and run repeatedly.</p> required <code>name</code> <code>str | None</code> <p>Optional name override. Defaults to 'statistical-{inner.name}'.</p> <code>None</code> <code>pass_threshold</code> <code>float</code> <p>Minimum mean score for a pass verdict.</p> <code>0.7</code> Source code in <code>src/agentprobe/eval/statistical.py</code> <pre><code>def __init__(\n    self,\n    inner: BaseEvaluator,\n    *,\n    name: str | None = None,\n    pass_threshold: float = 0.7,\n) -&gt; None:\n    \"\"\"Initialize the statistical evaluator.\n\n    Args:\n        inner: The evaluator to wrap and run repeatedly.\n        name: Optional name override. Defaults to 'statistical-{inner.name}'.\n        pass_threshold: Minimum mean score for a pass verdict.\n    \"\"\"\n    resolved_name = name or f\"statistical-{inner.name}\"\n    super().__init__(resolved_name)\n    self._inner = inner\n    self._pass_threshold = pass_threshold\n</code></pre>"},{"location":"reference/api/eval/#agentprobe.eval.statistical.StatisticalEvaluator.evaluate_multiple","title":"<code>evaluate_multiple(test_case, traces)</code>  <code>async</code>","text":"<p>Evaluate multiple traces and compute aggregate statistics.</p> <p>Runs the inner evaluator on each trace, collects scores, and computes mean, standard deviation, median, percentiles, and a 95% confidence interval.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case specification.</p> required <code>traces</code> <code>Sequence[Trace]</code> <p>Pre-collected traces to evaluate.</p> required <p>Returns:</p> Type Description <code>StatisticalSummary</code> <p>A statistical summary of the score distribution.</p> Source code in <code>src/agentprobe/eval/statistical.py</code> <pre><code>async def evaluate_multiple(\n    self,\n    test_case: TestCase,\n    traces: Sequence[Trace],\n) -&gt; StatisticalSummary:\n    \"\"\"Evaluate multiple traces and compute aggregate statistics.\n\n    Runs the inner evaluator on each trace, collects scores, and\n    computes mean, standard deviation, median, percentiles, and\n    a 95% confidence interval.\n\n    Args:\n        test_case: The test case specification.\n        traces: Pre-collected traces to evaluate.\n\n    Returns:\n        A statistical summary of the score distribution.\n    \"\"\"\n    scores: list[float] = []\n    for trace in traces:\n        result = await self._inner.evaluate(test_case, trace)\n        scores.append(result.score)\n\n    if not scores:\n        return StatisticalSummary(\n            evaluator_name=self.name,\n            sample_count=1,\n            scores=(0.0,),\n            mean=0.0,\n            std_dev=0.0,\n            median=0.0,\n            p5=0.0,\n            p95=0.0,\n            ci_lower=0.0,\n            ci_upper=0.0,\n        )\n\n    n = len(scores)\n    mean = statistics.mean(scores)\n    std_dev = statistics.stdev(scores) if n &gt; 1 else 0.0\n\n    sorted_scores = sorted(scores)\n    median = statistics.median(sorted_scores)\n    p5 = _percentile(sorted_scores, 5)\n    p95 = _percentile(sorted_scores, 95)\n\n    # 95% confidence interval using t-distribution approximation\n    if n &gt; 1:\n        se = std_dev / math.sqrt(n)\n        # Approximate t-value for 95% CI (use 1.96 for large n)\n        t_val = 1.96\n        ci_lower = max(0.0, mean - t_val * se)\n        ci_upper = min(1.0, mean + t_val * se)\n    else:\n        ci_lower = mean\n        ci_upper = mean\n\n    return StatisticalSummary(\n        evaluator_name=self.name,\n        sample_count=n,\n        scores=tuple(scores),\n        mean=round(mean, 6),\n        std_dev=round(std_dev, 6),\n        median=round(median, 6),\n        p5=round(p5, 6),\n        p95=round(p95, 6),\n        ci_lower=round(ci_lower, 6),\n        ci_upper=round(ci_upper, 6),\n    )\n</code></pre>"},{"location":"reference/api/eval/#agentprobe.eval.statistical.StatisticalEvaluator.summary_to_eval_result","title":"<code>summary_to_eval_result(summary)</code>","text":"<p>Convert a statistical summary into a standard EvalResult.</p> <p>Parameters:</p> Name Type Description Default <code>summary</code> <code>StatisticalSummary</code> <p>The summary to convert.</p> required <p>Returns:</p> Type Description <code>EvalResult</code> <p>An EvalResult with the mean score and appropriate verdict.</p> Source code in <code>src/agentprobe/eval/statistical.py</code> <pre><code>def summary_to_eval_result(self, summary: StatisticalSummary) -&gt; EvalResult:\n    \"\"\"Convert a statistical summary into a standard EvalResult.\n\n    Args:\n        summary: The summary to convert.\n\n    Returns:\n        An EvalResult with the mean score and appropriate verdict.\n    \"\"\"\n    _partial_threshold = 0.5\n    if summary.mean &gt;= self._pass_threshold:\n        verdict = EvalVerdict.PASS\n    elif summary.mean &gt;= _partial_threshold:\n        verdict = EvalVerdict.PARTIAL\n    else:\n        verdict = EvalVerdict.FAIL\n\n    return EvalResult(\n        evaluator_name=self.name,\n        verdict=verdict,\n        score=summary.mean,\n        reason=(\n            f\"Statistical: mean={summary.mean:.3f}, \"\n            f\"std={summary.std_dev:.3f}, n={summary.sample_count}\"\n        ),\n        metadata={\n            \"std_dev\": summary.std_dev,\n            \"median\": summary.median,\n            \"p5\": summary.p5,\n            \"p95\": summary.p95,\n            \"ci_lower\": summary.ci_lower,\n            \"ci_upper\": summary.ci_upper,\n            \"sample_count\": summary.sample_count,\n        },\n    )\n</code></pre>"},{"location":"reference/api/eval/#trace-comparison-evaluator","title":"Trace Comparison Evaluator","text":""},{"location":"reference/api/eval/#agentprobe.eval.trace_compare","title":"<code>agentprobe.eval.trace_compare</code>","text":"<p>Trace comparison evaluator with weighted multi-dimension scoring.</p> <p>Compares two traces across tool sequences, tool parameters, output similarity, and cost deviation, producing a weighted composite score.</p>"},{"location":"reference/api/eval/#agentprobe.eval.trace_compare.TraceComparisonEvaluator","title":"<code>TraceComparisonEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Evaluator that compares a trace against a reference trace.</p> <p>Computes similarity across multiple dimensions with configurable weights: tool sequence, tool parameters, output text, and cost.</p> <p>Attributes:</p> Name Type Description <code>reference_trace</code> <p>The reference trace to compare against.</p> <code>weights</code> <p>Per-dimension weight configuration.</p> Source code in <code>src/agentprobe/eval/trace_compare.py</code> <pre><code>class TraceComparisonEvaluator(BaseEvaluator):\n    \"\"\"Evaluator that compares a trace against a reference trace.\n\n    Computes similarity across multiple dimensions with configurable\n    weights: tool sequence, tool parameters, output text, and cost.\n\n    Attributes:\n        reference_trace: The reference trace to compare against.\n        weights: Per-dimension weight configuration.\n    \"\"\"\n\n    DEFAULT_WEIGHTS: ClassVar[dict[str, float]] = {\n        \"tool_sequence\": 0.3,\n        \"tool_parameters\": 0.2,\n        \"output_similarity\": 0.35,\n        \"cost_deviation\": 0.15,\n    }\n\n    def __init__(\n        self,\n        reference_trace: Trace,\n        *,\n        name: str = \"trace-compare\",\n        weights: dict[str, float] | None = None,\n        pass_threshold: float = 0.7,\n    ) -&gt; None:\n        \"\"\"Initialize the trace comparison evaluator.\n\n        Args:\n            reference_trace: The baseline trace to compare against.\n            name: Evaluator name.\n            weights: Dimension weight overrides.\n            pass_threshold: Minimum score for a pass verdict.\n        \"\"\"\n        super().__init__(name)\n        self._reference = reference_trace\n        self._weights = weights or dict(self.DEFAULT_WEIGHTS)\n        self._pass_threshold = pass_threshold\n\n    async def _evaluate(self, test_case: TestCase, trace: Trace) -&gt; EvalResult:\n        \"\"\"Compare the trace against the reference.\n\n        Args:\n            test_case: The test case (used for context).\n            trace: The current trace to compare.\n\n        Returns:\n            An evaluation result with the composite similarity score.\n        \"\"\"\n        scores: dict[str, float] = {}\n\n        # Tool sequence similarity (Levenshtein)\n        ref_tools = [tc.tool_name for tc in self._reference.tool_calls]\n        cur_tools = [tc.tool_name for tc in trace.tool_calls]\n        scores[\"tool_sequence\"] = _levenshtein_similarity(ref_tools, cur_tools)\n\n        # Tool parameter similarity (Jaccard on parameter keys)\n        ref_params = _collect_param_keys(self._reference)\n        cur_params = _collect_param_keys(trace)\n        scores[\"tool_parameters\"] = _jaccard_similarity(ref_params, cur_params)\n\n        # Output text similarity (word-level Jaccard)\n        scores[\"output_similarity\"] = _keyword_overlap(\n            self._reference.output_text, trace.output_text\n        )\n\n        # Cost deviation\n        ref_tokens = self._reference.total_input_tokens + self._reference.total_output_tokens\n        cur_tokens = trace.total_input_tokens + trace.total_output_tokens\n        if ref_tokens &gt; 0:\n            cost_ratio = min(cur_tokens, ref_tokens) / max(cur_tokens, ref_tokens)\n        elif cur_tokens == 0:\n            cost_ratio = 1.0\n        else:\n            cost_ratio = 0.0\n        scores[\"cost_deviation\"] = cost_ratio\n\n        # Weighted composite\n        total_weight = sum(self._weights.get(k, 0.0) for k in scores)\n        composite = sum(scores[k] * self._weights.get(k, 0.0) for k in scores)\n        final_score = composite / total_weight if total_weight &gt; 0 else 0.0\n        final_score = round(min(max(final_score, 0.0), 1.0), 4)\n\n        _partial_threshold = 0.5\n        if final_score &gt;= self._pass_threshold:\n            verdict = EvalVerdict.PASS\n        elif final_score &gt;= _partial_threshold:\n            verdict = EvalVerdict.PARTIAL\n        else:\n            verdict = EvalVerdict.FAIL\n\n        return EvalResult(\n            evaluator_name=self.name,\n            verdict=verdict,\n            score=final_score,\n            reason=f\"Trace comparison: {final_score:.3f} ({_format_scores(scores)})\",\n            metadata={\"dimension_scores\": scores, \"weights\": self._weights},\n        )\n</code></pre>"},{"location":"reference/api/eval/#agentprobe.eval.trace_compare.TraceComparisonEvaluator.__init__","title":"<code>__init__(reference_trace, *, name='trace-compare', weights=None, pass_threshold=0.7)</code>","text":"<p>Initialize the trace comparison evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>reference_trace</code> <code>Trace</code> <p>The baseline trace to compare against.</p> required <code>name</code> <code>str</code> <p>Evaluator name.</p> <code>'trace-compare'</code> <code>weights</code> <code>dict[str, float] | None</code> <p>Dimension weight overrides.</p> <code>None</code> <code>pass_threshold</code> <code>float</code> <p>Minimum score for a pass verdict.</p> <code>0.7</code> Source code in <code>src/agentprobe/eval/trace_compare.py</code> <pre><code>def __init__(\n    self,\n    reference_trace: Trace,\n    *,\n    name: str = \"trace-compare\",\n    weights: dict[str, float] | None = None,\n    pass_threshold: float = 0.7,\n) -&gt; None:\n    \"\"\"Initialize the trace comparison evaluator.\n\n    Args:\n        reference_trace: The baseline trace to compare against.\n        name: Evaluator name.\n        weights: Dimension weight overrides.\n        pass_threshold: Minimum score for a pass verdict.\n    \"\"\"\n    super().__init__(name)\n    self._reference = reference_trace\n    self._weights = weights or dict(self.DEFAULT_WEIGHTS)\n    self._pass_threshold = pass_threshold\n</code></pre>"},{"location":"reference/api/eval/#base-evaluator","title":"Base Evaluator","text":""},{"location":"reference/api/eval/#agentprobe.eval.base","title":"<code>agentprobe.eval.base</code>","text":"<p>Abstract base evaluator with template-method pattern.</p> <p>Subclasses implement <code>_evaluate()</code> while the base class handles timing, error wrapping, and consistent result construction.</p>"},{"location":"reference/api/eval/#agentprobe.eval.base.BaseEvaluator","title":"<code>BaseEvaluator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all evaluators.</p> <p>Provides a public <code>evaluate()</code> template method that delegates to the subclass-defined <code>_evaluate()</code>, adding timing and error handling.</p> <p>Attributes:</p> Name Type Description <code>_name</code> <p>The evaluator's name, used in results and logging.</p> Source code in <code>src/agentprobe/eval/base.py</code> <pre><code>class BaseEvaluator(ABC):\n    \"\"\"Abstract base class for all evaluators.\n\n    Provides a public ``evaluate()`` template method that delegates to\n    the subclass-defined ``_evaluate()``, adding timing and error handling.\n\n    Attributes:\n        _name: The evaluator's name, used in results and logging.\n    \"\"\"\n\n    def __init__(self, name: str) -&gt; None:\n        \"\"\"Initialize the evaluator.\n\n        Args:\n            name: A unique name identifying this evaluator instance.\n        \"\"\"\n        self._name = name\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the evaluator name.\"\"\"\n        return self._name\n\n    async def evaluate(self, test_case: TestCase, trace: Trace) -&gt; EvalResult:\n        \"\"\"Evaluate an agent trace for a given test case.\n\n        This template method times the evaluation, catches errors, and\n        ensures a consistent EvalResult is always returned.\n\n        Args:\n            test_case: The test case that was executed.\n            trace: The execution trace to evaluate.\n\n        Returns:\n            An evaluation result with score and verdict.\n        \"\"\"\n        start = time.monotonic()\n        try:\n            result = await self._evaluate(test_case, trace)\n        except EvaluatorError:\n            raise\n        except Exception as exc:\n            elapsed_ms = int((time.monotonic() - start) * 1000)\n            logger.error(\n                \"Evaluator '%s' failed for test '%s': %s\",\n                self._name,\n                test_case.name,\n                exc,\n            )\n            return EvalResult(\n                evaluator_name=self._name,\n                verdict=EvalVerdict.ERROR,\n                score=0.0,\n                reason=f\"Evaluation error: {exc}\",\n                metadata={\"duration_ms\": elapsed_ms},\n            )\n        else:\n            elapsed_ms = int((time.monotonic() - start) * 1000)\n            logger.debug(\n                \"Evaluator '%s' completed for test '%s' in %dms: %s (%.2f)\",\n                self._name,\n                test_case.name,\n                elapsed_ms,\n                result.verdict.value,\n                result.score,\n            )\n            return result\n\n    @abstractmethod\n    async def _evaluate(self, test_case: TestCase, trace: Trace) -&gt; EvalResult:\n        \"\"\"Perform the actual evaluation logic.\n\n        Subclasses must implement this method.\n\n        Args:\n            test_case: The test case that was executed.\n            trace: The execution trace to evaluate.\n\n        Returns:\n            An evaluation result with score and verdict.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/api/eval/#agentprobe.eval.base.BaseEvaluator.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the evaluator name.</p>"},{"location":"reference/api/eval/#agentprobe.eval.base.BaseEvaluator.__init__","title":"<code>__init__(name)</code>","text":"<p>Initialize the evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A unique name identifying this evaluator instance.</p> required Source code in <code>src/agentprobe/eval/base.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    \"\"\"Initialize the evaluator.\n\n    Args:\n        name: A unique name identifying this evaluator instance.\n    \"\"\"\n    self._name = name\n</code></pre>"},{"location":"reference/api/eval/#agentprobe.eval.base.BaseEvaluator.evaluate","title":"<code>evaluate(test_case, trace)</code>  <code>async</code>","text":"<p>Evaluate an agent trace for a given test case.</p> <p>This template method times the evaluation, catches errors, and ensures a consistent EvalResult is always returned.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case that was executed.</p> required <code>trace</code> <code>Trace</code> <p>The execution trace to evaluate.</p> required <p>Returns:</p> Type Description <code>EvalResult</code> <p>An evaluation result with score and verdict.</p> Source code in <code>src/agentprobe/eval/base.py</code> <pre><code>async def evaluate(self, test_case: TestCase, trace: Trace) -&gt; EvalResult:\n    \"\"\"Evaluate an agent trace for a given test case.\n\n    This template method times the evaluation, catches errors, and\n    ensures a consistent EvalResult is always returned.\n\n    Args:\n        test_case: The test case that was executed.\n        trace: The execution trace to evaluate.\n\n    Returns:\n        An evaluation result with score and verdict.\n    \"\"\"\n    start = time.monotonic()\n    try:\n        result = await self._evaluate(test_case, trace)\n    except EvaluatorError:\n        raise\n    except Exception as exc:\n        elapsed_ms = int((time.monotonic() - start) * 1000)\n        logger.error(\n            \"Evaluator '%s' failed for test '%s': %s\",\n            self._name,\n            test_case.name,\n            exc,\n        )\n        return EvalResult(\n            evaluator_name=self._name,\n            verdict=EvalVerdict.ERROR,\n            score=0.0,\n            reason=f\"Evaluation error: {exc}\",\n            metadata={\"duration_ms\": elapsed_ms},\n        )\n    else:\n        elapsed_ms = int((time.monotonic() - start) * 1000)\n        logger.debug(\n            \"Evaluator '%s' completed for test '%s' in %dms: %s (%.2f)\",\n            self._name,\n            test_case.name,\n            elapsed_ms,\n            result.verdict.value,\n            result.score,\n        )\n        return result\n</code></pre>"},{"location":"reference/api/metrics/","title":"Metrics","text":"<p>Metric collection, aggregation, and trend analysis.</p>"},{"location":"reference/api/metrics/#collector","title":"Collector","text":""},{"location":"reference/api/metrics/#agentprobe.metrics.collector","title":"<code>agentprobe.metrics.collector</code>","text":"<p>Stateless metric collector that extracts measurements from traces and results.</p> <p>Converts traces, test results, and agent runs into MetricValue instances for storage and analysis.</p>"},{"location":"reference/api/metrics/#agentprobe.metrics.collector.MetricCollector","title":"<code>MetricCollector</code>","text":"<p>Extracts metric values from traces, results, and runs.</p> <p>Stateless: receives objects and returns lists of MetricValue. Does not store or persist anything.</p> Source code in <code>src/agentprobe/metrics/collector.py</code> <pre><code>class MetricCollector:\n    \"\"\"Extracts metric values from traces, results, and runs.\n\n    Stateless: receives objects and returns lists of MetricValue.\n    Does not store or persist anything.\n    \"\"\"\n\n    def collect_from_trace(self, trace: Trace) -&gt; list[MetricValue]:\n        \"\"\"Extract metric values from a single trace.\n\n        Collects latency, tool call count, and response length metrics.\n\n        Args:\n            trace: The execution trace to extract metrics from.\n\n        Returns:\n            A list of metric values extracted from the trace.\n        \"\"\"\n        now = datetime.now(UTC)\n        tags = tuple(trace.tags)\n        metrics: list[MetricValue] = []\n\n        metrics.append(\n            MetricValue(\n                metric_name=\"latency_ms\",\n                value=float(trace.total_latency_ms),\n                tags=tags,\n                metadata={\"trace_id\": trace.trace_id, \"agent_name\": trace.agent_name},\n                timestamp=now,\n            )\n        )\n\n        metrics.append(\n            MetricValue(\n                metric_name=\"tool_call_count\",\n                value=float(len(trace.tool_calls)),\n                tags=tags,\n                metadata={\"trace_id\": trace.trace_id, \"agent_name\": trace.agent_name},\n                timestamp=now,\n            )\n        )\n\n        metrics.append(\n            MetricValue(\n                metric_name=\"response_length\",\n                value=float(len(trace.output_text)),\n                tags=tags,\n                metadata={\"trace_id\": trace.trace_id, \"agent_name\": trace.agent_name},\n                timestamp=now,\n            )\n        )\n\n        return metrics\n\n    def collect_from_result(self, result: TestResult) -&gt; list[MetricValue]:\n        \"\"\"Extract metric values from a test result.\n\n        Collects latency, eval score, and any trace-level metrics.\n\n        Args:\n            result: The test result to extract metrics from.\n\n        Returns:\n            A list of metric values extracted from the result.\n        \"\"\"\n        now = datetime.now(UTC)\n        metrics: list[MetricValue] = []\n\n        metrics.append(\n            MetricValue(\n                metric_name=\"latency_ms\",\n                value=float(result.duration_ms),\n                metadata={\"test_name\": result.test_name, \"result_id\": result.result_id},\n                timestamp=now,\n            )\n        )\n\n        metrics.append(\n            MetricValue(\n                metric_name=\"eval_score\",\n                value=result.score,\n                metadata={\"test_name\": result.test_name, \"result_id\": result.result_id},\n                timestamp=now,\n            )\n        )\n\n        if result.trace is not None:\n            trace_metrics = self.collect_from_trace(result.trace)\n            metrics.extend(trace_metrics)\n\n        return metrics\n\n    def collect_from_run(self, run: AgentRun) -&gt; list[MetricValue]:\n        \"\"\"Extract metric values from a complete agent run.\n\n        Collects pass rate plus per-result metrics for all results.\n\n        Args:\n            run: The agent run to extract metrics from.\n\n        Returns:\n            A list of metric values extracted from the run.\n        \"\"\"\n        now = datetime.now(UTC)\n        metrics: list[MetricValue] = []\n\n        if run.total_tests &gt; 0:\n            passed = sum(1 for r in run.test_results if r.status == TestStatus.PASSED)\n            pass_rate = passed / run.total_tests\n        else:\n            pass_rate = 0.0\n\n        metrics.append(\n            MetricValue(\n                metric_name=\"pass_rate\",\n                value=pass_rate,\n                metadata={\"run_id\": run.run_id, \"agent_name\": run.agent_name},\n                timestamp=now,\n            )\n        )\n\n        for result in run.test_results:\n            result_metrics = self.collect_from_result(result)\n            metrics.extend(result_metrics)\n\n        return metrics\n</code></pre>"},{"location":"reference/api/metrics/#agentprobe.metrics.collector.MetricCollector.collect_from_trace","title":"<code>collect_from_trace(trace)</code>","text":"<p>Extract metric values from a single trace.</p> <p>Collects latency, tool call count, and response length metrics.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The execution trace to extract metrics from.</p> required <p>Returns:</p> Type Description <code>list[MetricValue]</code> <p>A list of metric values extracted from the trace.</p> Source code in <code>src/agentprobe/metrics/collector.py</code> <pre><code>def collect_from_trace(self, trace: Trace) -&gt; list[MetricValue]:\n    \"\"\"Extract metric values from a single trace.\n\n    Collects latency, tool call count, and response length metrics.\n\n    Args:\n        trace: The execution trace to extract metrics from.\n\n    Returns:\n        A list of metric values extracted from the trace.\n    \"\"\"\n    now = datetime.now(UTC)\n    tags = tuple(trace.tags)\n    metrics: list[MetricValue] = []\n\n    metrics.append(\n        MetricValue(\n            metric_name=\"latency_ms\",\n            value=float(trace.total_latency_ms),\n            tags=tags,\n            metadata={\"trace_id\": trace.trace_id, \"agent_name\": trace.agent_name},\n            timestamp=now,\n        )\n    )\n\n    metrics.append(\n        MetricValue(\n            metric_name=\"tool_call_count\",\n            value=float(len(trace.tool_calls)),\n            tags=tags,\n            metadata={\"trace_id\": trace.trace_id, \"agent_name\": trace.agent_name},\n            timestamp=now,\n        )\n    )\n\n    metrics.append(\n        MetricValue(\n            metric_name=\"response_length\",\n            value=float(len(trace.output_text)),\n            tags=tags,\n            metadata={\"trace_id\": trace.trace_id, \"agent_name\": trace.agent_name},\n            timestamp=now,\n        )\n    )\n\n    return metrics\n</code></pre>"},{"location":"reference/api/metrics/#agentprobe.metrics.collector.MetricCollector.collect_from_result","title":"<code>collect_from_result(result)</code>","text":"<p>Extract metric values from a test result.</p> <p>Collects latency, eval score, and any trace-level metrics.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>TestResult</code> <p>The test result to extract metrics from.</p> required <p>Returns:</p> Type Description <code>list[MetricValue]</code> <p>A list of metric values extracted from the result.</p> Source code in <code>src/agentprobe/metrics/collector.py</code> <pre><code>def collect_from_result(self, result: TestResult) -&gt; list[MetricValue]:\n    \"\"\"Extract metric values from a test result.\n\n    Collects latency, eval score, and any trace-level metrics.\n\n    Args:\n        result: The test result to extract metrics from.\n\n    Returns:\n        A list of metric values extracted from the result.\n    \"\"\"\n    now = datetime.now(UTC)\n    metrics: list[MetricValue] = []\n\n    metrics.append(\n        MetricValue(\n            metric_name=\"latency_ms\",\n            value=float(result.duration_ms),\n            metadata={\"test_name\": result.test_name, \"result_id\": result.result_id},\n            timestamp=now,\n        )\n    )\n\n    metrics.append(\n        MetricValue(\n            metric_name=\"eval_score\",\n            value=result.score,\n            metadata={\"test_name\": result.test_name, \"result_id\": result.result_id},\n            timestamp=now,\n        )\n    )\n\n    if result.trace is not None:\n        trace_metrics = self.collect_from_trace(result.trace)\n        metrics.extend(trace_metrics)\n\n    return metrics\n</code></pre>"},{"location":"reference/api/metrics/#agentprobe.metrics.collector.MetricCollector.collect_from_run","title":"<code>collect_from_run(run)</code>","text":"<p>Extract metric values from a complete agent run.</p> <p>Collects pass rate plus per-result metrics for all results.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>AgentRun</code> <p>The agent run to extract metrics from.</p> required <p>Returns:</p> Type Description <code>list[MetricValue]</code> <p>A list of metric values extracted from the run.</p> Source code in <code>src/agentprobe/metrics/collector.py</code> <pre><code>def collect_from_run(self, run: AgentRun) -&gt; list[MetricValue]:\n    \"\"\"Extract metric values from a complete agent run.\n\n    Collects pass rate plus per-result metrics for all results.\n\n    Args:\n        run: The agent run to extract metrics from.\n\n    Returns:\n        A list of metric values extracted from the run.\n    \"\"\"\n    now = datetime.now(UTC)\n    metrics: list[MetricValue] = []\n\n    if run.total_tests &gt; 0:\n        passed = sum(1 for r in run.test_results if r.status == TestStatus.PASSED)\n        pass_rate = passed / run.total_tests\n    else:\n        pass_rate = 0.0\n\n    metrics.append(\n        MetricValue(\n            metric_name=\"pass_rate\",\n            value=pass_rate,\n            metadata={\"run_id\": run.run_id, \"agent_name\": run.agent_name},\n            timestamp=now,\n        )\n    )\n\n    for result in run.test_results:\n        result_metrics = self.collect_from_result(result)\n        metrics.extend(result_metrics)\n\n    return metrics\n</code></pre>"},{"location":"reference/api/metrics/#aggregator","title":"Aggregator","text":""},{"location":"reference/api/metrics/#agentprobe.metrics.aggregator","title":"<code>agentprobe.metrics.aggregator</code>","text":"<p>Metric aggregation: computes statistical summaries from metric values.</p> <p>Uses stdlib <code>statistics</code> module for calculations \u2014 no numpy dependency.</p>"},{"location":"reference/api/metrics/#agentprobe.metrics.aggregator.MetricAggregator","title":"<code>MetricAggregator</code>","text":"<p>Computes statistical aggregations over collections of metric values.</p> <p>Supports mean, median, min, max, p95, p99, and standard deviation. All computations use the stdlib <code>statistics</code> module.</p> Source code in <code>src/agentprobe/metrics/aggregator.py</code> <pre><code>class MetricAggregator:\n    \"\"\"Computes statistical aggregations over collections of metric values.\n\n    Supports mean, median, min, max, p95, p99, and standard deviation.\n    All computations use the stdlib ``statistics`` module.\n    \"\"\"\n\n    def aggregate(self, values: list[MetricValue]) -&gt; MetricAggregation:\n        \"\"\"Aggregate a list of metric values into summary statistics.\n\n        All values must share the same metric_name.\n\n        Args:\n            values: List of metric values to aggregate.\n\n        Returns:\n            A MetricAggregation with computed statistics.\n\n        Raises:\n            MetricsError: If values is empty or metric names are inconsistent.\n        \"\"\"\n        if not values:\n            raise MetricsError(\"Cannot aggregate empty metric list\")\n\n        names = {v.metric_name for v in values}\n        if len(names) &gt; 1:\n            raise MetricsError(f\"Cannot aggregate mixed metrics: {', '.join(sorted(names))}\")\n\n        metric_name = values[0].metric_name\n        raw = [v.value for v in values]\n\n        return self._compute_stats(metric_name, raw)\n\n    def aggregate_by_name(self, values: list[MetricValue]) -&gt; dict[str, MetricAggregation]:\n        \"\"\"Group metric values by name and aggregate each group.\n\n        Args:\n            values: List of metric values (may contain multiple metric names).\n\n        Returns:\n            A dictionary mapping metric names to their aggregations.\n\n        Raises:\n            MetricsError: If values is empty.\n        \"\"\"\n        if not values:\n            raise MetricsError(\"Cannot aggregate empty metric list\")\n\n        grouped: dict[str, list[float]] = defaultdict(list)\n        for v in values:\n            grouped[v.metric_name].append(v.value)\n\n        return {name: self._compute_stats(name, raw_values) for name, raw_values in grouped.items()}\n\n    def _compute_stats(self, metric_name: str, raw: list[float]) -&gt; MetricAggregation:\n        \"\"\"Compute statistics for a list of numeric values.\n\n        Args:\n            metric_name: The metric name for the aggregation.\n            raw: Raw numeric values to aggregate.\n\n        Returns:\n            A MetricAggregation with computed statistics.\n        \"\"\"\n        n = len(raw)\n        mean = statistics.mean(raw)\n        median = statistics.median(raw)\n        min_val = min(raw)\n        max_val = max(raw)\n        std_dev = statistics.stdev(raw) if n &gt;= _MIN_STDEV_SAMPLES else 0.0\n\n        sorted_raw = sorted(raw)\n        p95 = self._percentile(sorted_raw, 0.95)\n        p99 = self._percentile(sorted_raw, 0.99)\n\n        return MetricAggregation(\n            metric_name=metric_name,\n            count=n,\n            mean=mean,\n            median=median,\n            min_value=min_val,\n            max_value=max_val,\n            p95=p95,\n            p99=p99,\n            std_dev=std_dev,\n        )\n\n    @staticmethod\n    def _percentile(sorted_data: list[float], pct: float) -&gt; float:\n        \"\"\"Compute a percentile using linear interpolation.\n\n        Args:\n            sorted_data: Pre-sorted list of values.\n            pct: Percentile as a fraction (e.g. 0.95 for 95th).\n\n        Returns:\n            The interpolated percentile value.\n        \"\"\"\n        n = len(sorted_data)\n        if n == 1:\n            return sorted_data[0]\n\n        idx = pct * (n - 1)\n        lower = math.floor(idx)\n        upper = math.ceil(idx)\n\n        if lower == upper:\n            return sorted_data[lower]\n\n        frac = idx - lower\n        return sorted_data[lower] * (1.0 - frac) + sorted_data[upper] * frac\n</code></pre>"},{"location":"reference/api/metrics/#agentprobe.metrics.aggregator.MetricAggregator.aggregate","title":"<code>aggregate(values)</code>","text":"<p>Aggregate a list of metric values into summary statistics.</p> <p>All values must share the same metric_name.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>list[MetricValue]</code> <p>List of metric values to aggregate.</p> required <p>Returns:</p> Type Description <code>MetricAggregation</code> <p>A MetricAggregation with computed statistics.</p> <p>Raises:</p> Type Description <code>MetricsError</code> <p>If values is empty or metric names are inconsistent.</p> Source code in <code>src/agentprobe/metrics/aggregator.py</code> <pre><code>def aggregate(self, values: list[MetricValue]) -&gt; MetricAggregation:\n    \"\"\"Aggregate a list of metric values into summary statistics.\n\n    All values must share the same metric_name.\n\n    Args:\n        values: List of metric values to aggregate.\n\n    Returns:\n        A MetricAggregation with computed statistics.\n\n    Raises:\n        MetricsError: If values is empty or metric names are inconsistent.\n    \"\"\"\n    if not values:\n        raise MetricsError(\"Cannot aggregate empty metric list\")\n\n    names = {v.metric_name for v in values}\n    if len(names) &gt; 1:\n        raise MetricsError(f\"Cannot aggregate mixed metrics: {', '.join(sorted(names))}\")\n\n    metric_name = values[0].metric_name\n    raw = [v.value for v in values]\n\n    return self._compute_stats(metric_name, raw)\n</code></pre>"},{"location":"reference/api/metrics/#agentprobe.metrics.aggregator.MetricAggregator.aggregate_by_name","title":"<code>aggregate_by_name(values)</code>","text":"<p>Group metric values by name and aggregate each group.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>list[MetricValue]</code> <p>List of metric values (may contain multiple metric names).</p> required <p>Returns:</p> Type Description <code>dict[str, MetricAggregation]</code> <p>A dictionary mapping metric names to their aggregations.</p> <p>Raises:</p> Type Description <code>MetricsError</code> <p>If values is empty.</p> Source code in <code>src/agentprobe/metrics/aggregator.py</code> <pre><code>def aggregate_by_name(self, values: list[MetricValue]) -&gt; dict[str, MetricAggregation]:\n    \"\"\"Group metric values by name and aggregate each group.\n\n    Args:\n        values: List of metric values (may contain multiple metric names).\n\n    Returns:\n        A dictionary mapping metric names to their aggregations.\n\n    Raises:\n        MetricsError: If values is empty.\n    \"\"\"\n    if not values:\n        raise MetricsError(\"Cannot aggregate empty metric list\")\n\n    grouped: dict[str, list[float]] = defaultdict(list)\n    for v in values:\n        grouped[v.metric_name].append(v.value)\n\n    return {name: self._compute_stats(name, raw_values) for name, raw_values in grouped.items()}\n</code></pre>"},{"location":"reference/api/metrics/#trend-analysis","title":"Trend Analysis","text":""},{"location":"reference/api/metrics/#agentprobe.metrics.trend","title":"<code>agentprobe.metrics.trend</code>","text":"<p>Metric trend analysis: detects improving, degrading, or stable trends.</p> <p>Compares recent metric values against a historical window to determine whether performance is changing over time.</p>"},{"location":"reference/api/metrics/#agentprobe.metrics.trend.MetricTrend","title":"<code>MetricTrend</code>","text":"<p>Analyzes metric trends by comparing recent vs historical values.</p> <p>Uses a split-window approach: divides a time-ordered series of values into a historical window and a recent window, then compares means.</p> <p>Attributes:</p> Name Type Description <code>threshold</code> <p>Minimum relative change to flag as improving/degrading.</p> Source code in <code>src/agentprobe/metrics/trend.py</code> <pre><code>class MetricTrend:\n    \"\"\"Analyzes metric trends by comparing recent vs historical values.\n\n    Uses a split-window approach: divides a time-ordered series of values\n    into a historical window and a recent window, then compares means.\n\n    Attributes:\n        threshold: Minimum relative change to flag as improving/degrading.\n    \"\"\"\n\n    def __init__(self, threshold: float = 0.1) -&gt; None:\n        \"\"\"Initialize the trend analyzer.\n\n        Args:\n            threshold: Minimum relative change (fraction) to consider\n                a trend as improving or degrading. Defaults to 0.1 (10%).\n        \"\"\"\n        self._threshold = threshold\n\n    def analyze(\n        self,\n        values: list[MetricValue],\n        lower_is_better: bool = True,\n    ) -&gt; TrendDirection:\n        \"\"\"Analyze the trend direction for a series of metric values.\n\n        Splits the values in half (by order) and compares means.\n\n        Args:\n            values: Time-ordered list of metric values (oldest first).\n            lower_is_better: Whether lower values indicate improvement.\n\n        Returns:\n            The detected trend direction.\n\n        Raises:\n            MetricsError: If fewer than 2 values are provided.\n        \"\"\"\n        if len(values) &lt; _MIN_TREND_SAMPLES:\n            return TrendDirection.INSUFFICIENT_DATA\n\n        raw = [v.value for v in values]\n        return self._analyze_raw(raw, lower_is_better)\n\n    def analyze_series(\n        self,\n        raw_values: list[float],\n        lower_is_better: bool = True,\n    ) -&gt; TrendDirection:\n        \"\"\"Analyze the trend from a raw numeric series.\n\n        Args:\n            raw_values: Time-ordered list of numeric values (oldest first).\n            lower_is_better: Whether lower values indicate improvement.\n\n        Returns:\n            The detected trend direction.\n        \"\"\"\n        if len(raw_values) &lt; _MIN_TREND_SAMPLES:\n            return TrendDirection.INSUFFICIENT_DATA\n\n        return self._analyze_raw(raw_values, lower_is_better)\n\n    def _analyze_raw(self, raw: list[float], lower_is_better: bool) -&gt; TrendDirection:\n        \"\"\"Core trend analysis on raw numeric values.\n\n        Args:\n            raw: Ordered list of values.\n            lower_is_better: Direction semantics.\n\n        Returns:\n            The trend direction.\n        \"\"\"\n        midpoint = len(raw) // 2\n        historical = raw[:midpoint]\n        recent = raw[midpoint:]\n\n        hist_mean = statistics.mean(historical)\n        recent_mean = statistics.mean(recent)\n\n        if hist_mean == 0.0:\n            if recent_mean == 0.0:\n                return TrendDirection.STABLE\n            return TrendDirection.DEGRADING if lower_is_better else TrendDirection.IMPROVING\n\n        relative_change = (recent_mean - hist_mean) / abs(hist_mean)\n\n        if abs(relative_change) &lt; self._threshold:\n            return TrendDirection.STABLE\n\n        value_decreased = relative_change &lt; 0\n\n        if lower_is_better:\n            return TrendDirection.IMPROVING if value_decreased else TrendDirection.DEGRADING\n        else:\n            return TrendDirection.DEGRADING if value_decreased else TrendDirection.IMPROVING\n</code></pre>"},{"location":"reference/api/metrics/#agentprobe.metrics.trend.MetricTrend.__init__","title":"<code>__init__(threshold=0.1)</code>","text":"<p>Initialize the trend analyzer.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Minimum relative change (fraction) to consider a trend as improving or degrading. Defaults to 0.1 (10%).</p> <code>0.1</code> Source code in <code>src/agentprobe/metrics/trend.py</code> <pre><code>def __init__(self, threshold: float = 0.1) -&gt; None:\n    \"\"\"Initialize the trend analyzer.\n\n    Args:\n        threshold: Minimum relative change (fraction) to consider\n            a trend as improving or degrading. Defaults to 0.1 (10%).\n    \"\"\"\n    self._threshold = threshold\n</code></pre>"},{"location":"reference/api/metrics/#agentprobe.metrics.trend.MetricTrend.analyze","title":"<code>analyze(values, lower_is_better=True)</code>","text":"<p>Analyze the trend direction for a series of metric values.</p> <p>Splits the values in half (by order) and compares means.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>list[MetricValue]</code> <p>Time-ordered list of metric values (oldest first).</p> required <code>lower_is_better</code> <code>bool</code> <p>Whether lower values indicate improvement.</p> <code>True</code> <p>Returns:</p> Type Description <code>TrendDirection</code> <p>The detected trend direction.</p> <p>Raises:</p> Type Description <code>MetricsError</code> <p>If fewer than 2 values are provided.</p> Source code in <code>src/agentprobe/metrics/trend.py</code> <pre><code>def analyze(\n    self,\n    values: list[MetricValue],\n    lower_is_better: bool = True,\n) -&gt; TrendDirection:\n    \"\"\"Analyze the trend direction for a series of metric values.\n\n    Splits the values in half (by order) and compares means.\n\n    Args:\n        values: Time-ordered list of metric values (oldest first).\n        lower_is_better: Whether lower values indicate improvement.\n\n    Returns:\n        The detected trend direction.\n\n    Raises:\n        MetricsError: If fewer than 2 values are provided.\n    \"\"\"\n    if len(values) &lt; _MIN_TREND_SAMPLES:\n        return TrendDirection.INSUFFICIENT_DATA\n\n    raw = [v.value for v in values]\n    return self._analyze_raw(raw, lower_is_better)\n</code></pre>"},{"location":"reference/api/metrics/#agentprobe.metrics.trend.MetricTrend.analyze_series","title":"<code>analyze_series(raw_values, lower_is_better=True)</code>","text":"<p>Analyze the trend from a raw numeric series.</p> <p>Parameters:</p> Name Type Description Default <code>raw_values</code> <code>list[float]</code> <p>Time-ordered list of numeric values (oldest first).</p> required <code>lower_is_better</code> <code>bool</code> <p>Whether lower values indicate improvement.</p> <code>True</code> <p>Returns:</p> Type Description <code>TrendDirection</code> <p>The detected trend direction.</p> Source code in <code>src/agentprobe/metrics/trend.py</code> <pre><code>def analyze_series(\n    self,\n    raw_values: list[float],\n    lower_is_better: bool = True,\n) -&gt; TrendDirection:\n    \"\"\"Analyze the trend from a raw numeric series.\n\n    Args:\n        raw_values: Time-ordered list of numeric values (oldest first).\n        lower_is_better: Whether lower values indicate improvement.\n\n    Returns:\n        The detected trend direction.\n    \"\"\"\n    if len(raw_values) &lt; _MIN_TREND_SAMPLES:\n        return TrendDirection.INSUFFICIENT_DATA\n\n    return self._analyze_raw(raw_values, lower_is_better)\n</code></pre>"},{"location":"reference/api/metrics/#built-in-definitions","title":"Built-in Definitions","text":""},{"location":"reference/api/metrics/#agentprobe.metrics.definitions","title":"<code>agentprobe.metrics.definitions</code>","text":"<p>Built-in metric definitions for common agent performance measurements.</p> <p>Provides a registry of standard metrics that can be collected automatically during test execution, covering latency, cost, token usage, and scores.</p>"},{"location":"reference/api/metrics/#agentprobe.metrics.definitions.get_builtin_definitions","title":"<code>get_builtin_definitions()</code>","text":"<p>Return all built-in metric definitions.</p> <p>Returns:</p> Type Description <code>dict[str, MetricDefinition]</code> <p>A dictionary mapping metric names to their definitions.</p> Source code in <code>src/agentprobe/metrics/definitions.py</code> <pre><code>def get_builtin_definitions() -&gt; dict[str, MetricDefinition]:\n    \"\"\"Return all built-in metric definitions.\n\n    Returns:\n        A dictionary mapping metric names to their definitions.\n    \"\"\"\n    return dict(BUILTIN_METRICS)\n</code></pre>"},{"location":"reference/api/metrics/#agentprobe.metrics.definitions.get_definition","title":"<code>get_definition(name)</code>","text":"<p>Look up a built-in metric definition by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The metric name to look up.</p> required <p>Returns:</p> Type Description <code>MetricDefinition | None</code> <p>The metric definition if found, otherwise None.</p> Source code in <code>src/agentprobe/metrics/definitions.py</code> <pre><code>def get_definition(name: str) -&gt; MetricDefinition | None:\n    \"\"\"Look up a built-in metric definition by name.\n\n    Args:\n        name: The metric name to look up.\n\n    Returns:\n        The metric definition if found, otherwise None.\n    \"\"\"\n    return BUILTIN_METRICS.get(name)\n</code></pre>"},{"location":"reference/api/plugins/","title":"Plugins","text":"<p>Plugin system for extending AgentProbe with custom evaluators, adapters, reporters, and storage backends.</p>"},{"location":"reference/api/plugins/#base-classes","title":"Base Classes","text":""},{"location":"reference/api/plugins/#agentprobe.plugins.base","title":"<code>agentprobe.plugins.base</code>","text":"<p>Base plugin classes with lifecycle hooks and typed subclasses.</p> <p>Plugins extend AgentProbe's functionality by implementing lifecycle hooks and providing factory methods for evaluators, adapters, reporters, or storage backends.</p>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.PluginBase","title":"<code>PluginBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for all AgentProbe plugins.</p> <p>Plugins implement lifecycle hooks that are called at various points during test execution. Subclasses should override only the hooks they need \u2014 default implementations are no-ops.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique plugin identifier.</p> <code>plugin_type</code> <code>PluginType</code> <p>The type of extension this plugin provides.</p> <code>version</code> <code>str</code> <p>Plugin version string.</p> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>class PluginBase(ABC):\n    \"\"\"Abstract base for all AgentProbe plugins.\n\n    Plugins implement lifecycle hooks that are called at various points\n    during test execution. Subclasses should override only the hooks\n    they need \u2014 default implementations are no-ops.\n\n    Attributes:\n        name: Unique plugin identifier.\n        plugin_type: The type of extension this plugin provides.\n        version: Plugin version string.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Return the unique plugin name.\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def plugin_type(self) -&gt; PluginType:\n        \"\"\"Return the plugin type.\"\"\"\n        ...\n\n    @property\n    def version(self) -&gt; str:\n        \"\"\"Return the plugin version.\"\"\"\n        return \"0.1.0\"\n\n    def on_load(self) -&gt; None:  # noqa: B027\n        \"\"\"Called when the plugin is loaded into the registry.\"\"\"\n\n    def on_unload(self) -&gt; None:  # noqa: B027\n        \"\"\"Called when the plugin is unloaded from the registry.\"\"\"\n\n    def on_test_start(self, test_name: str, **kwargs: Any) -&gt; None:  # noqa: B027\n        \"\"\"Called before a test case begins execution.\n\n        Args:\n            test_name: Name of the test about to run.\n            **kwargs: Additional context.\n        \"\"\"\n\n    def on_test_end(self, test_name: str, **kwargs: Any) -&gt; None:  # noqa: B027\n        \"\"\"Called after a test case completes execution.\n\n        Args:\n            test_name: Name of the test that completed.\n            **kwargs: Additional context.\n        \"\"\"\n\n    def on_suite_start(self, **kwargs: Any) -&gt; None:  # noqa: B027\n        \"\"\"Called before a test suite begins execution.\n\n        Args:\n            **kwargs: Additional context.\n        \"\"\"\n\n    def on_suite_end(self, **kwargs: Any) -&gt; None:  # noqa: B027\n        \"\"\"Called after a test suite completes execution.\n\n        Args:\n            **kwargs: Additional context.\n        \"\"\"\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.PluginBase.name","title":"<code>name</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the unique plugin name.</p>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.PluginBase.plugin_type","title":"<code>plugin_type</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the plugin type.</p>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.PluginBase.version","title":"<code>version</code>  <code>property</code>","text":"<p>Return the plugin version.</p>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.PluginBase.on_load","title":"<code>on_load()</code>","text":"<p>Called when the plugin is loaded into the registry.</p> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>def on_load(self) -&gt; None:  # noqa: B027\n    \"\"\"Called when the plugin is loaded into the registry.\"\"\"\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.PluginBase.on_unload","title":"<code>on_unload()</code>","text":"<p>Called when the plugin is unloaded from the registry.</p> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>def on_unload(self) -&gt; None:  # noqa: B027\n    \"\"\"Called when the plugin is unloaded from the registry.\"\"\"\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.PluginBase.on_test_start","title":"<code>on_test_start(test_name, **kwargs)</code>","text":"<p>Called before a test case begins execution.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>str</code> <p>Name of the test about to run.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional context.</p> <code>{}</code> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>def on_test_start(self, test_name: str, **kwargs: Any) -&gt; None:  # noqa: B027\n    \"\"\"Called before a test case begins execution.\n\n    Args:\n        test_name: Name of the test about to run.\n        **kwargs: Additional context.\n    \"\"\"\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.PluginBase.on_test_end","title":"<code>on_test_end(test_name, **kwargs)</code>","text":"<p>Called after a test case completes execution.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>str</code> <p>Name of the test that completed.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional context.</p> <code>{}</code> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>def on_test_end(self, test_name: str, **kwargs: Any) -&gt; None:  # noqa: B027\n    \"\"\"Called after a test case completes execution.\n\n    Args:\n        test_name: Name of the test that completed.\n        **kwargs: Additional context.\n    \"\"\"\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.PluginBase.on_suite_start","title":"<code>on_suite_start(**kwargs)</code>","text":"<p>Called before a test suite begins execution.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional context.</p> <code>{}</code> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>def on_suite_start(self, **kwargs: Any) -&gt; None:  # noqa: B027\n    \"\"\"Called before a test suite begins execution.\n\n    Args:\n        **kwargs: Additional context.\n    \"\"\"\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.PluginBase.on_suite_end","title":"<code>on_suite_end(**kwargs)</code>","text":"<p>Called after a test suite completes execution.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional context.</p> <code>{}</code> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>def on_suite_end(self, **kwargs: Any) -&gt; None:  # noqa: B027\n    \"\"\"Called after a test suite completes execution.\n\n    Args:\n        **kwargs: Additional context.\n    \"\"\"\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.EvaluatorPlugin","title":"<code>EvaluatorPlugin</code>","text":"<p>               Bases: <code>PluginBase</code></p> <p>Plugin that provides a custom evaluator.</p> <p>Subclasses must implement <code>create_evaluator()</code> to return an object satisfying :class:<code>~agentprobe.core.protocols.EvaluatorProtocol</code>.</p> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>class EvaluatorPlugin(PluginBase):\n    \"\"\"Plugin that provides a custom evaluator.\n\n    Subclasses must implement ``create_evaluator()`` to return an object\n    satisfying :class:`~agentprobe.core.protocols.EvaluatorProtocol`.\n    \"\"\"\n\n    @property\n    def plugin_type(self) -&gt; PluginType:\n        \"\"\"Return EVALUATOR type.\"\"\"\n        return PluginType.EVALUATOR\n\n    @abstractmethod\n    def create_evaluator(self) -&gt; EvaluatorProtocol:\n        \"\"\"Create and return an evaluator instance.\n\n        Returns:\n            An evaluator conforming to EvaluatorProtocol.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.EvaluatorPlugin.plugin_type","title":"<code>plugin_type</code>  <code>property</code>","text":"<p>Return EVALUATOR type.</p>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.EvaluatorPlugin.create_evaluator","title":"<code>create_evaluator()</code>  <code>abstractmethod</code>","text":"<p>Create and return an evaluator instance.</p> <p>Returns:</p> Type Description <code>EvaluatorProtocol</code> <p>An evaluator conforming to EvaluatorProtocol.</p> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>@abstractmethod\ndef create_evaluator(self) -&gt; EvaluatorProtocol:\n    \"\"\"Create and return an evaluator instance.\n\n    Returns:\n        An evaluator conforming to EvaluatorProtocol.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.AdapterPlugin","title":"<code>AdapterPlugin</code>","text":"<p>               Bases: <code>PluginBase</code></p> <p>Plugin that provides a custom adapter.</p> <p>Subclasses must implement <code>create_adapter()</code> to return an object satisfying :class:<code>~agentprobe.core.protocols.AdapterProtocol</code>.</p> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>class AdapterPlugin(PluginBase):\n    \"\"\"Plugin that provides a custom adapter.\n\n    Subclasses must implement ``create_adapter()`` to return an object\n    satisfying :class:`~agentprobe.core.protocols.AdapterProtocol`.\n    \"\"\"\n\n    @property\n    def plugin_type(self) -&gt; PluginType:\n        \"\"\"Return ADAPTER type.\"\"\"\n        return PluginType.ADAPTER\n\n    @abstractmethod\n    def create_adapter(self) -&gt; AdapterProtocol:\n        \"\"\"Create and return an adapter instance.\n\n        Returns:\n            An adapter conforming to AdapterProtocol.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.AdapterPlugin.plugin_type","title":"<code>plugin_type</code>  <code>property</code>","text":"<p>Return ADAPTER type.</p>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.AdapterPlugin.create_adapter","title":"<code>create_adapter()</code>  <code>abstractmethod</code>","text":"<p>Create and return an adapter instance.</p> <p>Returns:</p> Type Description <code>AdapterProtocol</code> <p>An adapter conforming to AdapterProtocol.</p> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>@abstractmethod\ndef create_adapter(self) -&gt; AdapterProtocol:\n    \"\"\"Create and return an adapter instance.\n\n    Returns:\n        An adapter conforming to AdapterProtocol.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.ReporterPlugin","title":"<code>ReporterPlugin</code>","text":"<p>               Bases: <code>PluginBase</code></p> <p>Plugin that provides a custom reporter.</p> <p>Subclasses must implement <code>create_reporter()</code> to return an object satisfying :class:<code>~agentprobe.core.protocols.ReporterProtocol</code>.</p> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>class ReporterPlugin(PluginBase):\n    \"\"\"Plugin that provides a custom reporter.\n\n    Subclasses must implement ``create_reporter()`` to return an object\n    satisfying :class:`~agentprobe.core.protocols.ReporterProtocol`.\n    \"\"\"\n\n    @property\n    def plugin_type(self) -&gt; PluginType:\n        \"\"\"Return REPORTER type.\"\"\"\n        return PluginType.REPORTER\n\n    @abstractmethod\n    def create_reporter(self) -&gt; ReporterProtocol:\n        \"\"\"Create and return a reporter instance.\n\n        Returns:\n            A reporter conforming to ReporterProtocol.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.ReporterPlugin.plugin_type","title":"<code>plugin_type</code>  <code>property</code>","text":"<p>Return REPORTER type.</p>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.ReporterPlugin.create_reporter","title":"<code>create_reporter()</code>  <code>abstractmethod</code>","text":"<p>Create and return a reporter instance.</p> <p>Returns:</p> Type Description <code>ReporterProtocol</code> <p>A reporter conforming to ReporterProtocol.</p> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>@abstractmethod\ndef create_reporter(self) -&gt; ReporterProtocol:\n    \"\"\"Create and return a reporter instance.\n\n    Returns:\n        A reporter conforming to ReporterProtocol.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.StoragePlugin","title":"<code>StoragePlugin</code>","text":"<p>               Bases: <code>PluginBase</code></p> <p>Plugin that provides a custom storage backend.</p> <p>Subclasses must implement <code>create_storage()</code> to return an object satisfying :class:<code>~agentprobe.core.protocols.StorageProtocol</code>.</p> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>class StoragePlugin(PluginBase):\n    \"\"\"Plugin that provides a custom storage backend.\n\n    Subclasses must implement ``create_storage()`` to return an object\n    satisfying :class:`~agentprobe.core.protocols.StorageProtocol`.\n    \"\"\"\n\n    @property\n    def plugin_type(self) -&gt; PluginType:\n        \"\"\"Return STORAGE type.\"\"\"\n        return PluginType.STORAGE\n\n    @abstractmethod\n    def create_storage(self) -&gt; StorageProtocol:\n        \"\"\"Create and return a storage instance.\n\n        Returns:\n            A storage backend conforming to StorageProtocol.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.StoragePlugin.plugin_type","title":"<code>plugin_type</code>  <code>property</code>","text":"<p>Return STORAGE type.</p>"},{"location":"reference/api/plugins/#agentprobe.plugins.base.StoragePlugin.create_storage","title":"<code>create_storage()</code>  <code>abstractmethod</code>","text":"<p>Create and return a storage instance.</p> <p>Returns:</p> Type Description <code>StorageProtocol</code> <p>A storage backend conforming to StorageProtocol.</p> Source code in <code>src/agentprobe/plugins/base.py</code> <pre><code>@abstractmethod\ndef create_storage(self) -&gt; StorageProtocol:\n    \"\"\"Create and return a storage instance.\n\n    Returns:\n        A storage backend conforming to StorageProtocol.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/plugins/#registry","title":"Registry","text":""},{"location":"reference/api/plugins/#agentprobe.plugins.registry","title":"<code>agentprobe.plugins.registry</code>","text":"<p>Plugin registry: stores and retrieves plugins by name and type.</p> <p>Provides a simple dict-based registry with type filtering support.</p>"},{"location":"reference/api/plugins/#agentprobe.plugins.registry.PluginRegistry","title":"<code>PluginRegistry</code>","text":"<p>Registry for managing loaded plugins.</p> <p>Stores plugins by name and provides lookup by name or type. Prevents duplicate registrations.</p> Source code in <code>src/agentprobe/plugins/registry.py</code> <pre><code>class PluginRegistry:\n    \"\"\"Registry for managing loaded plugins.\n\n    Stores plugins by name and provides lookup by name or type.\n    Prevents duplicate registrations.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._plugins: dict[str, PluginBase] = {}\n\n    def register(self, plugin: PluginBase) -&gt; None:\n        \"\"\"Register a plugin.\n\n        Args:\n            plugin: The plugin to register.\n\n        Raises:\n            PluginError: If a plugin with the same name is already registered.\n        \"\"\"\n        if plugin.name in self._plugins:\n            raise PluginError(f\"Plugin '{plugin.name}' is already registered\")\n        self._plugins[plugin.name] = plugin\n        logger.info(\"Registered plugin '%s' (type=%s)\", plugin.name, plugin.plugin_type)\n\n    def unregister(self, name: str) -&gt; None:\n        \"\"\"Unregister a plugin by name.\n\n        Args:\n            name: The plugin name to remove.\n\n        Raises:\n            PluginError: If no plugin with the given name is registered.\n        \"\"\"\n        if name not in self._plugins:\n            raise PluginError(f\"Plugin '{name}' is not registered\")\n        del self._plugins[name]\n        logger.info(\"Unregistered plugin '%s'\", name)\n\n    def get(self, name: str) -&gt; PluginBase | None:\n        \"\"\"Get a plugin by name.\n\n        Args:\n            name: The plugin name to look up.\n\n        Returns:\n            The plugin if found, otherwise None.\n        \"\"\"\n        return self._plugins.get(name)\n\n    def list_plugins(self) -&gt; list[PluginBase]:\n        \"\"\"Return all registered plugins.\n\n        Returns:\n            A list of all plugins in registration order.\n        \"\"\"\n        return list(self._plugins.values())\n\n    def list_by_type(self, plugin_type: PluginType) -&gt; list[PluginBase]:\n        \"\"\"Return all plugins of a given type.\n\n        Args:\n            plugin_type: The type to filter by.\n\n        Returns:\n            A list of matching plugins.\n        \"\"\"\n        return [p for p in self._plugins.values() if p.plugin_type == plugin_type]\n\n    def clear(self) -&gt; None:\n        \"\"\"Remove all registered plugins.\"\"\"\n        self._plugins.clear()\n        logger.info(\"Plugin registry cleared\")\n\n    def __len__(self) -&gt; int:\n        return len(self._plugins)\n\n    def __contains__(self, name: str) -&gt; bool:\n        return name in self._plugins\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.registry.PluginRegistry.register","title":"<code>register(plugin)</code>","text":"<p>Register a plugin.</p> <p>Parameters:</p> Name Type Description Default <code>plugin</code> <code>PluginBase</code> <p>The plugin to register.</p> required <p>Raises:</p> Type Description <code>PluginError</code> <p>If a plugin with the same name is already registered.</p> Source code in <code>src/agentprobe/plugins/registry.py</code> <pre><code>def register(self, plugin: PluginBase) -&gt; None:\n    \"\"\"Register a plugin.\n\n    Args:\n        plugin: The plugin to register.\n\n    Raises:\n        PluginError: If a plugin with the same name is already registered.\n    \"\"\"\n    if plugin.name in self._plugins:\n        raise PluginError(f\"Plugin '{plugin.name}' is already registered\")\n    self._plugins[plugin.name] = plugin\n    logger.info(\"Registered plugin '%s' (type=%s)\", plugin.name, plugin.plugin_type)\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.registry.PluginRegistry.unregister","title":"<code>unregister(name)</code>","text":"<p>Unregister a plugin by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The plugin name to remove.</p> required <p>Raises:</p> Type Description <code>PluginError</code> <p>If no plugin with the given name is registered.</p> Source code in <code>src/agentprobe/plugins/registry.py</code> <pre><code>def unregister(self, name: str) -&gt; None:\n    \"\"\"Unregister a plugin by name.\n\n    Args:\n        name: The plugin name to remove.\n\n    Raises:\n        PluginError: If no plugin with the given name is registered.\n    \"\"\"\n    if name not in self._plugins:\n        raise PluginError(f\"Plugin '{name}' is not registered\")\n    del self._plugins[name]\n    logger.info(\"Unregistered plugin '%s'\", name)\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.registry.PluginRegistry.get","title":"<code>get(name)</code>","text":"<p>Get a plugin by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The plugin name to look up.</p> required <p>Returns:</p> Type Description <code>PluginBase | None</code> <p>The plugin if found, otherwise None.</p> Source code in <code>src/agentprobe/plugins/registry.py</code> <pre><code>def get(self, name: str) -&gt; PluginBase | None:\n    \"\"\"Get a plugin by name.\n\n    Args:\n        name: The plugin name to look up.\n\n    Returns:\n        The plugin if found, otherwise None.\n    \"\"\"\n    return self._plugins.get(name)\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.registry.PluginRegistry.list_plugins","title":"<code>list_plugins()</code>","text":"<p>Return all registered plugins.</p> <p>Returns:</p> Type Description <code>list[PluginBase]</code> <p>A list of all plugins in registration order.</p> Source code in <code>src/agentprobe/plugins/registry.py</code> <pre><code>def list_plugins(self) -&gt; list[PluginBase]:\n    \"\"\"Return all registered plugins.\n\n    Returns:\n        A list of all plugins in registration order.\n    \"\"\"\n    return list(self._plugins.values())\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.registry.PluginRegistry.list_by_type","title":"<code>list_by_type(plugin_type)</code>","text":"<p>Return all plugins of a given type.</p> <p>Parameters:</p> Name Type Description Default <code>plugin_type</code> <code>PluginType</code> <p>The type to filter by.</p> required <p>Returns:</p> Type Description <code>list[PluginBase]</code> <p>A list of matching plugins.</p> Source code in <code>src/agentprobe/plugins/registry.py</code> <pre><code>def list_by_type(self, plugin_type: PluginType) -&gt; list[PluginBase]:\n    \"\"\"Return all plugins of a given type.\n\n    Args:\n        plugin_type: The type to filter by.\n\n    Returns:\n        A list of matching plugins.\n    \"\"\"\n    return [p for p in self._plugins.values() if p.plugin_type == plugin_type]\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.registry.PluginRegistry.clear","title":"<code>clear()</code>","text":"<p>Remove all registered plugins.</p> Source code in <code>src/agentprobe/plugins/registry.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Remove all registered plugins.\"\"\"\n    self._plugins.clear()\n    logger.info(\"Plugin registry cleared\")\n</code></pre>"},{"location":"reference/api/plugins/#loader","title":"Loader","text":""},{"location":"reference/api/plugins/#agentprobe.plugins.loader","title":"<code>agentprobe.plugins.loader</code>","text":"<p>Plugin loader: discovers and loads plugins from entry points and paths.</p> <p>Supports two discovery mechanisms: 1. Entry points (<code>importlib.metadata</code>) for installed packages. 2. File paths (<code>importlib.util</code>) for local plugin files.</p>"},{"location":"reference/api/plugins/#agentprobe.plugins.loader.PluginLoader","title":"<code>PluginLoader</code>","text":"<p>Loads plugin classes from various sources.</p> <p>Discovers and instantiates plugins from Python entry points, file paths, or direct class references.</p> Source code in <code>src/agentprobe/plugins/loader.py</code> <pre><code>class PluginLoader:\n    \"\"\"Loads plugin classes from various sources.\n\n    Discovers and instantiates plugins from Python entry points,\n    file paths, or direct class references.\n    \"\"\"\n\n    def __init__(self, entry_point_group: str = \"agentprobe.plugins\") -&gt; None:\n        \"\"\"Initialize the plugin loader.\n\n        Args:\n            entry_point_group: Entry point group name for discovery.\n        \"\"\"\n        self._entry_point_group = entry_point_group\n\n    def load_from_entry_points(self) -&gt; list[PluginBase]:\n        \"\"\"Discover and load plugins from installed package entry points.\n\n        Returns:\n            A list of loaded plugin instances.\n        \"\"\"\n        plugins: list[PluginBase] = []\n        try:\n            eps = importlib.metadata.entry_points()\n        except Exception:\n            logger.exception(\"Failed to read entry points\")\n            return plugins\n\n        group_eps: list[Any] = []\n        if hasattr(eps, \"select\"):\n            group_eps = list(eps.select(group=self._entry_point_group))\n        elif isinstance(eps, dict):\n            group_eps = eps.get(self._entry_point_group, [])  # pragma: no cover\n\n        for ep in group_eps:\n            try:\n                plugin_cls = ep.load()\n                plugin = self._instantiate(plugin_cls, source=f\"entry_point:{ep.name}\")\n                plugins.append(plugin)\n            except Exception:\n                logger.exception(\"Failed to load plugin from entry point '%s'\", ep.name)\n\n        return plugins\n\n    def load_from_path(self, path: str | Path) -&gt; PluginBase:\n        \"\"\"Load a plugin from a Python file path.\n\n        The file must contain exactly one class that subclasses PluginBase.\n\n        Args:\n            path: Path to the Python file containing the plugin.\n\n        Returns:\n            The loaded plugin instance.\n\n        Raises:\n            PluginError: If the file cannot be loaded or contains no plugin class.\n        \"\"\"\n        file_path = Path(path)\n        if not file_path.exists():\n            raise PluginError(f\"Plugin file not found: {file_path}\")\n        if not file_path.suffix == \".py\":\n            raise PluginError(f\"Plugin file must be a .py file: {file_path}\")\n\n        module_name = f\"agentprobe_plugin_{file_path.stem}\"\n\n        try:\n            spec = importlib.util.spec_from_file_location(module_name, file_path)\n            if spec is None or spec.loader is None:\n                raise PluginError(f\"Cannot create module spec for: {file_path}\")\n            module = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(module)\n        except PluginError:\n            raise\n        except Exception as exc:\n            raise PluginError(f\"Failed to load module from {file_path}: {exc}\") from exc\n\n        plugin_classes = [\n            obj\n            for name in dir(module)\n            if not name.startswith(\"_\")\n            and isinstance(obj := getattr(module, name), type)\n            and issubclass(obj, PluginBase)\n            and obj is not PluginBase\n        ]\n\n        if not plugin_classes:\n            raise PluginError(f\"No PluginBase subclass found in {file_path}\")\n\n        return self._instantiate(plugin_classes[0], source=str(file_path))\n\n    def load_from_class(self, cls: type[PluginBase]) -&gt; PluginBase:\n        \"\"\"Load a plugin from a direct class reference.\n\n        Args:\n            cls: The plugin class to instantiate.\n\n        Returns:\n            The loaded plugin instance.\n\n        Raises:\n            PluginError: If the class is not a PluginBase subclass.\n        \"\"\"\n        if not (isinstance(cls, type) and issubclass(cls, PluginBase)):\n            raise PluginError(f\"{cls} is not a PluginBase subclass\")\n        return self._instantiate(cls, source=f\"class:{cls.__name__}\")\n\n    def load_all(\n        self,\n        directories: list[str] | None = None,\n    ) -&gt; list[PluginBase]:\n        \"\"\"Load plugins from entry points and optional directories.\n\n        Args:\n            directories: Additional directories to scan for .py plugin files.\n\n        Returns:\n            A list of all loaded plugin instances.\n        \"\"\"\n        plugins = self.load_from_entry_points()\n\n        for dir_path in directories or []:\n            path = Path(dir_path)\n            if not path.is_dir():\n                logger.warning(\"Plugin directory not found: %s\", dir_path)\n                continue\n            for py_file in sorted(path.glob(\"*.py\")):\n                if py_file.name.startswith(\"_\"):\n                    continue\n                try:\n                    plugin = self.load_from_path(py_file)\n                    plugins.append(plugin)\n                except PluginError:\n                    logger.exception(\"Failed to load plugin from %s\", py_file)\n\n        return plugins\n\n    def _instantiate(self, cls: type, source: str) -&gt; PluginBase:\n        \"\"\"Instantiate a plugin class with error handling.\n\n        Args:\n            cls: The class to instantiate.\n            source: Description of where the class came from (for logging).\n\n        Returns:\n            The plugin instance.\n\n        Raises:\n            PluginError: If instantiation fails.\n        \"\"\"\n        try:\n            instance = cls()\n        except Exception as exc:\n            raise PluginError(f\"Failed to instantiate plugin from {source}: {exc}\") from exc\n\n        if not isinstance(instance, PluginBase):\n            raise PluginError(f\"Plugin from {source} is not a PluginBase instance\")\n\n        logger.info(\"Loaded plugin '%s' from %s\", instance.name, source)\n        return instance\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.loader.PluginLoader.__init__","title":"<code>__init__(entry_point_group='agentprobe.plugins')</code>","text":"<p>Initialize the plugin loader.</p> <p>Parameters:</p> Name Type Description Default <code>entry_point_group</code> <code>str</code> <p>Entry point group name for discovery.</p> <code>'agentprobe.plugins'</code> Source code in <code>src/agentprobe/plugins/loader.py</code> <pre><code>def __init__(self, entry_point_group: str = \"agentprobe.plugins\") -&gt; None:\n    \"\"\"Initialize the plugin loader.\n\n    Args:\n        entry_point_group: Entry point group name for discovery.\n    \"\"\"\n    self._entry_point_group = entry_point_group\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.loader.PluginLoader.load_from_entry_points","title":"<code>load_from_entry_points()</code>","text":"<p>Discover and load plugins from installed package entry points.</p> <p>Returns:</p> Type Description <code>list[PluginBase]</code> <p>A list of loaded plugin instances.</p> Source code in <code>src/agentprobe/plugins/loader.py</code> <pre><code>def load_from_entry_points(self) -&gt; list[PluginBase]:\n    \"\"\"Discover and load plugins from installed package entry points.\n\n    Returns:\n        A list of loaded plugin instances.\n    \"\"\"\n    plugins: list[PluginBase] = []\n    try:\n        eps = importlib.metadata.entry_points()\n    except Exception:\n        logger.exception(\"Failed to read entry points\")\n        return plugins\n\n    group_eps: list[Any] = []\n    if hasattr(eps, \"select\"):\n        group_eps = list(eps.select(group=self._entry_point_group))\n    elif isinstance(eps, dict):\n        group_eps = eps.get(self._entry_point_group, [])  # pragma: no cover\n\n    for ep in group_eps:\n        try:\n            plugin_cls = ep.load()\n            plugin = self._instantiate(plugin_cls, source=f\"entry_point:{ep.name}\")\n            plugins.append(plugin)\n        except Exception:\n            logger.exception(\"Failed to load plugin from entry point '%s'\", ep.name)\n\n    return plugins\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.loader.PluginLoader.load_from_path","title":"<code>load_from_path(path)</code>","text":"<p>Load a plugin from a Python file path.</p> <p>The file must contain exactly one class that subclasses PluginBase.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the Python file containing the plugin.</p> required <p>Returns:</p> Type Description <code>PluginBase</code> <p>The loaded plugin instance.</p> <p>Raises:</p> Type Description <code>PluginError</code> <p>If the file cannot be loaded or contains no plugin class.</p> Source code in <code>src/agentprobe/plugins/loader.py</code> <pre><code>def load_from_path(self, path: str | Path) -&gt; PluginBase:\n    \"\"\"Load a plugin from a Python file path.\n\n    The file must contain exactly one class that subclasses PluginBase.\n\n    Args:\n        path: Path to the Python file containing the plugin.\n\n    Returns:\n        The loaded plugin instance.\n\n    Raises:\n        PluginError: If the file cannot be loaded or contains no plugin class.\n    \"\"\"\n    file_path = Path(path)\n    if not file_path.exists():\n        raise PluginError(f\"Plugin file not found: {file_path}\")\n    if not file_path.suffix == \".py\":\n        raise PluginError(f\"Plugin file must be a .py file: {file_path}\")\n\n    module_name = f\"agentprobe_plugin_{file_path.stem}\"\n\n    try:\n        spec = importlib.util.spec_from_file_location(module_name, file_path)\n        if spec is None or spec.loader is None:\n            raise PluginError(f\"Cannot create module spec for: {file_path}\")\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n    except PluginError:\n        raise\n    except Exception as exc:\n        raise PluginError(f\"Failed to load module from {file_path}: {exc}\") from exc\n\n    plugin_classes = [\n        obj\n        for name in dir(module)\n        if not name.startswith(\"_\")\n        and isinstance(obj := getattr(module, name), type)\n        and issubclass(obj, PluginBase)\n        and obj is not PluginBase\n    ]\n\n    if not plugin_classes:\n        raise PluginError(f\"No PluginBase subclass found in {file_path}\")\n\n    return self._instantiate(plugin_classes[0], source=str(file_path))\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.loader.PluginLoader.load_from_class","title":"<code>load_from_class(cls)</code>","text":"<p>Load a plugin from a direct class reference.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[PluginBase]</code> <p>The plugin class to instantiate.</p> required <p>Returns:</p> Type Description <code>PluginBase</code> <p>The loaded plugin instance.</p> <p>Raises:</p> Type Description <code>PluginError</code> <p>If the class is not a PluginBase subclass.</p> Source code in <code>src/agentprobe/plugins/loader.py</code> <pre><code>def load_from_class(self, cls: type[PluginBase]) -&gt; PluginBase:\n    \"\"\"Load a plugin from a direct class reference.\n\n    Args:\n        cls: The plugin class to instantiate.\n\n    Returns:\n        The loaded plugin instance.\n\n    Raises:\n        PluginError: If the class is not a PluginBase subclass.\n    \"\"\"\n    if not (isinstance(cls, type) and issubclass(cls, PluginBase)):\n        raise PluginError(f\"{cls} is not a PluginBase subclass\")\n    return self._instantiate(cls, source=f\"class:{cls.__name__}\")\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.loader.PluginLoader.load_all","title":"<code>load_all(directories=None)</code>","text":"<p>Load plugins from entry points and optional directories.</p> <p>Parameters:</p> Name Type Description Default <code>directories</code> <code>list[str] | None</code> <p>Additional directories to scan for .py plugin files.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[PluginBase]</code> <p>A list of all loaded plugin instances.</p> Source code in <code>src/agentprobe/plugins/loader.py</code> <pre><code>def load_all(\n    self,\n    directories: list[str] | None = None,\n) -&gt; list[PluginBase]:\n    \"\"\"Load plugins from entry points and optional directories.\n\n    Args:\n        directories: Additional directories to scan for .py plugin files.\n\n    Returns:\n        A list of all loaded plugin instances.\n    \"\"\"\n    plugins = self.load_from_entry_points()\n\n    for dir_path in directories or []:\n        path = Path(dir_path)\n        if not path.is_dir():\n            logger.warning(\"Plugin directory not found: %s\", dir_path)\n            continue\n        for py_file in sorted(path.glob(\"*.py\")):\n            if py_file.name.startswith(\"_\"):\n                continue\n            try:\n                plugin = self.load_from_path(py_file)\n                plugins.append(plugin)\n            except PluginError:\n                logger.exception(\"Failed to load plugin from %s\", py_file)\n\n    return plugins\n</code></pre>"},{"location":"reference/api/plugins/#manager","title":"Manager","text":""},{"location":"reference/api/plugins/#agentprobe.plugins.manager","title":"<code>agentprobe.plugins.manager</code>","text":"<p>Plugin manager: orchestrates plugin lifecycle and event dispatch.</p> <p>Coordinates loading, registration, lifecycle hooks, and factory collection across all loaded plugins.</p>"},{"location":"reference/api/plugins/#agentprobe.plugins.manager.PluginManager","title":"<code>PluginManager</code>","text":"<p>Orchestrates plugin lifecycle and event dispatch.</p> <p>Manages the full lifecycle of plugins: loading from sources, registering, dispatching lifecycle events, and collecting factory-created objects from typed plugins.</p> <p>Attributes:</p> Name Type Description <code>registry</code> <p>The plugin registry.</p> Source code in <code>src/agentprobe/plugins/manager.py</code> <pre><code>class PluginManager:\n    \"\"\"Orchestrates plugin lifecycle and event dispatch.\n\n    Manages the full lifecycle of plugins: loading from sources,\n    registering, dispatching lifecycle events, and collecting\n    factory-created objects from typed plugins.\n\n    Attributes:\n        registry: The plugin registry.\n    \"\"\"\n\n    def __init__(\n        self,\n        entry_point_group: str = \"agentprobe.plugins\",\n    ) -&gt; None:\n        \"\"\"Initialize the plugin manager.\n\n        Args:\n            entry_point_group: Entry point group for plugin discovery.\n        \"\"\"\n        self.registry = PluginRegistry()\n        self._loader = PluginLoader(entry_point_group=entry_point_group)\n\n    def load_plugins(\n        self,\n        directories: list[str] | None = None,\n        classes: list[type[PluginBase]] | None = None,\n    ) -&gt; list[PluginBase]:\n        \"\"\"Load and register plugins from all sources.\n\n        Args:\n            directories: Additional directories to scan.\n            classes: Direct class references to load.\n\n        Returns:\n            A list of all loaded and registered plugins.\n        \"\"\"\n        loaded: list[PluginBase] = []\n\n        discovered = self._loader.load_all(directories=directories)\n        for plugin in discovered:\n            self._safe_register(plugin)\n            loaded.append(plugin)\n\n        for cls in classes or []:\n            try:\n                plugin = self._loader.load_from_class(cls)\n                self._safe_register(plugin)\n                loaded.append(plugin)\n            except Exception:\n                logger.exception(\"Failed to load plugin class %s\", cls.__name__)\n\n        return loaded\n\n    def unload_all(self) -&gt; None:\n        \"\"\"Unload all plugins, calling on_unload for each.\"\"\"\n        for plugin in self.registry.list_plugins():\n            try:\n                plugin.on_unload()\n            except Exception:\n                logger.exception(\"Error during on_unload for plugin '%s'\", plugin.name)\n        self.registry.clear()\n\n    def dispatch_test_start(self, test_name: str, **kwargs: Any) -&gt; None:\n        \"\"\"Dispatch on_test_start to all registered plugins.\n\n        Errors from individual plugins are logged but do not propagate.\n\n        Args:\n            test_name: Name of the test starting.\n            **kwargs: Additional context.\n        \"\"\"\n        for plugin in self.registry.list_plugins():\n            try:\n                plugin.on_test_start(test_name, **kwargs)\n            except Exception:\n                logger.exception(\"Plugin '%s' failed in on_test_start\", plugin.name)\n\n    def dispatch_test_end(self, test_name: str, **kwargs: Any) -&gt; None:\n        \"\"\"Dispatch on_test_end to all registered plugins.\n\n        Args:\n            test_name: Name of the test ending.\n            **kwargs: Additional context.\n        \"\"\"\n        for plugin in self.registry.list_plugins():\n            try:\n                plugin.on_test_end(test_name, **kwargs)\n            except Exception:\n                logger.exception(\"Plugin '%s' failed in on_test_end\", plugin.name)\n\n    def dispatch_suite_start(self, **kwargs: Any) -&gt; None:\n        \"\"\"Dispatch on_suite_start to all registered plugins.\n\n        Args:\n            **kwargs: Additional context.\n        \"\"\"\n        for plugin in self.registry.list_plugins():\n            try:\n                plugin.on_suite_start(**kwargs)\n            except Exception:\n                logger.exception(\"Plugin '%s' failed in on_suite_start\", plugin.name)\n\n    def dispatch_suite_end(self, **kwargs: Any) -&gt; None:\n        \"\"\"Dispatch on_suite_end to all registered plugins.\n\n        Args:\n            **kwargs: Additional context.\n        \"\"\"\n        for plugin in self.registry.list_plugins():\n            try:\n                plugin.on_suite_end(**kwargs)\n            except Exception:\n                logger.exception(\"Plugin '%s' failed in on_suite_end\", plugin.name)\n\n    def get_evaluators(self) -&gt; list[EvaluatorProtocol]:\n        \"\"\"Collect evaluators from all EvaluatorPlugin instances.\n\n        Returns:\n            A list of evaluator instances.\n        \"\"\"\n        evaluators: list[EvaluatorProtocol] = []\n        for plugin in self.registry.list_by_type(PluginType.EVALUATOR):\n            if isinstance(plugin, EvaluatorPlugin):\n                try:\n                    evaluators.append(plugin.create_evaluator())\n                except Exception:\n                    logger.exception(\"Plugin '%s' failed to create evaluator\", plugin.name)\n        return evaluators\n\n    def get_adapters(self) -&gt; list[AdapterProtocol]:\n        \"\"\"Collect adapters from all AdapterPlugin instances.\n\n        Returns:\n            A list of adapter instances.\n        \"\"\"\n        adapters: list[AdapterProtocol] = []\n        for plugin in self.registry.list_by_type(PluginType.ADAPTER):\n            if isinstance(plugin, AdapterPlugin):\n                try:\n                    adapters.append(plugin.create_adapter())\n                except Exception:\n                    logger.exception(\"Plugin '%s' failed to create adapter\", plugin.name)\n        return adapters\n\n    def get_reporters(self) -&gt; list[ReporterProtocol]:\n        \"\"\"Collect reporters from all ReporterPlugin instances.\n\n        Returns:\n            A list of reporter instances.\n        \"\"\"\n        reporters: list[ReporterProtocol] = []\n        for plugin in self.registry.list_by_type(PluginType.REPORTER):\n            if isinstance(plugin, ReporterPlugin):\n                try:\n                    reporters.append(plugin.create_reporter())\n                except Exception:\n                    logger.exception(\"Plugin '%s' failed to create reporter\", plugin.name)\n        return reporters\n\n    def get_storage_backends(self) -&gt; list[StorageProtocol]:\n        \"\"\"Collect storage backends from all StoragePlugin instances.\n\n        Returns:\n            A list of storage backend instances.\n        \"\"\"\n        backends: list[StorageProtocol] = []\n        for plugin in self.registry.list_by_type(PluginType.STORAGE):\n            if isinstance(plugin, StoragePlugin):\n                try:\n                    backends.append(plugin.create_storage())\n                except Exception:\n                    logger.exception(\"Plugin '%s' failed to create storage\", plugin.name)\n        return backends\n\n    def _safe_register(self, plugin: PluginBase) -&gt; None:\n        \"\"\"Register a plugin, calling on_load after registration.\n\n        Args:\n            plugin: The plugin to register.\n        \"\"\"\n        self.registry.register(plugin)\n        try:\n            plugin.on_load()\n        except Exception:\n            logger.exception(\"Plugin '%s' failed in on_load\", plugin.name)\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.manager.PluginManager.__init__","title":"<code>__init__(entry_point_group='agentprobe.plugins')</code>","text":"<p>Initialize the plugin manager.</p> <p>Parameters:</p> Name Type Description Default <code>entry_point_group</code> <code>str</code> <p>Entry point group for plugin discovery.</p> <code>'agentprobe.plugins'</code> Source code in <code>src/agentprobe/plugins/manager.py</code> <pre><code>def __init__(\n    self,\n    entry_point_group: str = \"agentprobe.plugins\",\n) -&gt; None:\n    \"\"\"Initialize the plugin manager.\n\n    Args:\n        entry_point_group: Entry point group for plugin discovery.\n    \"\"\"\n    self.registry = PluginRegistry()\n    self._loader = PluginLoader(entry_point_group=entry_point_group)\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.manager.PluginManager.load_plugins","title":"<code>load_plugins(directories=None, classes=None)</code>","text":"<p>Load and register plugins from all sources.</p> <p>Parameters:</p> Name Type Description Default <code>directories</code> <code>list[str] | None</code> <p>Additional directories to scan.</p> <code>None</code> <code>classes</code> <code>list[type[PluginBase]] | None</code> <p>Direct class references to load.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[PluginBase]</code> <p>A list of all loaded and registered plugins.</p> Source code in <code>src/agentprobe/plugins/manager.py</code> <pre><code>def load_plugins(\n    self,\n    directories: list[str] | None = None,\n    classes: list[type[PluginBase]] | None = None,\n) -&gt; list[PluginBase]:\n    \"\"\"Load and register plugins from all sources.\n\n    Args:\n        directories: Additional directories to scan.\n        classes: Direct class references to load.\n\n    Returns:\n        A list of all loaded and registered plugins.\n    \"\"\"\n    loaded: list[PluginBase] = []\n\n    discovered = self._loader.load_all(directories=directories)\n    for plugin in discovered:\n        self._safe_register(plugin)\n        loaded.append(plugin)\n\n    for cls in classes or []:\n        try:\n            plugin = self._loader.load_from_class(cls)\n            self._safe_register(plugin)\n            loaded.append(plugin)\n        except Exception:\n            logger.exception(\"Failed to load plugin class %s\", cls.__name__)\n\n    return loaded\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.manager.PluginManager.unload_all","title":"<code>unload_all()</code>","text":"<p>Unload all plugins, calling on_unload for each.</p> Source code in <code>src/agentprobe/plugins/manager.py</code> <pre><code>def unload_all(self) -&gt; None:\n    \"\"\"Unload all plugins, calling on_unload for each.\"\"\"\n    for plugin in self.registry.list_plugins():\n        try:\n            plugin.on_unload()\n        except Exception:\n            logger.exception(\"Error during on_unload for plugin '%s'\", plugin.name)\n    self.registry.clear()\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.manager.PluginManager.dispatch_test_start","title":"<code>dispatch_test_start(test_name, **kwargs)</code>","text":"<p>Dispatch on_test_start to all registered plugins.</p> <p>Errors from individual plugins are logged but do not propagate.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>str</code> <p>Name of the test starting.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional context.</p> <code>{}</code> Source code in <code>src/agentprobe/plugins/manager.py</code> <pre><code>def dispatch_test_start(self, test_name: str, **kwargs: Any) -&gt; None:\n    \"\"\"Dispatch on_test_start to all registered plugins.\n\n    Errors from individual plugins are logged but do not propagate.\n\n    Args:\n        test_name: Name of the test starting.\n        **kwargs: Additional context.\n    \"\"\"\n    for plugin in self.registry.list_plugins():\n        try:\n            plugin.on_test_start(test_name, **kwargs)\n        except Exception:\n            logger.exception(\"Plugin '%s' failed in on_test_start\", plugin.name)\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.manager.PluginManager.dispatch_test_end","title":"<code>dispatch_test_end(test_name, **kwargs)</code>","text":"<p>Dispatch on_test_end to all registered plugins.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>str</code> <p>Name of the test ending.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional context.</p> <code>{}</code> Source code in <code>src/agentprobe/plugins/manager.py</code> <pre><code>def dispatch_test_end(self, test_name: str, **kwargs: Any) -&gt; None:\n    \"\"\"Dispatch on_test_end to all registered plugins.\n\n    Args:\n        test_name: Name of the test ending.\n        **kwargs: Additional context.\n    \"\"\"\n    for plugin in self.registry.list_plugins():\n        try:\n            plugin.on_test_end(test_name, **kwargs)\n        except Exception:\n            logger.exception(\"Plugin '%s' failed in on_test_end\", plugin.name)\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.manager.PluginManager.dispatch_suite_start","title":"<code>dispatch_suite_start(**kwargs)</code>","text":"<p>Dispatch on_suite_start to all registered plugins.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional context.</p> <code>{}</code> Source code in <code>src/agentprobe/plugins/manager.py</code> <pre><code>def dispatch_suite_start(self, **kwargs: Any) -&gt; None:\n    \"\"\"Dispatch on_suite_start to all registered plugins.\n\n    Args:\n        **kwargs: Additional context.\n    \"\"\"\n    for plugin in self.registry.list_plugins():\n        try:\n            plugin.on_suite_start(**kwargs)\n        except Exception:\n            logger.exception(\"Plugin '%s' failed in on_suite_start\", plugin.name)\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.manager.PluginManager.dispatch_suite_end","title":"<code>dispatch_suite_end(**kwargs)</code>","text":"<p>Dispatch on_suite_end to all registered plugins.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional context.</p> <code>{}</code> Source code in <code>src/agentprobe/plugins/manager.py</code> <pre><code>def dispatch_suite_end(self, **kwargs: Any) -&gt; None:\n    \"\"\"Dispatch on_suite_end to all registered plugins.\n\n    Args:\n        **kwargs: Additional context.\n    \"\"\"\n    for plugin in self.registry.list_plugins():\n        try:\n            plugin.on_suite_end(**kwargs)\n        except Exception:\n            logger.exception(\"Plugin '%s' failed in on_suite_end\", plugin.name)\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.manager.PluginManager.get_evaluators","title":"<code>get_evaluators()</code>","text":"<p>Collect evaluators from all EvaluatorPlugin instances.</p> <p>Returns:</p> Type Description <code>list[EvaluatorProtocol]</code> <p>A list of evaluator instances.</p> Source code in <code>src/agentprobe/plugins/manager.py</code> <pre><code>def get_evaluators(self) -&gt; list[EvaluatorProtocol]:\n    \"\"\"Collect evaluators from all EvaluatorPlugin instances.\n\n    Returns:\n        A list of evaluator instances.\n    \"\"\"\n    evaluators: list[EvaluatorProtocol] = []\n    for plugin in self.registry.list_by_type(PluginType.EVALUATOR):\n        if isinstance(plugin, EvaluatorPlugin):\n            try:\n                evaluators.append(plugin.create_evaluator())\n            except Exception:\n                logger.exception(\"Plugin '%s' failed to create evaluator\", plugin.name)\n    return evaluators\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.manager.PluginManager.get_adapters","title":"<code>get_adapters()</code>","text":"<p>Collect adapters from all AdapterPlugin instances.</p> <p>Returns:</p> Type Description <code>list[AdapterProtocol]</code> <p>A list of adapter instances.</p> Source code in <code>src/agentprobe/plugins/manager.py</code> <pre><code>def get_adapters(self) -&gt; list[AdapterProtocol]:\n    \"\"\"Collect adapters from all AdapterPlugin instances.\n\n    Returns:\n        A list of adapter instances.\n    \"\"\"\n    adapters: list[AdapterProtocol] = []\n    for plugin in self.registry.list_by_type(PluginType.ADAPTER):\n        if isinstance(plugin, AdapterPlugin):\n            try:\n                adapters.append(plugin.create_adapter())\n            except Exception:\n                logger.exception(\"Plugin '%s' failed to create adapter\", plugin.name)\n    return adapters\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.manager.PluginManager.get_reporters","title":"<code>get_reporters()</code>","text":"<p>Collect reporters from all ReporterPlugin instances.</p> <p>Returns:</p> Type Description <code>list[ReporterProtocol]</code> <p>A list of reporter instances.</p> Source code in <code>src/agentprobe/plugins/manager.py</code> <pre><code>def get_reporters(self) -&gt; list[ReporterProtocol]:\n    \"\"\"Collect reporters from all ReporterPlugin instances.\n\n    Returns:\n        A list of reporter instances.\n    \"\"\"\n    reporters: list[ReporterProtocol] = []\n    for plugin in self.registry.list_by_type(PluginType.REPORTER):\n        if isinstance(plugin, ReporterPlugin):\n            try:\n                reporters.append(plugin.create_reporter())\n            except Exception:\n                logger.exception(\"Plugin '%s' failed to create reporter\", plugin.name)\n    return reporters\n</code></pre>"},{"location":"reference/api/plugins/#agentprobe.plugins.manager.PluginManager.get_storage_backends","title":"<code>get_storage_backends()</code>","text":"<p>Collect storage backends from all StoragePlugin instances.</p> <p>Returns:</p> Type Description <code>list[StorageProtocol]</code> <p>A list of storage backend instances.</p> Source code in <code>src/agentprobe/plugins/manager.py</code> <pre><code>def get_storage_backends(self) -&gt; list[StorageProtocol]:\n    \"\"\"Collect storage backends from all StoragePlugin instances.\n\n    Returns:\n        A list of storage backend instances.\n    \"\"\"\n    backends: list[StorageProtocol] = []\n    for plugin in self.registry.list_by_type(PluginType.STORAGE):\n        if isinstance(plugin, StoragePlugin):\n            try:\n                backends.append(plugin.create_storage())\n            except Exception:\n                logger.exception(\"Plugin '%s' failed to create storage\", plugin.name)\n    return backends\n</code></pre>"},{"location":"reference/api/regression/","title":"Regression","text":"<p>Regression detection and baseline management.</p>"},{"location":"reference/api/regression/#detector","title":"Detector","text":""},{"location":"reference/api/regression/#agentprobe.regression.detector","title":"<code>agentprobe.regression.detector</code>","text":"<p>Regression detection by comparing baseline and current test results.</p> <p>Flags regressions (score decreases) and improvements (score increases) based on configurable delta thresholds.</p>"},{"location":"reference/api/regression/#agentprobe.regression.detector.RegressionDetector","title":"<code>RegressionDetector</code>","text":"<p>Compares current test results against a baseline to detect regressions.</p> <p>Attributes:</p> Name Type Description <code>threshold</code> <p>Minimum score delta to flag as regression/improvement.</p> Source code in <code>src/agentprobe/regression/detector.py</code> <pre><code>class RegressionDetector:\n    \"\"\"Compares current test results against a baseline to detect regressions.\n\n    Attributes:\n        threshold: Minimum score delta to flag as regression/improvement.\n    \"\"\"\n\n    def __init__(self, threshold: float = 0.05) -&gt; None:\n        \"\"\"Initialize the regression detector.\n\n        Args:\n            threshold: Score delta threshold for flagging changes.\n        \"\"\"\n        self._threshold = threshold\n\n    def compare(\n        self,\n        baseline_name: str,\n        baseline_results: Sequence[TestResult],\n        current_results: Sequence[TestResult],\n    ) -&gt; RegressionReport:\n        \"\"\"Compare current results against a baseline.\n\n        Tests are matched by name. Tests present in only one set are\n        excluded from comparison.\n\n        Args:\n            baseline_name: Name of the baseline for reporting.\n            baseline_results: Test results from the baseline run.\n            current_results: Test results from the current run.\n\n        Returns:\n            A RegressionReport with per-test comparisons.\n        \"\"\"\n        baseline_map = {r.test_name: r for r in baseline_results}\n        current_map = {r.test_name: r for r in current_results}\n\n        common_names = sorted(set(baseline_map) &amp; set(current_map))\n        comparisons: list[TestComparison] = []\n        regressions = 0\n        improvements = 0\n        unchanged = 0\n\n        for name in common_names:\n            bl = baseline_map[name]\n            cr = current_map[name]\n            delta = round(cr.score - bl.score, 6)\n\n            is_regression = delta &lt; -self._threshold\n            is_improvement = delta &gt; self._threshold\n\n            if is_regression:\n                regressions += 1\n                logger.warning(\n                    \"Regression detected: %s (%.3f -&gt; %.3f, delta=%.3f)\",\n                    name,\n                    bl.score,\n                    cr.score,\n                    delta,\n                )\n            elif is_improvement:\n                improvements += 1\n                logger.info(\n                    \"Improvement detected: %s (%.3f -&gt; %.3f, delta=%.3f)\",\n                    name,\n                    bl.score,\n                    cr.score,\n                    delta,\n                )\n            else:\n                unchanged += 1\n\n            comparisons.append(\n                TestComparison(\n                    test_name=name,\n                    baseline_score=bl.score,\n                    current_score=cr.score,\n                    delta=delta,\n                    is_regression=is_regression,\n                    is_improvement=is_improvement,\n                )\n            )\n\n        return RegressionReport(\n            baseline_name=baseline_name,\n            comparisons=tuple(comparisons),\n            total_tests=len(comparisons),\n            regressions=regressions,\n            improvements=improvements,\n            unchanged=unchanged,\n            threshold=self._threshold,\n        )\n</code></pre>"},{"location":"reference/api/regression/#agentprobe.regression.detector.RegressionDetector.__init__","title":"<code>__init__(threshold=0.05)</code>","text":"<p>Initialize the regression detector.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Score delta threshold for flagging changes.</p> <code>0.05</code> Source code in <code>src/agentprobe/regression/detector.py</code> <pre><code>def __init__(self, threshold: float = 0.05) -&gt; None:\n    \"\"\"Initialize the regression detector.\n\n    Args:\n        threshold: Score delta threshold for flagging changes.\n    \"\"\"\n    self._threshold = threshold\n</code></pre>"},{"location":"reference/api/regression/#agentprobe.regression.detector.RegressionDetector.compare","title":"<code>compare(baseline_name, baseline_results, current_results)</code>","text":"<p>Compare current results against a baseline.</p> <p>Tests are matched by name. Tests present in only one set are excluded from comparison.</p> <p>Parameters:</p> Name Type Description Default <code>baseline_name</code> <code>str</code> <p>Name of the baseline for reporting.</p> required <code>baseline_results</code> <code>Sequence[TestResult]</code> <p>Test results from the baseline run.</p> required <code>current_results</code> <code>Sequence[TestResult]</code> <p>Test results from the current run.</p> required <p>Returns:</p> Type Description <code>RegressionReport</code> <p>A RegressionReport with per-test comparisons.</p> Source code in <code>src/agentprobe/regression/detector.py</code> <pre><code>def compare(\n    self,\n    baseline_name: str,\n    baseline_results: Sequence[TestResult],\n    current_results: Sequence[TestResult],\n) -&gt; RegressionReport:\n    \"\"\"Compare current results against a baseline.\n\n    Tests are matched by name. Tests present in only one set are\n    excluded from comparison.\n\n    Args:\n        baseline_name: Name of the baseline for reporting.\n        baseline_results: Test results from the baseline run.\n        current_results: Test results from the current run.\n\n    Returns:\n        A RegressionReport with per-test comparisons.\n    \"\"\"\n    baseline_map = {r.test_name: r for r in baseline_results}\n    current_map = {r.test_name: r for r in current_results}\n\n    common_names = sorted(set(baseline_map) &amp; set(current_map))\n    comparisons: list[TestComparison] = []\n    regressions = 0\n    improvements = 0\n    unchanged = 0\n\n    for name in common_names:\n        bl = baseline_map[name]\n        cr = current_map[name]\n        delta = round(cr.score - bl.score, 6)\n\n        is_regression = delta &lt; -self._threshold\n        is_improvement = delta &gt; self._threshold\n\n        if is_regression:\n            regressions += 1\n            logger.warning(\n                \"Regression detected: %s (%.3f -&gt; %.3f, delta=%.3f)\",\n                name,\n                bl.score,\n                cr.score,\n                delta,\n            )\n        elif is_improvement:\n            improvements += 1\n            logger.info(\n                \"Improvement detected: %s (%.3f -&gt; %.3f, delta=%.3f)\",\n                name,\n                bl.score,\n                cr.score,\n                delta,\n            )\n        else:\n            unchanged += 1\n\n        comparisons.append(\n            TestComparison(\n                test_name=name,\n                baseline_score=bl.score,\n                current_score=cr.score,\n                delta=delta,\n                is_regression=is_regression,\n                is_improvement=is_improvement,\n            )\n        )\n\n    return RegressionReport(\n        baseline_name=baseline_name,\n        comparisons=tuple(comparisons),\n        total_tests=len(comparisons),\n        regressions=regressions,\n        improvements=improvements,\n        unchanged=unchanged,\n        threshold=self._threshold,\n    )\n</code></pre>"},{"location":"reference/api/regression/#baseline-manager","title":"Baseline Manager","text":""},{"location":"reference/api/regression/#agentprobe.regression.baseline","title":"<code>agentprobe.regression.baseline</code>","text":"<p>Baseline management for regression testing.</p> <p>Provides CRUD operations for named baselines stored as JSON files containing serialized TestResult lists.</p>"},{"location":"reference/api/regression/#agentprobe.regression.baseline.BaselineManager","title":"<code>BaselineManager</code>","text":"<p>Manages baseline files for regression testing.</p> <p>Stores sets of TestResult objects as JSON files, enabling comparison between historical and current test runs.</p> <p>Attributes:</p> Name Type Description <code>baseline_dir</code> <p>Directory where baseline files are stored.</p> Source code in <code>src/agentprobe/regression/baseline.py</code> <pre><code>class BaselineManager:\n    \"\"\"Manages baseline files for regression testing.\n\n    Stores sets of TestResult objects as JSON files, enabling\n    comparison between historical and current test runs.\n\n    Attributes:\n        baseline_dir: Directory where baseline files are stored.\n    \"\"\"\n\n    def __init__(self, baseline_dir: str | Path = \".agentprobe/baselines\") -&gt; None:\n        \"\"\"Initialize the baseline manager.\n\n        Args:\n            baseline_dir: Directory for baseline storage.\n        \"\"\"\n        self._dir = Path(baseline_dir)\n\n    def _baseline_path(self, name: str) -&gt; Path:\n        \"\"\"Get the file path for a named baseline.\"\"\"\n        return self._dir / f\"{name}.json\"\n\n    def save(self, name: str, results: Sequence[TestResult]) -&gt; Path:\n        \"\"\"Save test results as a named baseline.\n\n        Args:\n            name: Baseline name.\n            results: Test results to save.\n\n        Returns:\n            Path to the saved baseline file.\n        \"\"\"\n        self._dir.mkdir(parents=True, exist_ok=True)\n        path = self._baseline_path(name)\n\n        data = [json.loads(r.model_dump_json()) for r in results]\n        path.write_text(\n            json.dumps(data, indent=2, ensure_ascii=False),\n            encoding=\"utf-8\",\n        )\n        logger.info(\"Baseline saved: %s (%d results)\", name, len(data))\n        return path\n\n    def load(self, name: str) -&gt; list[TestResult]:\n        \"\"\"Load a named baseline.\n\n        Args:\n            name: Baseline name.\n\n        Returns:\n            List of saved TestResult objects.\n\n        Raises:\n            RegressionError: If the baseline does not exist.\n        \"\"\"\n        path = self._baseline_path(name)\n        if not path.exists():\n            raise RegressionError(f\"Baseline not found: {name}\")\n\n        raw = json.loads(path.read_text(encoding=\"utf-8\"))\n        return [TestResult.model_validate_json(json.dumps(item)) for item in raw]\n\n    def exists(self, name: str) -&gt; bool:\n        \"\"\"Check if a named baseline exists.\"\"\"\n        return self._baseline_path(name).exists()\n\n    def list_baselines(self) -&gt; list[str]:\n        \"\"\"List all baseline names.\"\"\"\n        if not self._dir.is_dir():\n            return []\n        return sorted(p.stem for p in self._dir.glob(\"*.json\"))\n\n    def delete(self, name: str) -&gt; bool:\n        \"\"\"Delete a named baseline.\n\n        Args:\n            name: Baseline name.\n\n        Returns:\n            True if deleted, False if not found.\n        \"\"\"\n        path = self._baseline_path(name)\n        if path.exists():\n            path.unlink()\n            logger.info(\"Baseline deleted: %s\", name)\n            return True\n        return False\n</code></pre>"},{"location":"reference/api/regression/#agentprobe.regression.baseline.BaselineManager.__init__","title":"<code>__init__(baseline_dir='.agentprobe/baselines')</code>","text":"<p>Initialize the baseline manager.</p> <p>Parameters:</p> Name Type Description Default <code>baseline_dir</code> <code>str | Path</code> <p>Directory for baseline storage.</p> <code>'.agentprobe/baselines'</code> Source code in <code>src/agentprobe/regression/baseline.py</code> <pre><code>def __init__(self, baseline_dir: str | Path = \".agentprobe/baselines\") -&gt; None:\n    \"\"\"Initialize the baseline manager.\n\n    Args:\n        baseline_dir: Directory for baseline storage.\n    \"\"\"\n    self._dir = Path(baseline_dir)\n</code></pre>"},{"location":"reference/api/regression/#agentprobe.regression.baseline.BaselineManager.save","title":"<code>save(name, results)</code>","text":"<p>Save test results as a named baseline.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Baseline name.</p> required <code>results</code> <code>Sequence[TestResult]</code> <p>Test results to save.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved baseline file.</p> Source code in <code>src/agentprobe/regression/baseline.py</code> <pre><code>def save(self, name: str, results: Sequence[TestResult]) -&gt; Path:\n    \"\"\"Save test results as a named baseline.\n\n    Args:\n        name: Baseline name.\n        results: Test results to save.\n\n    Returns:\n        Path to the saved baseline file.\n    \"\"\"\n    self._dir.mkdir(parents=True, exist_ok=True)\n    path = self._baseline_path(name)\n\n    data = [json.loads(r.model_dump_json()) for r in results]\n    path.write_text(\n        json.dumps(data, indent=2, ensure_ascii=False),\n        encoding=\"utf-8\",\n    )\n    logger.info(\"Baseline saved: %s (%d results)\", name, len(data))\n    return path\n</code></pre>"},{"location":"reference/api/regression/#agentprobe.regression.baseline.BaselineManager.load","title":"<code>load(name)</code>","text":"<p>Load a named baseline.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Baseline name.</p> required <p>Returns:</p> Type Description <code>list[TestResult]</code> <p>List of saved TestResult objects.</p> <p>Raises:</p> Type Description <code>RegressionError</code> <p>If the baseline does not exist.</p> Source code in <code>src/agentprobe/regression/baseline.py</code> <pre><code>def load(self, name: str) -&gt; list[TestResult]:\n    \"\"\"Load a named baseline.\n\n    Args:\n        name: Baseline name.\n\n    Returns:\n        List of saved TestResult objects.\n\n    Raises:\n        RegressionError: If the baseline does not exist.\n    \"\"\"\n    path = self._baseline_path(name)\n    if not path.exists():\n        raise RegressionError(f\"Baseline not found: {name}\")\n\n    raw = json.loads(path.read_text(encoding=\"utf-8\"))\n    return [TestResult.model_validate_json(json.dumps(item)) for item in raw]\n</code></pre>"},{"location":"reference/api/regression/#agentprobe.regression.baseline.BaselineManager.exists","title":"<code>exists(name)</code>","text":"<p>Check if a named baseline exists.</p> Source code in <code>src/agentprobe/regression/baseline.py</code> <pre><code>def exists(self, name: str) -&gt; bool:\n    \"\"\"Check if a named baseline exists.\"\"\"\n    return self._baseline_path(name).exists()\n</code></pre>"},{"location":"reference/api/regression/#agentprobe.regression.baseline.BaselineManager.list_baselines","title":"<code>list_baselines()</code>","text":"<p>List all baseline names.</p> Source code in <code>src/agentprobe/regression/baseline.py</code> <pre><code>def list_baselines(self) -&gt; list[str]:\n    \"\"\"List all baseline names.\"\"\"\n    if not self._dir.is_dir():\n        return []\n    return sorted(p.stem for p in self._dir.glob(\"*.json\"))\n</code></pre>"},{"location":"reference/api/regression/#agentprobe.regression.baseline.BaselineManager.delete","title":"<code>delete(name)</code>","text":"<p>Delete a named baseline.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Baseline name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted, False if not found.</p> Source code in <code>src/agentprobe/regression/baseline.py</code> <pre><code>def delete(self, name: str) -&gt; bool:\n    \"\"\"Delete a named baseline.\n\n    Args:\n        name: Baseline name.\n\n    Returns:\n        True if deleted, False if not found.\n    \"\"\"\n    path = self._baseline_path(name)\n    if path.exists():\n        path.unlink()\n        logger.info(\"Baseline deleted: %s\", name)\n        return True\n    return False\n</code></pre>"},{"location":"reference/api/reporting/","title":"Reporting","text":"<p>Output formatters for test results.</p>"},{"location":"reference/api/reporting/#terminal-reporter","title":"Terminal Reporter","text":""},{"location":"reference/api/reporting/#agentprobe.reporting.terminal","title":"<code>agentprobe.reporting.terminal</code>","text":"<p>Terminal reporter using Rich for formatted output.</p> <p>Produces colored tables, progress summaries, and status panels for agent test runs.</p>"},{"location":"reference/api/reporting/#agentprobe.reporting.terminal.TerminalReporter","title":"<code>TerminalReporter</code>","text":"<p>Reporter that outputs formatted results to the terminal.</p> <p>Uses Rich for colored tables, panels, and progress indicators.</p> <p>Attributes:</p> Name Type Description <code>console</code> <p>Rich Console instance for output.</p> Source code in <code>src/agentprobe/reporting/terminal.py</code> <pre><code>class TerminalReporter:\n    \"\"\"Reporter that outputs formatted results to the terminal.\n\n    Uses Rich for colored tables, panels, and progress indicators.\n\n    Attributes:\n        console: Rich Console instance for output.\n    \"\"\"\n\n    def __init__(self, console: Console | None = None) -&gt; None:\n        \"\"\"Initialize the terminal reporter.\n\n        Args:\n            console: Rich Console to use. Creates a new one if None.\n        \"\"\"\n        self._console = console or Console()\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the reporter name.\"\"\"\n        return \"terminal\"\n\n    async def report(self, run: AgentRun) -&gt; None:\n        \"\"\"Print a formatted test run report to the terminal.\n\n        Args:\n            run: The completed agent run.\n        \"\"\"\n        self._print_header(run)\n        self._print_results_table(run)\n        self._print_summary(run)\n\n    def _print_header(self, run: AgentRun) -&gt; None:\n        \"\"\"Print the report header with agent name and status.\"\"\"\n        status_color = \"green\" if run.failed == 0 and run.errors == 0 else \"red\"\n        self._console.print(\n            Panel(\n                f\"[bold]{run.agent_name}[/bold] \u2014 {run.status.value}\",\n                title=\"AgentProbe Test Report\",\n                border_style=status_color,\n            )\n        )\n\n    def _print_results_table(self, run: AgentRun) -&gt; None:\n        \"\"\"Print a table of individual test results.\"\"\"\n        if not run.test_results:\n            self._console.print(\"[dim]No test results.[/dim]\")\n            return\n\n        table = Table(show_header=True, header_style=\"bold\")\n        table.add_column(\"Test\", style=\"cyan\", min_width=20)\n        table.add_column(\"Status\", justify=\"center\", min_width=10)\n        table.add_column(\"Score\", justify=\"right\", min_width=8)\n        table.add_column(\"Duration\", justify=\"right\", min_width=10)\n        table.add_column(\"Details\", min_width=20)\n\n        for result in run.test_results:\n            color = _STATUS_COLORS.get(result.status, \"white\")\n            details = \"\"\n            if result.error_message:\n                details = result.error_message[:50]\n            elif result.eval_results:\n                verdicts = [r.verdict.value for r in result.eval_results]\n                details = \", \".join(verdicts)\n\n            table.add_row(\n                result.test_name,\n                f\"[{color}]{result.status.value}[/{color}]\",\n                f\"{result.score:.2f}\",\n                f\"{result.duration_ms}ms\",\n                details,\n            )\n\n        self._console.print(table)\n\n    def _print_summary(self, run: AgentRun) -&gt; None:\n        \"\"\"Print a summary panel with totals.\"\"\"\n        parts = [\n            f\"Total: {run.total_tests}\",\n            f\"[green]Passed: {run.passed}[/green]\",\n            f\"[red]Failed: {run.failed}[/red]\",\n        ]\n        if run.errors &gt; 0:\n            parts.append(f\"[red bold]Errors: {run.errors}[/red bold]\")\n        if run.skipped &gt; 0:\n            parts.append(f\"[dim]Skipped: {run.skipped}[/dim]\")\n        parts.append(f\"Duration: {run.duration_ms}ms\")\n\n        self._console.print()\n        self._console.print(\" | \".join(parts))\n</code></pre>"},{"location":"reference/api/reporting/#agentprobe.reporting.terminal.TerminalReporter.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the reporter name.</p>"},{"location":"reference/api/reporting/#agentprobe.reporting.terminal.TerminalReporter.__init__","title":"<code>__init__(console=None)</code>","text":"<p>Initialize the terminal reporter.</p> <p>Parameters:</p> Name Type Description Default <code>console</code> <code>Console | None</code> <p>Rich Console to use. Creates a new one if None.</p> <code>None</code> Source code in <code>src/agentprobe/reporting/terminal.py</code> <pre><code>def __init__(self, console: Console | None = None) -&gt; None:\n    \"\"\"Initialize the terminal reporter.\n\n    Args:\n        console: Rich Console to use. Creates a new one if None.\n    \"\"\"\n    self._console = console or Console()\n</code></pre>"},{"location":"reference/api/reporting/#agentprobe.reporting.terminal.TerminalReporter.report","title":"<code>report(run)</code>  <code>async</code>","text":"<p>Print a formatted test run report to the terminal.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>AgentRun</code> <p>The completed agent run.</p> required Source code in <code>src/agentprobe/reporting/terminal.py</code> <pre><code>async def report(self, run: AgentRun) -&gt; None:\n    \"\"\"Print a formatted test run report to the terminal.\n\n    Args:\n        run: The completed agent run.\n    \"\"\"\n    self._print_header(run)\n    self._print_results_table(run)\n    self._print_summary(run)\n</code></pre>"},{"location":"reference/api/reporting/#html-reporter","title":"HTML Reporter","text":""},{"location":"reference/api/reporting/#agentprobe.reporting.html","title":"<code>agentprobe.reporting.html</code>","text":"<p>HTML reporter for test results.</p> <p>Generates a standalone HTML file with embedded CSS for viewing test results in a browser.</p>"},{"location":"reference/api/reporting/#agentprobe.reporting.html.HTMLReporter","title":"<code>HTMLReporter</code>","text":"<p>Reporter that writes results as a standalone HTML file.</p> <p>Produces a single HTML file with embedded CSS, requiring no external dependencies for viewing.</p> <p>Attributes:</p> Name Type Description <code>output_dir</code> <p>Directory to write report files to.</p> Source code in <code>src/agentprobe/reporting/html.py</code> <pre><code>class HTMLReporter:\n    \"\"\"Reporter that writes results as a standalone HTML file.\n\n    Produces a single HTML file with embedded CSS, requiring no external\n    dependencies for viewing.\n\n    Attributes:\n        output_dir: Directory to write report files to.\n    \"\"\"\n\n    def __init__(self, output_dir: str | Path = \"agentprobe-report\") -&gt; None:\n        \"\"\"Initialize the HTML reporter.\n\n        Args:\n            output_dir: Directory for report output.\n        \"\"\"\n        self._output_dir = Path(output_dir)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the reporter name.\"\"\"\n        return \"html\"\n\n    async def report(self, run: AgentRun) -&gt; None:\n        \"\"\"Write the agent run as an HTML file.\n\n        Args:\n            run: The completed agent run.\n        \"\"\"\n        self._output_dir.mkdir(parents=True, exist_ok=True)\n        output_path = self._output_dir / f\"report-{run.run_id}.html\"\n\n        content = self._build_html(run)\n        output_path.write_text(content, encoding=\"utf-8\")\n        logger.info(\"HTML report written to %s\", output_path)\n\n    def _build_html(self, run: AgentRun) -&gt; str:\n        \"\"\"Build the complete HTML content.\n\n        Args:\n            run: The completed agent run.\n\n        Returns:\n            The full HTML string.\n        \"\"\"\n        rows = self._build_rows(run)\n        overall_color = \"#22c55e\" if run.failed == 0 and run.errors == 0 else \"#ef4444\"\n\n        return f\"\"\"&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n&lt;meta charset=\"utf-8\"&gt;\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n&lt;title&gt;AgentProbe Report \u2014 {html.escape(run.agent_name)}&lt;/title&gt;\n&lt;style&gt;\n  body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n         max-width: 960px; margin: 2rem auto; padding: 0 1rem; background: #fafafa; color: #1a1a1a; }}\n  h1 {{ margin-bottom: 0.5rem; }}\n  .summary {{ display: flex; gap: 1.5rem; margin: 1rem 0; padding: 1rem;\n              background: #fff; border-radius: 8px; border: 1px solid #e5e7eb; }}\n  .summary .stat {{ text-align: center; }}\n  .summary .stat .value {{ font-size: 1.5rem; font-weight: 700; }}\n  .summary .stat .label {{ font-size: 0.8rem; color: #6b7280; text-transform: uppercase; }}\n  table {{ width: 100%; border-collapse: collapse; background: #fff;\n           border-radius: 8px; overflow: hidden; border: 1px solid #e5e7eb; }}\n  th {{ background: #f9fafb; text-align: left; padding: 0.75rem 1rem;\n       font-size: 0.8rem; text-transform: uppercase; color: #6b7280; border-bottom: 1px solid #e5e7eb; }}\n  td {{ padding: 0.75rem 1rem; border-bottom: 1px solid #f3f4f6; }}\n  .status {{ padding: 2px 8px; border-radius: 4px; color: #fff; font-size: 0.8rem; font-weight: 600; }}\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;AgentProbe Test Report&lt;/h1&gt;\n&lt;p&gt;Agent: &lt;strong&gt;{html.escape(run.agent_name)}&lt;/strong&gt; \u2014\n   Status: &lt;span style=\"color:{overall_color};font-weight:700\"&gt;{html.escape(run.status.value)}&lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"summary\"&gt;\n  &lt;div class=\"stat\"&gt;&lt;div class=\"value\"&gt;{run.total_tests}&lt;/div&gt;&lt;div class=\"label\"&gt;Total&lt;/div&gt;&lt;/div&gt;\n  &lt;div class=\"stat\"&gt;&lt;div class=\"value\" style=\"color:#22c55e\"&gt;{run.passed}&lt;/div&gt;&lt;div class=\"label\"&gt;Passed&lt;/div&gt;&lt;/div&gt;\n  &lt;div class=\"stat\"&gt;&lt;div class=\"value\" style=\"color:#ef4444\"&gt;{run.failed}&lt;/div&gt;&lt;div class=\"label\"&gt;Failed&lt;/div&gt;&lt;/div&gt;\n  &lt;div class=\"stat\"&gt;&lt;div class=\"value\" style=\"color:#dc2626\"&gt;{run.errors}&lt;/div&gt;&lt;div class=\"label\"&gt;Errors&lt;/div&gt;&lt;/div&gt;\n  &lt;div class=\"stat\"&gt;&lt;div class=\"value\"&gt;{run.duration_ms}ms&lt;/div&gt;&lt;div class=\"label\"&gt;Duration&lt;/div&gt;&lt;/div&gt;\n&lt;/div&gt;\n&lt;table&gt;\n&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Test&lt;/th&gt;&lt;th&gt;Status&lt;/th&gt;&lt;th&gt;Score&lt;/th&gt;&lt;th&gt;Duration&lt;/th&gt;&lt;th&gt;Details&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;\n&lt;tbody&gt;\n{rows}\n&lt;/tbody&gt;\n&lt;/table&gt;\n&lt;/body&gt;\n&lt;/html&gt;\"\"\"\n\n    def _build_rows(self, run: AgentRun) -&gt; str:\n        \"\"\"Build HTML table rows for test results.\n\n        Args:\n            run: The completed agent run.\n\n        Returns:\n            HTML string containing all table rows.\n        \"\"\"\n        lines: list[str] = []\n        for result in run.test_results:\n            color = _STATUS_COLORS.get(result.status, \"#6b7280\")\n            details = \"\"\n            if result.error_message:\n                details = html.escape(result.error_message[:100])\n            elif result.eval_results:\n                verdicts = [r.verdict.value for r in result.eval_results]\n                details = html.escape(\", \".join(verdicts))\n\n            lines.append(\n                f\"&lt;tr&gt;&lt;td&gt;{html.escape(result.test_name)}&lt;/td&gt;\"\n                f'&lt;td&gt;&lt;span class=\"status\" style=\"background:{color}\"&gt;'\n                f\"{html.escape(result.status.value)}&lt;/span&gt;&lt;/td&gt;\"\n                f\"&lt;td&gt;{result.score:.2f}&lt;/td&gt;\"\n                f\"&lt;td&gt;{result.duration_ms}ms&lt;/td&gt;\"\n                f\"&lt;td&gt;{details}&lt;/td&gt;&lt;/tr&gt;\"\n            )\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/api/reporting/#agentprobe.reporting.html.HTMLReporter.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the reporter name.</p>"},{"location":"reference/api/reporting/#agentprobe.reporting.html.HTMLReporter.__init__","title":"<code>__init__(output_dir='agentprobe-report')</code>","text":"<p>Initialize the HTML reporter.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | Path</code> <p>Directory for report output.</p> <code>'agentprobe-report'</code> Source code in <code>src/agentprobe/reporting/html.py</code> <pre><code>def __init__(self, output_dir: str | Path = \"agentprobe-report\") -&gt; None:\n    \"\"\"Initialize the HTML reporter.\n\n    Args:\n        output_dir: Directory for report output.\n    \"\"\"\n    self._output_dir = Path(output_dir)\n</code></pre>"},{"location":"reference/api/reporting/#agentprobe.reporting.html.HTMLReporter.report","title":"<code>report(run)</code>  <code>async</code>","text":"<p>Write the agent run as an HTML file.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>AgentRun</code> <p>The completed agent run.</p> required Source code in <code>src/agentprobe/reporting/html.py</code> <pre><code>async def report(self, run: AgentRun) -&gt; None:\n    \"\"\"Write the agent run as an HTML file.\n\n    Args:\n        run: The completed agent run.\n    \"\"\"\n    self._output_dir.mkdir(parents=True, exist_ok=True)\n    output_path = self._output_dir / f\"report-{run.run_id}.html\"\n\n    content = self._build_html(run)\n    output_path.write_text(content, encoding=\"utf-8\")\n    logger.info(\"HTML report written to %s\", output_path)\n</code></pre>"},{"location":"reference/api/reporting/#json-reporter","title":"JSON Reporter","text":""},{"location":"reference/api/reporting/#agentprobe.reporting.json_reporter","title":"<code>agentprobe.reporting.json_reporter</code>","text":"<p>JSON file reporter for test results.</p> <p>Writes the complete AgentRun as a JSON file for consumption by CI/CD pipelines and other tools.</p>"},{"location":"reference/api/reporting/#agentprobe.reporting.json_reporter.JSONReporter","title":"<code>JSONReporter</code>","text":"<p>Reporter that writes results to a JSON file.</p> <p>Attributes:</p> Name Type Description <code>output_dir</code> <p>Directory to write report files to.</p> Source code in <code>src/agentprobe/reporting/json_reporter.py</code> <pre><code>class JSONReporter:\n    \"\"\"Reporter that writes results to a JSON file.\n\n    Attributes:\n        output_dir: Directory to write report files to.\n    \"\"\"\n\n    def __init__(self, output_dir: str | Path = \"agentprobe-report\") -&gt; None:\n        \"\"\"Initialize the JSON reporter.\n\n        Args:\n            output_dir: Directory for report output.\n        \"\"\"\n        self._output_dir = Path(output_dir)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the reporter name.\"\"\"\n        return \"json\"\n\n    async def report(self, run: AgentRun) -&gt; None:\n        \"\"\"Write the agent run as a JSON file.\n\n        Args:\n            run: The completed agent run.\n        \"\"\"\n        self._output_dir.mkdir(parents=True, exist_ok=True)\n        output_path = self._output_dir / f\"report-{run.run_id}.json\"\n\n        data = json.loads(run.model_dump_json())\n        output_path.write_text(\n            json.dumps(data, indent=2, ensure_ascii=False),\n            encoding=\"utf-8\",\n        )\n        logger.info(\"JSON report written to %s\", output_path)\n</code></pre>"},{"location":"reference/api/reporting/#agentprobe.reporting.json_reporter.JSONReporter.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the reporter name.</p>"},{"location":"reference/api/reporting/#agentprobe.reporting.json_reporter.JSONReporter.__init__","title":"<code>__init__(output_dir='agentprobe-report')</code>","text":"<p>Initialize the JSON reporter.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | Path</code> <p>Directory for report output.</p> <code>'agentprobe-report'</code> Source code in <code>src/agentprobe/reporting/json_reporter.py</code> <pre><code>def __init__(self, output_dir: str | Path = \"agentprobe-report\") -&gt; None:\n    \"\"\"Initialize the JSON reporter.\n\n    Args:\n        output_dir: Directory for report output.\n    \"\"\"\n    self._output_dir = Path(output_dir)\n</code></pre>"},{"location":"reference/api/reporting/#agentprobe.reporting.json_reporter.JSONReporter.report","title":"<code>report(run)</code>  <code>async</code>","text":"<p>Write the agent run as a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>AgentRun</code> <p>The completed agent run.</p> required Source code in <code>src/agentprobe/reporting/json_reporter.py</code> <pre><code>async def report(self, run: AgentRun) -&gt; None:\n    \"\"\"Write the agent run as a JSON file.\n\n    Args:\n        run: The completed agent run.\n    \"\"\"\n    self._output_dir.mkdir(parents=True, exist_ok=True)\n    output_path = self._output_dir / f\"report-{run.run_id}.json\"\n\n    data = json.loads(run.model_dump_json())\n    output_path.write_text(\n        json.dumps(data, indent=2, ensure_ascii=False),\n        encoding=\"utf-8\",\n    )\n    logger.info(\"JSON report written to %s\", output_path)\n</code></pre>"},{"location":"reference/api/reporting/#junit-xml-reporter","title":"JUnit XML Reporter","text":""},{"location":"reference/api/reporting/#agentprobe.reporting.junit","title":"<code>agentprobe.reporting.junit</code>","text":"<p>JUnit XML reporter for test results.</p> <p>Generates JUnit-compatible XML output suitable for CI/CD systems like Jenkins, GitHub Actions, and GitLab CI.</p>"},{"location":"reference/api/reporting/#agentprobe.reporting.junit.JUnitReporter","title":"<code>JUnitReporter</code>","text":"<p>Reporter that writes results as JUnit XML.</p> <p>Produces a standard JUnit XML file that can be consumed by CI/CD pipelines for test reporting and status visualization.</p> <p>Attributes:</p> Name Type Description <code>output_dir</code> <p>Directory to write report files to.</p> Source code in <code>src/agentprobe/reporting/junit.py</code> <pre><code>class JUnitReporter:\n    \"\"\"Reporter that writes results as JUnit XML.\n\n    Produces a standard JUnit XML file that can be consumed by CI/CD\n    pipelines for test reporting and status visualization.\n\n    Attributes:\n        output_dir: Directory to write report files to.\n    \"\"\"\n\n    def __init__(self, output_dir: str | Path = \"agentprobe-report\") -&gt; None:\n        \"\"\"Initialize the JUnit reporter.\n\n        Args:\n            output_dir: Directory for report output.\n        \"\"\"\n        self._output_dir = Path(output_dir)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the reporter name.\"\"\"\n        return \"junit\"\n\n    async def report(self, run: AgentRun) -&gt; None:\n        \"\"\"Write the agent run as a JUnit XML file.\n\n        Args:\n            run: The completed agent run.\n        \"\"\"\n        self._output_dir.mkdir(parents=True, exist_ok=True)\n        output_path = self._output_dir / f\"report-{run.run_id}.xml\"\n\n        root = self._build_xml(run)\n        tree = ET.ElementTree(root)\n        ET.indent(tree, space=\"  \")\n        tree.write(str(output_path), encoding=\"unicode\", xml_declaration=True)\n        logger.info(\"JUnit XML report written to %s\", output_path)\n\n    def _build_xml(self, run: AgentRun) -&gt; ET.Element:\n        \"\"\"Build the JUnit XML element tree.\n\n        Args:\n            run: The completed agent run.\n\n        Returns:\n            The root XML element.\n        \"\"\"\n        testsuite = ET.Element(\"testsuite\")\n        testsuite.set(\"name\", run.agent_name)\n        testsuite.set(\"tests\", str(run.total_tests))\n        testsuite.set(\"failures\", str(run.failed))\n        testsuite.set(\"errors\", str(run.errors))\n        testsuite.set(\"skipped\", str(run.skipped))\n        testsuite.set(\"time\", f\"{run.duration_ms / 1000:.3f}\")\n\n        for result in run.test_results:\n            testcase = self._build_testcase(result, run.agent_name)\n            testsuite.append(testcase)\n\n        return testsuite\n\n    def _build_testcase(self, result: TestResult, suite_name: str) -&gt; ET.Element:\n        \"\"\"Build a testcase XML element.\n\n        Args:\n            result: A single test result.\n            suite_name: The parent test suite name.\n\n        Returns:\n            A testcase XML element.\n        \"\"\"\n        testcase = ET.Element(\"testcase\")\n        testcase.set(\"name\", result.test_name)\n        testcase.set(\"classname\", suite_name)\n        testcase.set(\"time\", f\"{result.duration_ms / 1000:.3f}\")\n\n        if result.status == TestStatus.FAILED:\n            failure = ET.SubElement(testcase, \"failure\")\n            failure.set(\"message\", result.error_message or \"Test failed\")\n            failure.set(\"type\", \"AssertionError\")\n            if result.eval_results:\n                failure.text = \"\\n\".join(\n                    f\"{er.evaluator_name}: {er.verdict.value} ({er.score:.2f}) - {er.reason}\"\n                    for er in result.eval_results\n                )\n\n        elif result.status == TestStatus.ERROR:\n            error = ET.SubElement(testcase, \"error\")\n            error.set(\"message\", result.error_message or \"Test error\")\n            error.set(\"type\", \"RuntimeError\")\n\n        elif result.status == TestStatus.SKIPPED:\n            skipped = ET.SubElement(testcase, \"skipped\")\n            skipped.set(\"message\", result.error_message or \"Test skipped\")\n\n        return testcase\n</code></pre>"},{"location":"reference/api/reporting/#agentprobe.reporting.junit.JUnitReporter.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the reporter name.</p>"},{"location":"reference/api/reporting/#agentprobe.reporting.junit.JUnitReporter.__init__","title":"<code>__init__(output_dir='agentprobe-report')</code>","text":"<p>Initialize the JUnit reporter.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | Path</code> <p>Directory for report output.</p> <code>'agentprobe-report'</code> Source code in <code>src/agentprobe/reporting/junit.py</code> <pre><code>def __init__(self, output_dir: str | Path = \"agentprobe-report\") -&gt; None:\n    \"\"\"Initialize the JUnit reporter.\n\n    Args:\n        output_dir: Directory for report output.\n    \"\"\"\n    self._output_dir = Path(output_dir)\n</code></pre>"},{"location":"reference/api/reporting/#agentprobe.reporting.junit.JUnitReporter.report","title":"<code>report(run)</code>  <code>async</code>","text":"<p>Write the agent run as a JUnit XML file.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>AgentRun</code> <p>The completed agent run.</p> required Source code in <code>src/agentprobe/reporting/junit.py</code> <pre><code>async def report(self, run: AgentRun) -&gt; None:\n    \"\"\"Write the agent run as a JUnit XML file.\n\n    Args:\n        run: The completed agent run.\n    \"\"\"\n    self._output_dir.mkdir(parents=True, exist_ok=True)\n    output_path = self._output_dir / f\"report-{run.run_id}.xml\"\n\n    root = self._build_xml(run)\n    tree = ET.ElementTree(root)\n    ET.indent(tree, space=\"  \")\n    tree.write(str(output_path), encoding=\"unicode\", xml_declaration=True)\n    logger.info(\"JUnit XML report written to %s\", output_path)\n</code></pre>"},{"location":"reference/api/reporting/#markdown-reporter","title":"Markdown Reporter","text":""},{"location":"reference/api/reporting/#agentprobe.reporting.markdown","title":"<code>agentprobe.reporting.markdown</code>","text":"<p>Markdown reporter for test results.</p> <p>Generates a Markdown file with tables and summary sections suitable for rendering in GitHub, GitLab, or documentation systems.</p>"},{"location":"reference/api/reporting/#agentprobe.reporting.markdown.MarkdownReporter","title":"<code>MarkdownReporter</code>","text":"<p>Reporter that writes results as a Markdown file.</p> <p>Produces a Markdown document with a summary section and a results table, compatible with GitHub, GitLab, and other renderers.</p> <p>Attributes:</p> Name Type Description <code>output_dir</code> <p>Directory to write report files to.</p> Source code in <code>src/agentprobe/reporting/markdown.py</code> <pre><code>class MarkdownReporter:\n    \"\"\"Reporter that writes results as a Markdown file.\n\n    Produces a Markdown document with a summary section and a results\n    table, compatible with GitHub, GitLab, and other renderers.\n\n    Attributes:\n        output_dir: Directory to write report files to.\n    \"\"\"\n\n    def __init__(self, output_dir: str | Path = \"agentprobe-report\") -&gt; None:\n        \"\"\"Initialize the Markdown reporter.\n\n        Args:\n            output_dir: Directory for report output.\n        \"\"\"\n        self._output_dir = Path(output_dir)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the reporter name.\"\"\"\n        return \"markdown\"\n\n    async def report(self, run: AgentRun) -&gt; None:\n        \"\"\"Write the agent run as a Markdown file.\n\n        Args:\n            run: The completed agent run.\n        \"\"\"\n        self._output_dir.mkdir(parents=True, exist_ok=True)\n        output_path = self._output_dir / f\"report-{run.run_id}.md\"\n\n        content = self._build_markdown(run)\n        output_path.write_text(content, encoding=\"utf-8\")\n        logger.info(\"Markdown report written to %s\", output_path)\n\n    def _build_markdown(self, run: AgentRun) -&gt; str:\n        \"\"\"Build the complete Markdown content.\n\n        Args:\n            run: The completed agent run.\n\n        Returns:\n            The full Markdown string.\n        \"\"\"\n        lines: list[str] = []\n\n        lines.append(f\"# AgentProbe Test Report \u2014 {run.agent_name}\")\n        lines.append(\"\")\n        lines.append(f\"**Status:** {run.status.value}\")\n        lines.append(\"\")\n\n        # Summary\n        lines.append(\"## Summary\")\n        lines.append(\"\")\n        lines.append(\"| Metric | Value |\")\n        lines.append(\"|--------|-------|\")\n        lines.append(f\"| Total Tests | {run.total_tests} |\")\n        lines.append(f\"| Passed | {run.passed} |\")\n        lines.append(f\"| Failed | {run.failed} |\")\n        lines.append(f\"| Errors | {run.errors} |\")\n        lines.append(f\"| Skipped | {run.skipped} |\")\n        lines.append(f\"| Duration | {run.duration_ms}ms |\")\n        lines.append(\"\")\n\n        # Results table\n        if run.test_results:\n            lines.append(\"## Results\")\n            lines.append(\"\")\n            lines.append(\"| Test | Status | Score | Duration | Details |\")\n            lines.append(\"|------|--------|-------|----------|---------|\")\n\n            for result in run.test_results:\n                status_label = _STATUS_EMOJI.get(result.status.value, result.status.value)\n                details = \"\"\n                if result.error_message:\n                    details = result.error_message[:80]\n                elif result.eval_results:\n                    verdicts = [r.verdict.value for r in result.eval_results]\n                    details = \", \".join(verdicts)\n\n                lines.append(\n                    f\"| {result.test_name} | {status_label} | {result.score:.2f} \"\n                    f\"| {result.duration_ms}ms | {details} |\"\n                )\n            lines.append(\"\")\n\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/api/reporting/#agentprobe.reporting.markdown.MarkdownReporter.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the reporter name.</p>"},{"location":"reference/api/reporting/#agentprobe.reporting.markdown.MarkdownReporter.__init__","title":"<code>__init__(output_dir='agentprobe-report')</code>","text":"<p>Initialize the Markdown reporter.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | Path</code> <p>Directory for report output.</p> <code>'agentprobe-report'</code> Source code in <code>src/agentprobe/reporting/markdown.py</code> <pre><code>def __init__(self, output_dir: str | Path = \"agentprobe-report\") -&gt; None:\n    \"\"\"Initialize the Markdown reporter.\n\n    Args:\n        output_dir: Directory for report output.\n    \"\"\"\n    self._output_dir = Path(output_dir)\n</code></pre>"},{"location":"reference/api/reporting/#agentprobe.reporting.markdown.MarkdownReporter.report","title":"<code>report(run)</code>  <code>async</code>","text":"<p>Write the agent run as a Markdown file.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>AgentRun</code> <p>The completed agent run.</p> required Source code in <code>src/agentprobe/reporting/markdown.py</code> <pre><code>async def report(self, run: AgentRun) -&gt; None:\n    \"\"\"Write the agent run as a Markdown file.\n\n    Args:\n        run: The completed agent run.\n    \"\"\"\n    self._output_dir.mkdir(parents=True, exist_ok=True)\n    output_path = self._output_dir / f\"report-{run.run_id}.md\"\n\n    content = self._build_markdown(run)\n    output_path.write_text(content, encoding=\"utf-8\")\n    logger.info(\"Markdown report written to %s\", output_path)\n</code></pre>"},{"location":"reference/api/reporting/#csv-reporter","title":"CSV Reporter","text":""},{"location":"reference/api/reporting/#agentprobe.reporting.csv_reporter","title":"<code>agentprobe.reporting.csv_reporter</code>","text":"<p>CSV reporter for test results.</p> <p>Writes test results as a CSV file for easy import into spreadsheets and data analysis tools.</p>"},{"location":"reference/api/reporting/#agentprobe.reporting.csv_reporter.CSVReporter","title":"<code>CSVReporter</code>","text":"<p>Reporter that writes results as a CSV file.</p> <p>Produces a CSV file with one row per test result, suitable for import into spreadsheets and data analysis pipelines.</p> <p>Attributes:</p> Name Type Description <code>output_dir</code> <p>Directory to write report files to.</p> Source code in <code>src/agentprobe/reporting/csv_reporter.py</code> <pre><code>class CSVReporter:\n    \"\"\"Reporter that writes results as a CSV file.\n\n    Produces a CSV file with one row per test result, suitable for\n    import into spreadsheets and data analysis pipelines.\n\n    Attributes:\n        output_dir: Directory to write report files to.\n    \"\"\"\n\n    def __init__(self, output_dir: str | Path = \"agentprobe-report\") -&gt; None:\n        \"\"\"Initialize the CSV reporter.\n\n        Args:\n            output_dir: Directory for report output.\n        \"\"\"\n        self._output_dir = Path(output_dir)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the reporter name.\"\"\"\n        return \"csv\"\n\n    async def report(self, run: AgentRun) -&gt; None:\n        \"\"\"Write the agent run as a CSV file.\n\n        Args:\n            run: The completed agent run.\n        \"\"\"\n        self._output_dir.mkdir(parents=True, exist_ok=True)\n        output_path = self._output_dir / f\"report-{run.run_id}.csv\"\n\n        content = self._build_csv(run)\n        output_path.write_text(content, encoding=\"utf-8\")\n        logger.info(\"CSV report written to %s\", output_path)\n\n    def _build_csv(self, run: AgentRun) -&gt; str:\n        \"\"\"Build the CSV content as a string.\n\n        Args:\n            run: The completed agent run.\n\n        Returns:\n            The complete CSV string with headers and data rows.\n        \"\"\"\n        output = io.StringIO(newline=\"\")\n        writer = csv.writer(output)\n        writer.writerow(_CSV_HEADERS)\n\n        for result in run.test_results:\n            eval_verdicts = \"\"\n            if result.eval_results:\n                verdicts = [r.verdict.value for r in result.eval_results]\n                eval_verdicts = \"; \".join(verdicts)\n\n            writer.writerow(\n                [\n                    result.test_name,\n                    result.status.value,\n                    f\"{result.score:.4f}\",\n                    str(result.duration_ms),\n                    result.error_message or \"\",\n                    eval_verdicts,\n                ]\n            )\n\n        return output.getvalue()\n</code></pre>"},{"location":"reference/api/reporting/#agentprobe.reporting.csv_reporter.CSVReporter.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return the reporter name.</p>"},{"location":"reference/api/reporting/#agentprobe.reporting.csv_reporter.CSVReporter.__init__","title":"<code>__init__(output_dir='agentprobe-report')</code>","text":"<p>Initialize the CSV reporter.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | Path</code> <p>Directory for report output.</p> <code>'agentprobe-report'</code> Source code in <code>src/agentprobe/reporting/csv_reporter.py</code> <pre><code>def __init__(self, output_dir: str | Path = \"agentprobe-report\") -&gt; None:\n    \"\"\"Initialize the CSV reporter.\n\n    Args:\n        output_dir: Directory for report output.\n    \"\"\"\n    self._output_dir = Path(output_dir)\n</code></pre>"},{"location":"reference/api/reporting/#agentprobe.reporting.csv_reporter.CSVReporter.report","title":"<code>report(run)</code>  <code>async</code>","text":"<p>Write the agent run as a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>AgentRun</code> <p>The completed agent run.</p> required Source code in <code>src/agentprobe/reporting/csv_reporter.py</code> <pre><code>async def report(self, run: AgentRun) -&gt; None:\n    \"\"\"Write the agent run as a CSV file.\n\n    Args:\n        run: The completed agent run.\n    \"\"\"\n    self._output_dir.mkdir(parents=True, exist_ok=True)\n    output_path = self._output_dir / f\"report-{run.run_id}.csv\"\n\n    content = self._build_csv(run)\n    output_path.write_text(content, encoding=\"utf-8\")\n    logger.info(\"CSV report written to %s\", output_path)\n</code></pre>"},{"location":"reference/api/safety/","title":"Safety","text":"<p>Safety scanning engine and test suites.</p>"},{"location":"reference/api/safety/#scanner","title":"Scanner","text":""},{"location":"reference/api/safety/#agentprobe.safety.scanner","title":"<code>agentprobe.safety.scanner</code>","text":"<p>Safety scanner engine: runs safety test suites against agents.</p> <p>Provides the SafetyScanner orchestrator and base SafetySuite ABC for implementing specific safety test suites (prompt injection, data leakage, etc.).</p>"},{"location":"reference/api/safety/#agentprobe.safety.scanner.SafetySuiteResult","title":"<code>SafetySuiteResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result from a single safety suite execution.</p> <p>Attributes:</p> Name Type Description <code>suite_name</code> <code>str</code> <p>Name of the safety suite.</p> <code>total_tests</code> <code>int</code> <p>Number of tests in the suite.</p> <code>passed</code> <code>int</code> <p>Number of tests that passed (no safety violation).</p> <code>failed</code> <code>int</code> <p>Number of tests that detected a safety issue.</p> <code>results</code> <code>tuple[dict[str, Any], ...]</code> <p>Detailed per-test results.</p> Source code in <code>src/agentprobe/safety/scanner.py</code> <pre><code>class SafetySuiteResult(BaseModel):\n    \"\"\"Result from a single safety suite execution.\n\n    Attributes:\n        suite_name: Name of the safety suite.\n        total_tests: Number of tests in the suite.\n        passed: Number of tests that passed (no safety violation).\n        failed: Number of tests that detected a safety issue.\n        results: Detailed per-test results.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    suite_name: str\n    total_tests: int = Field(default=0, ge=0)\n    passed: int = Field(default=0, ge=0)\n    failed: int = Field(default=0, ge=0)\n    results: tuple[dict[str, Any], ...] = ()\n</code></pre>"},{"location":"reference/api/safety/#agentprobe.safety.scanner.SafetyScanResult","title":"<code>SafetyScanResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Aggregate result from all safety suites.</p> <p>Attributes:</p> Name Type Description <code>total_suites</code> <code>int</code> <p>Number of suites executed.</p> <code>total_tests</code> <code>int</code> <p>Total tests across all suites.</p> <code>total_passed</code> <code>int</code> <p>Tests that passed across all suites.</p> <code>total_failed</code> <code>int</code> <p>Tests that failed across all suites.</p> <code>suite_results</code> <code>tuple[SafetySuiteResult, ...]</code> <p>Per-suite results.</p> Source code in <code>src/agentprobe/safety/scanner.py</code> <pre><code>class SafetyScanResult(BaseModel):\n    \"\"\"Aggregate result from all safety suites.\n\n    Attributes:\n        total_suites: Number of suites executed.\n        total_tests: Total tests across all suites.\n        total_passed: Tests that passed across all suites.\n        total_failed: Tests that failed across all suites.\n        suite_results: Per-suite results.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    total_suites: int = Field(default=0, ge=0)\n    total_tests: int = Field(default=0, ge=0)\n    total_passed: int = Field(default=0, ge=0)\n    total_failed: int = Field(default=0, ge=0)\n    suite_results: tuple[SafetySuiteResult, ...] = ()\n</code></pre>"},{"location":"reference/api/safety/#agentprobe.safety.scanner.SafetySuite","title":"<code>SafetySuite</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for safety test suites.</p> <p>Each suite implements a specific category of safety testing (e.g. prompt injection, data leakage, bias detection).</p> Source code in <code>src/agentprobe/safety/scanner.py</code> <pre><code>class SafetySuite(ABC):\n    \"\"\"Abstract base class for safety test suites.\n\n    Each suite implements a specific category of safety testing\n    (e.g. prompt injection, data leakage, bias detection).\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Return the suite name.\"\"\"\n        ...\n\n    @abstractmethod\n    async def run(self, adapter: AdapterProtocol) -&gt; SafetySuiteResult:\n        \"\"\"Execute all tests in this suite against an adapter.\n\n        Args:\n            adapter: The agent adapter to test.\n\n        Returns:\n            Results from the suite execution.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/api/safety/#agentprobe.safety.scanner.SafetySuite.name","title":"<code>name</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Return the suite name.</p>"},{"location":"reference/api/safety/#agentprobe.safety.scanner.SafetySuite.run","title":"<code>run(adapter)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Execute all tests in this suite against an adapter.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>AdapterProtocol</code> <p>The agent adapter to test.</p> required <p>Returns:</p> Type Description <code>SafetySuiteResult</code> <p>Results from the suite execution.</p> Source code in <code>src/agentprobe/safety/scanner.py</code> <pre><code>@abstractmethod\nasync def run(self, adapter: AdapterProtocol) -&gt; SafetySuiteResult:\n    \"\"\"Execute all tests in this suite against an adapter.\n\n    Args:\n        adapter: The agent adapter to test.\n\n    Returns:\n        Results from the suite execution.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/safety/#agentprobe.safety.scanner.SafetyScanner","title":"<code>SafetyScanner</code>","text":"<p>Orchestrates safety testing by running configured suites.</p> <p>Attributes:</p> Name Type Description <code>suites</code> <p>List of safety suite instances to run.</p> Source code in <code>src/agentprobe/safety/scanner.py</code> <pre><code>class SafetyScanner:\n    \"\"\"Orchestrates safety testing by running configured suites.\n\n    Attributes:\n        suites: List of safety suite instances to run.\n    \"\"\"\n\n    def __init__(self, suites: list[SafetySuite] | None = None) -&gt; None:\n        \"\"\"Initialize the safety scanner.\n\n        Args:\n            suites: Safety suites to run. If None, uses an empty list.\n        \"\"\"\n        self._suites = suites or []\n\n    @classmethod\n    def from_config(cls, suite_names: list[str]) -&gt; SafetyScanner:\n        \"\"\"Create a scanner from a list of suite names.\n\n        Looks up suite classes in the global registry.\n\n        Args:\n            suite_names: Names of suites to instantiate.\n\n        Returns:\n            A configured SafetyScanner.\n        \"\"\"\n        suites: list[SafetySuite] = []\n        for name in suite_names:\n            suite_class = _suite_registry.get(name)\n            if suite_class is None:\n                logger.warning(\"Unknown safety suite: %s\", name)\n                continue\n            suites.append(suite_class())\n        return cls(suites=suites)\n\n    async def scan(self, adapter: AdapterProtocol) -&gt; SafetyScanResult:\n        \"\"\"Run all configured safety suites against an adapter.\n\n        Args:\n            adapter: The agent adapter to test.\n\n        Returns:\n            Aggregate scan results.\n        \"\"\"\n        suite_results: list[SafetySuiteResult] = []\n\n        for suite in self._suites:\n            try:\n                result = await suite.run(adapter)\n                suite_results.append(result)\n            except Exception:\n                logger.exception(\"Safety suite '%s' failed\", suite.name)\n                suite_results.append(\n                    SafetySuiteResult(\n                        suite_name=suite.name,\n                        total_tests=0,\n                        passed=0,\n                        failed=0,\n                    )\n                )\n\n        total_tests = sum(r.total_tests for r in suite_results)\n        total_passed = sum(r.passed for r in suite_results)\n        total_failed = sum(r.failed for r in suite_results)\n\n        return SafetyScanResult(\n            total_suites=len(suite_results),\n            total_tests=total_tests,\n            total_passed=total_passed,\n            total_failed=total_failed,\n            suite_results=tuple(suite_results),\n        )\n</code></pre>"},{"location":"reference/api/safety/#agentprobe.safety.scanner.SafetyScanner.__init__","title":"<code>__init__(suites=None)</code>","text":"<p>Initialize the safety scanner.</p> <p>Parameters:</p> Name Type Description Default <code>suites</code> <code>list[SafetySuite] | None</code> <p>Safety suites to run. If None, uses an empty list.</p> <code>None</code> Source code in <code>src/agentprobe/safety/scanner.py</code> <pre><code>def __init__(self, suites: list[SafetySuite] | None = None) -&gt; None:\n    \"\"\"Initialize the safety scanner.\n\n    Args:\n        suites: Safety suites to run. If None, uses an empty list.\n    \"\"\"\n    self._suites = suites or []\n</code></pre>"},{"location":"reference/api/safety/#agentprobe.safety.scanner.SafetyScanner.from_config","title":"<code>from_config(suite_names)</code>  <code>classmethod</code>","text":"<p>Create a scanner from a list of suite names.</p> <p>Looks up suite classes in the global registry.</p> <p>Parameters:</p> Name Type Description Default <code>suite_names</code> <code>list[str]</code> <p>Names of suites to instantiate.</p> required <p>Returns:</p> Type Description <code>SafetyScanner</code> <p>A configured SafetyScanner.</p> Source code in <code>src/agentprobe/safety/scanner.py</code> <pre><code>@classmethod\ndef from_config(cls, suite_names: list[str]) -&gt; SafetyScanner:\n    \"\"\"Create a scanner from a list of suite names.\n\n    Looks up suite classes in the global registry.\n\n    Args:\n        suite_names: Names of suites to instantiate.\n\n    Returns:\n        A configured SafetyScanner.\n    \"\"\"\n    suites: list[SafetySuite] = []\n    for name in suite_names:\n        suite_class = _suite_registry.get(name)\n        if suite_class is None:\n            logger.warning(\"Unknown safety suite: %s\", name)\n            continue\n        suites.append(suite_class())\n    return cls(suites=suites)\n</code></pre>"},{"location":"reference/api/safety/#agentprobe.safety.scanner.SafetyScanner.scan","title":"<code>scan(adapter)</code>  <code>async</code>","text":"<p>Run all configured safety suites against an adapter.</p> <p>Parameters:</p> Name Type Description Default <code>adapter</code> <code>AdapterProtocol</code> <p>The agent adapter to test.</p> required <p>Returns:</p> Type Description <code>SafetyScanResult</code> <p>Aggregate scan results.</p> Source code in <code>src/agentprobe/safety/scanner.py</code> <pre><code>async def scan(self, adapter: AdapterProtocol) -&gt; SafetyScanResult:\n    \"\"\"Run all configured safety suites against an adapter.\n\n    Args:\n        adapter: The agent adapter to test.\n\n    Returns:\n        Aggregate scan results.\n    \"\"\"\n    suite_results: list[SafetySuiteResult] = []\n\n    for suite in self._suites:\n        try:\n            result = await suite.run(adapter)\n            suite_results.append(result)\n        except Exception:\n            logger.exception(\"Safety suite '%s' failed\", suite.name)\n            suite_results.append(\n                SafetySuiteResult(\n                    suite_name=suite.name,\n                    total_tests=0,\n                    passed=0,\n                    failed=0,\n                )\n            )\n\n    total_tests = sum(r.total_tests for r in suite_results)\n    total_passed = sum(r.passed for r in suite_results)\n    total_failed = sum(r.failed for r in suite_results)\n\n    return SafetyScanResult(\n        total_suites=len(suite_results),\n        total_tests=total_tests,\n        total_passed=total_passed,\n        total_failed=total_failed,\n        suite_results=tuple(suite_results),\n    )\n</code></pre>"},{"location":"reference/api/safety/#agentprobe.safety.scanner.register_suite","title":"<code>register_suite(suite_class)</code>","text":"<p>Register a safety suite class in the global registry.</p> <p>Parameters:</p> Name Type Description Default <code>suite_class</code> <code>type[SafetySuite]</code> <p>The suite class to register.</p> required <p>Returns:</p> Type Description <code>type[SafetySuite]</code> <p>The same class (for use as a decorator).</p> Source code in <code>src/agentprobe/safety/scanner.py</code> <pre><code>def register_suite(suite_class: type[SafetySuite]) -&gt; type[SafetySuite]:\n    \"\"\"Register a safety suite class in the global registry.\n\n    Args:\n        suite_class: The suite class to register.\n\n    Returns:\n        The same class (for use as a decorator).\n    \"\"\"\n    instance = suite_class()\n    _suite_registry[instance.name] = suite_class\n    return suite_class\n</code></pre>"},{"location":"reference/api/safety/#agentprobe.safety.scanner.get_registered_suites","title":"<code>get_registered_suites()</code>","text":"<p>Return all registered safety suite classes.</p> Source code in <code>src/agentprobe/safety/scanner.py</code> <pre><code>def get_registered_suites() -&gt; dict[str, type[SafetySuite]]:\n    \"\"\"Return all registered safety suite classes.\"\"\"\n    return dict(_suite_registry)\n</code></pre>"},{"location":"reference/api/security/","title":"Security","text":"<p>PII detection and redaction utilities.</p>"},{"location":"reference/api/security/#pii-scanner","title":"PII Scanner","text":""},{"location":"reference/api/security/#agentprobe.security.pii","title":"<code>agentprobe.security.pii</code>","text":"<p>PII detection and redaction utilities.</p> <p>Provides pattern-based scanning for common PII types (email, phone, SSN, credit card, IP address) with scan and redact operations.</p>"},{"location":"reference/api/security/#agentprobe.security.pii.PIIMatch","title":"<code>PIIMatch</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single PII detection match.</p> <p>Attributes:</p> Name Type Description <code>pii_type</code> <code>str</code> <p>Category of PII detected.</p> <code>value</code> <code>str</code> <p>The matched text.</p> <code>start</code> <code>int</code> <p>Start index in the source text.</p> <code>end</code> <code>int</code> <p>End index in the source text.</p> Source code in <code>src/agentprobe/security/pii.py</code> <pre><code>class PIIMatch(BaseModel):\n    \"\"\"A single PII detection match.\n\n    Attributes:\n        pii_type: Category of PII detected.\n        value: The matched text.\n        start: Start index in the source text.\n        end: End index in the source text.\n    \"\"\"\n\n    model_config = ConfigDict(strict=True, frozen=True, extra=\"forbid\")\n\n    pii_type: str\n    value: str\n    start: int = Field(ge=0)\n    end: int = Field(ge=0)\n</code></pre>"},{"location":"reference/api/security/#agentprobe.security.pii.PIIRedactor","title":"<code>PIIRedactor</code>","text":"<p>Scans text for PII and optionally redacts matches.</p> <p>Supports configurable PII types and custom patterns.</p> <p>Attributes:</p> Name Type Description <code>enabled_types</code> <p>Set of PII type names to detect.</p> Source code in <code>src/agentprobe/security/pii.py</code> <pre><code>class PIIRedactor:\n    \"\"\"Scans text for PII and optionally redacts matches.\n\n    Supports configurable PII types and custom patterns.\n\n    Attributes:\n        enabled_types: Set of PII type names to detect.\n    \"\"\"\n\n    def __init__(\n        self,\n        enabled_types: set[str] | None = None,\n        custom_patterns: dict[str, re.Pattern[str]] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the PII redactor.\n\n        Args:\n            enabled_types: PII types to enable. None enables all built-in types.\n            custom_patterns: Additional named patterns to check.\n        \"\"\"\n        self._patterns: dict[str, re.Pattern[str]] = {}\n\n        if enabled_types is None:\n            self._patterns.update(_PII_PATTERNS)\n        else:\n            for pii_type in enabled_types:\n                if pii_type in _PII_PATTERNS:\n                    self._patterns[pii_type] = _PII_PATTERNS[pii_type]\n\n        if custom_patterns:\n            self._patterns.update(custom_patterns)\n\n    def scan(self, text: str) -&gt; list[PIIMatch]:\n        \"\"\"Scan text for PII matches.\n\n        Args:\n            text: The text to scan.\n\n        Returns:\n            List of PII matches found, sorted by position.\n        \"\"\"\n        matches: list[PIIMatch] = []\n\n        for pii_type, pattern in self._patterns.items():\n            matches.extend(\n                PIIMatch(\n                    pii_type=pii_type,\n                    value=match.group(),\n                    start=match.start(),\n                    end=match.end(),\n                )\n                for match in pattern.finditer(text)\n            )\n\n        matches.sort(key=lambda m: m.start)\n        return matches\n\n    def redact(self, text: str) -&gt; str:\n        \"\"\"Redact all detected PII from text.\n\n        Replaces each match with a type-specific label (e.g. [EMAIL]).\n\n        Args:\n            text: The text to redact.\n\n        Returns:\n            Text with PII replaced by labels.\n        \"\"\"\n        matches = self.scan(text)\n        if not matches:\n            return text\n\n        # Process matches in reverse order to preserve indices\n        result = text\n        for match in reversed(matches):\n            label = _REDACTION_LABELS.get(match.pii_type, f\"[{match.pii_type.upper()}]\")\n            result = result[: match.start] + label + result[match.end :]\n\n        return result\n\n    def has_pii(self, text: str) -&gt; bool:\n        \"\"\"Check if text contains any detectable PII.\n\n        Args:\n            text: The text to check.\n\n        Returns:\n            True if PII was detected.\n        \"\"\"\n        return len(self.scan(text)) &gt; 0\n</code></pre>"},{"location":"reference/api/security/#agentprobe.security.pii.PIIRedactor.__init__","title":"<code>__init__(enabled_types=None, custom_patterns=None)</code>","text":"<p>Initialize the PII redactor.</p> <p>Parameters:</p> Name Type Description Default <code>enabled_types</code> <code>set[str] | None</code> <p>PII types to enable. None enables all built-in types.</p> <code>None</code> <code>custom_patterns</code> <code>dict[str, Pattern[str]] | None</code> <p>Additional named patterns to check.</p> <code>None</code> Source code in <code>src/agentprobe/security/pii.py</code> <pre><code>def __init__(\n    self,\n    enabled_types: set[str] | None = None,\n    custom_patterns: dict[str, re.Pattern[str]] | None = None,\n) -&gt; None:\n    \"\"\"Initialize the PII redactor.\n\n    Args:\n        enabled_types: PII types to enable. None enables all built-in types.\n        custom_patterns: Additional named patterns to check.\n    \"\"\"\n    self._patterns: dict[str, re.Pattern[str]] = {}\n\n    if enabled_types is None:\n        self._patterns.update(_PII_PATTERNS)\n    else:\n        for pii_type in enabled_types:\n            if pii_type in _PII_PATTERNS:\n                self._patterns[pii_type] = _PII_PATTERNS[pii_type]\n\n    if custom_patterns:\n        self._patterns.update(custom_patterns)\n</code></pre>"},{"location":"reference/api/security/#agentprobe.security.pii.PIIRedactor.scan","title":"<code>scan(text)</code>","text":"<p>Scan text for PII matches.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to scan.</p> required <p>Returns:</p> Type Description <code>list[PIIMatch]</code> <p>List of PII matches found, sorted by position.</p> Source code in <code>src/agentprobe/security/pii.py</code> <pre><code>def scan(self, text: str) -&gt; list[PIIMatch]:\n    \"\"\"Scan text for PII matches.\n\n    Args:\n        text: The text to scan.\n\n    Returns:\n        List of PII matches found, sorted by position.\n    \"\"\"\n    matches: list[PIIMatch] = []\n\n    for pii_type, pattern in self._patterns.items():\n        matches.extend(\n            PIIMatch(\n                pii_type=pii_type,\n                value=match.group(),\n                start=match.start(),\n                end=match.end(),\n            )\n            for match in pattern.finditer(text)\n        )\n\n    matches.sort(key=lambda m: m.start)\n    return matches\n</code></pre>"},{"location":"reference/api/security/#agentprobe.security.pii.PIIRedactor.redact","title":"<code>redact(text)</code>","text":"<p>Redact all detected PII from text.</p> <p>Replaces each match with a type-specific label (e.g. [EMAIL]).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to redact.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Text with PII replaced by labels.</p> Source code in <code>src/agentprobe/security/pii.py</code> <pre><code>def redact(self, text: str) -&gt; str:\n    \"\"\"Redact all detected PII from text.\n\n    Replaces each match with a type-specific label (e.g. [EMAIL]).\n\n    Args:\n        text: The text to redact.\n\n    Returns:\n        Text with PII replaced by labels.\n    \"\"\"\n    matches = self.scan(text)\n    if not matches:\n        return text\n\n    # Process matches in reverse order to preserve indices\n    result = text\n    for match in reversed(matches):\n        label = _REDACTION_LABELS.get(match.pii_type, f\"[{match.pii_type.upper()}]\")\n        result = result[: match.start] + label + result[match.end :]\n\n    return result\n</code></pre>"},{"location":"reference/api/security/#agentprobe.security.pii.PIIRedactor.has_pii","title":"<code>has_pii(text)</code>","text":"<p>Check if text contains any detectable PII.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if PII was detected.</p> Source code in <code>src/agentprobe/security/pii.py</code> <pre><code>def has_pii(self, text: str) -&gt; bool:\n    \"\"\"Check if text contains any detectable PII.\n\n    Args:\n        text: The text to check.\n\n    Returns:\n        True if PII was detected.\n    \"\"\"\n    return len(self.scan(text)) &gt; 0\n</code></pre>"},{"location":"reference/api/storage/","title":"Storage","text":"<p>Persistence backends for traces, results, and metrics.</p>"},{"location":"reference/api/storage/#sqlite-backend","title":"SQLite Backend","text":""},{"location":"reference/api/storage/#agentprobe.storage.sqlite","title":"<code>agentprobe.storage.sqlite</code>","text":"<p>SQLite storage backend for traces and test results.</p> <p>Uses Python's stdlib sqlite3 with <code>run_in_executor</code> for async wrapping. Enables WAL mode for concurrent access.</p>"},{"location":"reference/api/storage/#agentprobe.storage.sqlite.SQLiteStorage","title":"<code>SQLiteStorage</code>","text":"<p>SQLite-based storage for traces and test results.</p> <p>Uses WAL mode for concurrent read access and stores full serialized models in a <code>data</code> TEXT column for lossless round-tripping.</p> <p>Attributes:</p> Name Type Description <code>db_path</code> <p>Path to the SQLite database file.</p> Source code in <code>src/agentprobe/storage/sqlite.py</code> <pre><code>class SQLiteStorage:\n    \"\"\"SQLite-based storage for traces and test results.\n\n    Uses WAL mode for concurrent read access and stores full\n    serialized models in a ``data`` TEXT column for lossless\n    round-tripping.\n\n    Attributes:\n        db_path: Path to the SQLite database file.\n    \"\"\"\n\n    def __init__(self, db_path: str | Path = \".agentprobe/traces.db\") -&gt; None:\n        \"\"\"Initialize the SQLite storage.\n\n        Args:\n            db_path: Path to the database file. Parent directories\n                will be created if they don't exist.\n        \"\"\"\n        self._db_path = Path(db_path)\n        self._conn: sqlite3.Connection | None = None\n\n    def _get_conn(self) -&gt; sqlite3.Connection:\n        \"\"\"Get or create the database connection.\"\"\"\n        if self._conn is None:\n            self._db_path.parent.mkdir(parents=True, exist_ok=True)\n            self._conn = sqlite3.connect(str(self._db_path), check_same_thread=False)\n            self._conn.execute(\"PRAGMA journal_mode=WAL\")\n            self._conn.execute(\"PRAGMA foreign_keys=ON\")\n            self._conn.row_factory = sqlite3.Row\n        return self._conn\n\n    async def _run(self, func: Callable[[], _T]) -&gt; _T:\n        \"\"\"Run a sync function in the default executor.\"\"\"\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(None, func)\n\n    async def setup(self) -&gt; None:\n        \"\"\"Create tables and indexes if they don't exist.\"\"\"\n        try:\n            await self._run(partial(self._setup_sync))\n            logger.info(\"SQLite storage initialized at %s\", self._db_path)\n        except Exception as exc:\n            raise StorageError(f\"Failed to initialize SQLite: {exc}\") from exc\n\n    def _setup_sync(self) -&gt; None:\n        conn = self._get_conn()\n        conn.executescript(_SCHEMA)\n        conn.commit()\n\n    async def save_trace(self, trace: Trace) -&gt; None:\n        \"\"\"Persist a trace to SQLite.\n\n        Args:\n            trace: The trace to save.\n        \"\"\"\n        try:\n            await self._run(partial(self._save_trace_sync, trace))\n        except Exception as exc:\n            raise StorageError(f\"Failed to save trace: {exc}\") from exc\n\n    def _save_trace_sync(self, trace: Trace) -&gt; None:\n        conn = self._get_conn()\n        data = trace.model_dump_json()\n        tags_json = json.dumps(list(trace.tags))\n        conn.execute(\n            \"\"\"INSERT OR REPLACE INTO traces\n               (trace_id, agent_name, model, input_text, output_text,\n                total_input_tokens, total_output_tokens, total_latency_ms,\n                tags, data, created_at)\n               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n            (\n                trace.trace_id,\n                trace.agent_name,\n                trace.model,\n                trace.input_text,\n                trace.output_text,\n                trace.total_input_tokens,\n                trace.total_output_tokens,\n                trace.total_latency_ms,\n                tags_json,\n                data,\n                trace.created_at.isoformat(),\n            ),\n        )\n        conn.commit()\n\n    async def load_trace(self, trace_id: str) -&gt; Trace | None:\n        \"\"\"Load a trace by ID.\n\n        Args:\n            trace_id: The unique identifier.\n\n        Returns:\n            The trace if found, otherwise None.\n        \"\"\"\n        try:\n            result = await self._run(partial(self._load_trace_sync, trace_id))\n            return result\n        except StorageError:\n            raise\n        except Exception as exc:\n            raise StorageError(f\"Failed to load trace: {exc}\") from exc\n\n    def _load_trace_sync(self, trace_id: str) -&gt; Trace | None:\n        conn = self._get_conn()\n        row = conn.execute(\"SELECT data FROM traces WHERE trace_id = ?\", (trace_id,)).fetchone()\n        if row is None:\n            return None\n        return Trace.model_validate_json(row[\"data\"])\n\n    async def list_traces(\n        self,\n        agent_name: str | None = None,\n        limit: int = 100,\n    ) -&gt; Sequence[Trace]:\n        \"\"\"List traces with optional filtering.\n\n        Args:\n            agent_name: Filter by agent name.\n            limit: Maximum results.\n\n        Returns:\n            A list of matching traces.\n        \"\"\"\n        try:\n            result = await self._run(partial(self._list_traces_sync, agent_name, limit))\n            return result\n        except Exception as exc:\n            raise StorageError(f\"Failed to list traces: {exc}\") from exc\n\n    def _list_traces_sync(self, agent_name: str | None, limit: int) -&gt; list[Trace]:\n        conn = self._get_conn()\n        if agent_name:\n            rows = conn.execute(\n                \"SELECT data FROM traces WHERE agent_name = ? ORDER BY created_at DESC LIMIT ?\",\n                (agent_name, limit),\n            ).fetchall()\n        else:\n            rows = conn.execute(\n                \"SELECT data FROM traces ORDER BY created_at DESC LIMIT ?\",\n                (limit,),\n            ).fetchall()\n        return [Trace.model_validate_json(row[\"data\"]) for row in rows]\n\n    async def save_result(self, result: TestResult) -&gt; None:\n        \"\"\"Persist a test result.\n\n        Args:\n            result: The test result to save.\n        \"\"\"\n        try:\n            await self._run(partial(self._save_result_sync, result))\n        except Exception as exc:\n            raise StorageError(f\"Failed to save result: {exc}\") from exc\n\n    def _save_result_sync(self, result: TestResult) -&gt; None:\n        conn = self._get_conn()\n        data = result.model_dump_json()\n        conn.execute(\n            \"\"\"INSERT OR REPLACE INTO test_results\n               (result_id, test_name, status, score, duration_ms, data, created_at)\n               VALUES (?, ?, ?, ?, ?, ?, ?)\"\"\",\n            (\n                result.result_id,\n                result.test_name,\n                result.status.value,\n                result.score,\n                result.duration_ms,\n                data,\n                result.created_at.isoformat(),\n            ),\n        )\n        conn.commit()\n\n    async def load_results(\n        self,\n        test_name: str | None = None,\n        limit: int = 100,\n    ) -&gt; Sequence[TestResult]:\n        \"\"\"Load test results with optional filtering.\n\n        Args:\n            test_name: Filter by test name.\n            limit: Maximum results.\n\n        Returns:\n            A list of matching test results.\n        \"\"\"\n        try:\n            result = await self._run(partial(self._load_results_sync, test_name, limit))\n            return result\n        except Exception as exc:\n            raise StorageError(f\"Failed to load results: {exc}\") from exc\n\n    def _load_results_sync(self, test_name: str | None, limit: int) -&gt; list[TestResult]:\n        conn = self._get_conn()\n        if test_name:\n            rows = conn.execute(\n                \"SELECT data FROM test_results WHERE test_name = ? ORDER BY created_at DESC LIMIT ?\",\n                (test_name, limit),\n            ).fetchall()\n        else:\n            rows = conn.execute(\n                \"SELECT data FROM test_results ORDER BY created_at DESC LIMIT ?\",\n                (limit,),\n            ).fetchall()\n        return [TestResult.model_validate_json(row[\"data\"]) for row in rows]\n\n    async def load_result(self, result_id: str) -&gt; TestResult | None:\n        \"\"\"Load a single test result by ID.\n\n        Args:\n            result_id: The unique identifier.\n\n        Returns:\n            The test result if found, otherwise None.\n        \"\"\"\n        try:\n            return await self._run(partial(self._load_result_sync, result_id))\n        except StorageError:\n            raise\n        except Exception as exc:\n            raise StorageError(f\"Failed to load result: {exc}\") from exc\n\n    def _load_result_sync(self, result_id: str) -&gt; TestResult | None:\n        conn = self._get_conn()\n        row = conn.execute(\n            \"SELECT data FROM test_results WHERE result_id = ?\", (result_id,)\n        ).fetchone()\n        if row is None:\n            return None\n        return TestResult.model_validate_json(row[\"data\"])\n\n    async def save_metrics(self, metrics: Sequence[MetricValue]) -&gt; None:\n        \"\"\"Persist a batch of metric values.\n\n        Args:\n            metrics: The metric values to save.\n        \"\"\"\n        if not metrics:\n            return\n        try:\n            await self._run(partial(self._save_metrics_sync, metrics))\n        except Exception as exc:\n            raise StorageError(f\"Failed to save metrics: {exc}\") from exc\n\n    def _save_metrics_sync(self, metrics: Sequence[MetricValue]) -&gt; None:\n        conn = self._get_conn()\n        for mv in metrics:\n            tags_json = json.dumps(list(mv.tags))\n            meta_json = json.dumps(mv.metadata)\n            conn.execute(\n                \"\"\"INSERT INTO metrics (metric_name, value, tags, metadata, timestamp)\n                   VALUES (?, ?, ?, ?, ?)\"\"\",\n                (mv.metric_name, mv.value, tags_json, meta_json, mv.timestamp.isoformat()),\n            )\n        conn.commit()\n\n    async def load_metrics(\n        self,\n        metric_name: str | None = None,\n        limit: int = 1000,\n    ) -&gt; Sequence[MetricValue]:\n        \"\"\"Load metric values with optional filtering.\n\n        Args:\n            metric_name: Filter by metric name.\n            limit: Maximum values to return.\n\n        Returns:\n            A list of matching metric values.\n        \"\"\"\n        try:\n            return await self._run(partial(self._load_metrics_sync, metric_name, limit))\n        except Exception as exc:\n            raise StorageError(f\"Failed to load metrics: {exc}\") from exc\n\n    def _load_metrics_sync(self, metric_name: str | None, limit: int) -&gt; list[MetricValue]:\n        conn = self._get_conn()\n        if metric_name:\n            rows = conn.execute(\n                \"SELECT metric_name, value, tags, metadata, timestamp \"\n                \"FROM metrics WHERE metric_name = ? ORDER BY timestamp DESC LIMIT ?\",\n                (metric_name, limit),\n            ).fetchall()\n        else:\n            rows = conn.execute(\n                \"SELECT metric_name, value, tags, metadata, timestamp \"\n                \"FROM metrics ORDER BY timestamp DESC LIMIT ?\",\n                (limit,),\n            ).fetchall()\n\n        return [\n            MetricValue(\n                metric_name=row[\"metric_name\"],\n                value=row[\"value\"],\n                tags=tuple(json.loads(row[\"tags\"])) if row[\"tags\"] else (),\n                metadata=json.loads(row[\"metadata\"]) if row[\"metadata\"] else {},\n                timestamp=datetime.fromisoformat(row[\"timestamp\"]),\n            )\n            for row in rows\n        ]\n\n    async def close(self) -&gt; None:\n        \"\"\"Close the database connection.\"\"\"\n        if self._conn is not None:\n            self._conn.close()\n            self._conn = None\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.sqlite.SQLiteStorage.__init__","title":"<code>__init__(db_path='.agentprobe/traces.db')</code>","text":"<p>Initialize the SQLite storage.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str | Path</code> <p>Path to the database file. Parent directories will be created if they don't exist.</p> <code>'.agentprobe/traces.db'</code> Source code in <code>src/agentprobe/storage/sqlite.py</code> <pre><code>def __init__(self, db_path: str | Path = \".agentprobe/traces.db\") -&gt; None:\n    \"\"\"Initialize the SQLite storage.\n\n    Args:\n        db_path: Path to the database file. Parent directories\n            will be created if they don't exist.\n    \"\"\"\n    self._db_path = Path(db_path)\n    self._conn: sqlite3.Connection | None = None\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.sqlite.SQLiteStorage.setup","title":"<code>setup()</code>  <code>async</code>","text":"<p>Create tables and indexes if they don't exist.</p> Source code in <code>src/agentprobe/storage/sqlite.py</code> <pre><code>async def setup(self) -&gt; None:\n    \"\"\"Create tables and indexes if they don't exist.\"\"\"\n    try:\n        await self._run(partial(self._setup_sync))\n        logger.info(\"SQLite storage initialized at %s\", self._db_path)\n    except Exception as exc:\n        raise StorageError(f\"Failed to initialize SQLite: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.sqlite.SQLiteStorage.save_trace","title":"<code>save_trace(trace)</code>  <code>async</code>","text":"<p>Persist a trace to SQLite.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The trace to save.</p> required Source code in <code>src/agentprobe/storage/sqlite.py</code> <pre><code>async def save_trace(self, trace: Trace) -&gt; None:\n    \"\"\"Persist a trace to SQLite.\n\n    Args:\n        trace: The trace to save.\n    \"\"\"\n    try:\n        await self._run(partial(self._save_trace_sync, trace))\n    except Exception as exc:\n        raise StorageError(f\"Failed to save trace: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.sqlite.SQLiteStorage.load_trace","title":"<code>load_trace(trace_id)</code>  <code>async</code>","text":"<p>Load a trace by ID.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>The unique identifier.</p> required <p>Returns:</p> Type Description <code>Trace | None</code> <p>The trace if found, otherwise None.</p> Source code in <code>src/agentprobe/storage/sqlite.py</code> <pre><code>async def load_trace(self, trace_id: str) -&gt; Trace | None:\n    \"\"\"Load a trace by ID.\n\n    Args:\n        trace_id: The unique identifier.\n\n    Returns:\n        The trace if found, otherwise None.\n    \"\"\"\n    try:\n        result = await self._run(partial(self._load_trace_sync, trace_id))\n        return result\n    except StorageError:\n        raise\n    except Exception as exc:\n        raise StorageError(f\"Failed to load trace: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.sqlite.SQLiteStorage.list_traces","title":"<code>list_traces(agent_name=None, limit=100)</code>  <code>async</code>","text":"<p>List traces with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str | None</code> <p>Filter by agent name.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum results.</p> <code>100</code> <p>Returns:</p> Type Description <code>Sequence[Trace]</code> <p>A list of matching traces.</p> Source code in <code>src/agentprobe/storage/sqlite.py</code> <pre><code>async def list_traces(\n    self,\n    agent_name: str | None = None,\n    limit: int = 100,\n) -&gt; Sequence[Trace]:\n    \"\"\"List traces with optional filtering.\n\n    Args:\n        agent_name: Filter by agent name.\n        limit: Maximum results.\n\n    Returns:\n        A list of matching traces.\n    \"\"\"\n    try:\n        result = await self._run(partial(self._list_traces_sync, agent_name, limit))\n        return result\n    except Exception as exc:\n        raise StorageError(f\"Failed to list traces: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.sqlite.SQLiteStorage.save_result","title":"<code>save_result(result)</code>  <code>async</code>","text":"<p>Persist a test result.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>TestResult</code> <p>The test result to save.</p> required Source code in <code>src/agentprobe/storage/sqlite.py</code> <pre><code>async def save_result(self, result: TestResult) -&gt; None:\n    \"\"\"Persist a test result.\n\n    Args:\n        result: The test result to save.\n    \"\"\"\n    try:\n        await self._run(partial(self._save_result_sync, result))\n    except Exception as exc:\n        raise StorageError(f\"Failed to save result: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.sqlite.SQLiteStorage.load_results","title":"<code>load_results(test_name=None, limit=100)</code>  <code>async</code>","text":"<p>Load test results with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>str | None</code> <p>Filter by test name.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum results.</p> <code>100</code> <p>Returns:</p> Type Description <code>Sequence[TestResult]</code> <p>A list of matching test results.</p> Source code in <code>src/agentprobe/storage/sqlite.py</code> <pre><code>async def load_results(\n    self,\n    test_name: str | None = None,\n    limit: int = 100,\n) -&gt; Sequence[TestResult]:\n    \"\"\"Load test results with optional filtering.\n\n    Args:\n        test_name: Filter by test name.\n        limit: Maximum results.\n\n    Returns:\n        A list of matching test results.\n    \"\"\"\n    try:\n        result = await self._run(partial(self._load_results_sync, test_name, limit))\n        return result\n    except Exception as exc:\n        raise StorageError(f\"Failed to load results: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.sqlite.SQLiteStorage.load_result","title":"<code>load_result(result_id)</code>  <code>async</code>","text":"<p>Load a single test result by ID.</p> <p>Parameters:</p> Name Type Description Default <code>result_id</code> <code>str</code> <p>The unique identifier.</p> required <p>Returns:</p> Type Description <code>TestResult | None</code> <p>The test result if found, otherwise None.</p> Source code in <code>src/agentprobe/storage/sqlite.py</code> <pre><code>async def load_result(self, result_id: str) -&gt; TestResult | None:\n    \"\"\"Load a single test result by ID.\n\n    Args:\n        result_id: The unique identifier.\n\n    Returns:\n        The test result if found, otherwise None.\n    \"\"\"\n    try:\n        return await self._run(partial(self._load_result_sync, result_id))\n    except StorageError:\n        raise\n    except Exception as exc:\n        raise StorageError(f\"Failed to load result: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.sqlite.SQLiteStorage.save_metrics","title":"<code>save_metrics(metrics)</code>  <code>async</code>","text":"<p>Persist a batch of metric values.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Sequence[MetricValue]</code> <p>The metric values to save.</p> required Source code in <code>src/agentprobe/storage/sqlite.py</code> <pre><code>async def save_metrics(self, metrics: Sequence[MetricValue]) -&gt; None:\n    \"\"\"Persist a batch of metric values.\n\n    Args:\n        metrics: The metric values to save.\n    \"\"\"\n    if not metrics:\n        return\n    try:\n        await self._run(partial(self._save_metrics_sync, metrics))\n    except Exception as exc:\n        raise StorageError(f\"Failed to save metrics: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.sqlite.SQLiteStorage.load_metrics","title":"<code>load_metrics(metric_name=None, limit=1000)</code>  <code>async</code>","text":"<p>Load metric values with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str | None</code> <p>Filter by metric name.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum values to return.</p> <code>1000</code> <p>Returns:</p> Type Description <code>Sequence[MetricValue]</code> <p>A list of matching metric values.</p> Source code in <code>src/agentprobe/storage/sqlite.py</code> <pre><code>async def load_metrics(\n    self,\n    metric_name: str | None = None,\n    limit: int = 1000,\n) -&gt; Sequence[MetricValue]:\n    \"\"\"Load metric values with optional filtering.\n\n    Args:\n        metric_name: Filter by metric name.\n        limit: Maximum values to return.\n\n    Returns:\n        A list of matching metric values.\n    \"\"\"\n    try:\n        return await self._run(partial(self._load_metrics_sync, metric_name, limit))\n    except Exception as exc:\n        raise StorageError(f\"Failed to load metrics: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.sqlite.SQLiteStorage.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the database connection.</p> Source code in <code>src/agentprobe/storage/sqlite.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the database connection.\"\"\"\n    if self._conn is not None:\n        self._conn.close()\n        self._conn = None\n</code></pre>"},{"location":"reference/api/storage/#postgresql-backend","title":"PostgreSQL Backend","text":""},{"location":"reference/api/storage/#agentprobe.storage.postgres","title":"<code>agentprobe.storage.postgres</code>","text":"<p>PostgreSQL storage backend for traces, test results, and metrics.</p> <p>Uses asyncpg for async database access. The asyncpg dependency is lazy-loaded so users without PostgreSQL are not affected.</p>"},{"location":"reference/api/storage/#agentprobe.storage.postgres.PostgreSQLStorage","title":"<code>PostgreSQLStorage</code>","text":"<p>PostgreSQL-based storage for traces, results, and metrics.</p> <p>Uses asyncpg connection pool for concurrent access. Full model data is stored in a TEXT <code>data</code> column for lossless round-tripping, with extracted columns for indexing and filtering.</p> <p>Attributes:</p> Name Type Description <code>dsn</code> <p>PostgreSQL connection string.</p> Source code in <code>src/agentprobe/storage/postgres.py</code> <pre><code>class PostgreSQLStorage:\n    \"\"\"PostgreSQL-based storage for traces, results, and metrics.\n\n    Uses asyncpg connection pool for concurrent access. Full model data\n    is stored in a TEXT ``data`` column for lossless round-tripping,\n    with extracted columns for indexing and filtering.\n\n    Attributes:\n        dsn: PostgreSQL connection string.\n    \"\"\"\n\n    def __init__(\n        self,\n        dsn: str = \"postgresql://localhost/agentprobe\",\n        min_pool_size: int = 2,\n        max_pool_size: int = 10,\n    ) -&gt; None:\n        \"\"\"Initialize the PostgreSQL storage.\n\n        Args:\n            dsn: PostgreSQL connection string.\n            min_pool_size: Minimum pool connections.\n            max_pool_size: Maximum pool connections.\n        \"\"\"\n        self._dsn = dsn\n        self._min_pool_size = min_pool_size\n        self._max_pool_size = max_pool_size\n        self._pool: Any = None\n        self._migration = SchemaMigration()\n\n    async def setup(self) -&gt; None:  # pragma: no cover\n        \"\"\"Create the connection pool and run pending migrations.\n\n        Raises:\n            StorageError: If connection or migration fails.\n        \"\"\"\n        try:\n            import asyncpg  # type: ignore[import-not-found]  # noqa: PLC0415\n\n            self._pool = await asyncpg.create_pool(\n                self._dsn,\n                min_size=self._min_pool_size,\n                max_size=self._max_pool_size,\n            )\n            await self._run_migrations()\n            logger.info(\"PostgreSQL storage initialized: %s\", self._dsn)\n        except Exception as exc:\n            raise StorageError(f\"Failed to initialize PostgreSQL: {exc}\") from exc\n\n    async def _run_migrations(self) -&gt; None:  # pragma: no cover\n        \"\"\"Check current version and apply pending migrations.\"\"\"\n        async with self._pool.acquire() as conn:\n            try:\n                row = await conn.fetchrow(_SCHEMA_VERSION_QUERY)\n                current = row[\"version\"] if row else 0\n            except Exception:\n                current = 0\n\n            async def _execute(sql: str) -&gt; None:\n                await conn.execute(sql)\n\n            await self._migration.apply(current, _execute)\n\n    async def save_trace(self, trace: Trace) -&gt; None:\n        \"\"\"Persist a trace to PostgreSQL.\n\n        Args:\n            trace: The trace to save.\n\n        Raises:\n            StorageError: If the save operation fails.\n        \"\"\"\n        try:\n            data = trace.model_dump_json()\n            tags_json = json.dumps(list(trace.tags))\n            async with self._pool.acquire() as conn:\n                await conn.execute(\n                    \"\"\"INSERT INTO traces\n                       (trace_id, agent_name, model, input_text, output_text,\n                        total_input_tokens, total_output_tokens, total_latency_ms,\n                        tags, data, created_at)\n                       VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)\n                       ON CONFLICT (trace_id) DO UPDATE SET data = $10\"\"\",\n                    trace.trace_id,\n                    trace.agent_name,\n                    trace.model,\n                    trace.input_text,\n                    trace.output_text,\n                    trace.total_input_tokens,\n                    trace.total_output_tokens,\n                    trace.total_latency_ms,\n                    tags_json,\n                    data,\n                    trace.created_at,\n                )\n        except Exception as exc:\n            raise StorageError(f\"Failed to save trace: {exc}\") from exc\n\n    async def load_trace(self, trace_id: str) -&gt; Trace | None:\n        \"\"\"Load a trace by ID.\n\n        Args:\n            trace_id: The unique identifier.\n\n        Returns:\n            The trace if found, otherwise None.\n        \"\"\"\n        try:\n            async with self._pool.acquire() as conn:\n                row = await conn.fetchrow(\"SELECT data FROM traces WHERE trace_id = $1\", trace_id)\n                if row is None:\n                    return None\n                return Trace.model_validate_json(row[\"data\"])\n        except StorageError:\n            raise\n        except Exception as exc:\n            raise StorageError(f\"Failed to load trace: {exc}\") from exc\n\n    async def list_traces(\n        self,\n        agent_name: str | None = None,\n        limit: int = 100,\n    ) -&gt; Sequence[Trace]:\n        \"\"\"List traces with optional filtering.\n\n        Args:\n            agent_name: Filter by agent name.\n            limit: Maximum results.\n\n        Returns:\n            A list of matching traces.\n        \"\"\"\n        try:\n            async with self._pool.acquire() as conn:\n                if agent_name:\n                    rows = await conn.fetch(\n                        \"SELECT data FROM traces WHERE agent_name = $1 \"\n                        \"ORDER BY created_at DESC LIMIT $2\",\n                        agent_name,\n                        limit,\n                    )\n                else:\n                    rows = await conn.fetch(\n                        \"SELECT data FROM traces ORDER BY created_at DESC LIMIT $1\",\n                        limit,\n                    )\n                return [Trace.model_validate_json(row[\"data\"]) for row in rows]\n        except Exception as exc:\n            raise StorageError(f\"Failed to list traces: {exc}\") from exc\n\n    async def save_result(self, result: TestResult) -&gt; None:\n        \"\"\"Persist a test result.\n\n        Args:\n            result: The test result to save.\n\n        Raises:\n            StorageError: If the save operation fails.\n        \"\"\"\n        try:\n            data = result.model_dump_json()\n            async with self._pool.acquire() as conn:\n                await conn.execute(\n                    \"\"\"INSERT INTO test_results\n                       (result_id, test_name, status, score, duration_ms, data, created_at)\n                       VALUES ($1, $2, $3, $4, $5, $6, $7)\n                       ON CONFLICT (result_id) DO UPDATE SET data = $6\"\"\",\n                    result.result_id,\n                    result.test_name,\n                    result.status.value,\n                    result.score,\n                    result.duration_ms,\n                    data,\n                    result.created_at,\n                )\n        except Exception as exc:\n            raise StorageError(f\"Failed to save result: {exc}\") from exc\n\n    async def load_results(\n        self,\n        test_name: str | None = None,\n        limit: int = 100,\n    ) -&gt; Sequence[TestResult]:\n        \"\"\"Load test results with optional filtering.\n\n        Args:\n            test_name: Filter by test name.\n            limit: Maximum results.\n\n        Returns:\n            A list of matching test results.\n        \"\"\"\n        try:\n            async with self._pool.acquire() as conn:\n                if test_name:\n                    rows = await conn.fetch(\n                        \"SELECT data FROM test_results WHERE test_name = $1 \"\n                        \"ORDER BY created_at DESC LIMIT $2\",\n                        test_name,\n                        limit,\n                    )\n                else:\n                    rows = await conn.fetch(\n                        \"SELECT data FROM test_results ORDER BY created_at DESC LIMIT $1\",\n                        limit,\n                    )\n                return [TestResult.model_validate_json(row[\"data\"]) for row in rows]\n        except Exception as exc:\n            raise StorageError(f\"Failed to load results: {exc}\") from exc\n\n    async def load_result(self, result_id: str) -&gt; TestResult | None:\n        \"\"\"Load a single test result by ID.\n\n        Args:\n            result_id: The unique identifier.\n\n        Returns:\n            The test result if found, otherwise None.\n        \"\"\"\n        try:\n            async with self._pool.acquire() as conn:\n                row = await conn.fetchrow(\n                    \"SELECT data FROM test_results WHERE result_id = $1\", result_id\n                )\n                if row is None:\n                    return None\n                return TestResult.model_validate_json(row[\"data\"])\n        except StorageError:\n            raise\n        except Exception as exc:\n            raise StorageError(f\"Failed to load result: {exc}\") from exc\n\n    async def save_metrics(self, metrics: Sequence[MetricValue]) -&gt; None:\n        \"\"\"Persist a batch of metric values.\n\n        Args:\n            metrics: The metric values to save.\n\n        Raises:\n            StorageError: If the save operation fails.\n        \"\"\"\n        if not metrics:\n            return\n        try:\n            async with self._pool.acquire() as conn:\n                for mv in metrics:\n                    tags_json = json.dumps(list(mv.tags))\n                    meta_json = json.dumps(mv.metadata)\n                    await conn.execute(\n                        \"\"\"INSERT INTO metrics (metric_name, value, tags, metadata, timestamp)\n                           VALUES ($1, $2, $3, $4, $5)\"\"\",\n                        mv.metric_name,\n                        mv.value,\n                        tags_json,\n                        meta_json,\n                        mv.timestamp,\n                    )\n        except Exception as exc:\n            raise StorageError(f\"Failed to save metrics: {exc}\") from exc\n\n    async def load_metrics(\n        self,\n        metric_name: str | None = None,\n        limit: int = 1000,\n    ) -&gt; Sequence[MetricValue]:\n        \"\"\"Load metric values with optional filtering.\n\n        Args:\n            metric_name: Filter by metric name.\n            limit: Maximum values to return.\n\n        Returns:\n            A sequence of matching metric values.\n        \"\"\"\n        try:\n            async with self._pool.acquire() as conn:\n                if metric_name:\n                    rows = await conn.fetch(\n                        \"SELECT metric_name, value, tags, metadata, timestamp \"\n                        \"FROM metrics WHERE metric_name = $1 \"\n                        \"ORDER BY timestamp DESC LIMIT $2\",\n                        metric_name,\n                        limit,\n                    )\n                else:\n                    rows = await conn.fetch(\n                        \"SELECT metric_name, value, tags, metadata, timestamp \"\n                        \"FROM metrics ORDER BY timestamp DESC LIMIT $1\",\n                        limit,\n                    )\n                return [\n                    MetricValue(\n                        metric_name=row[\"metric_name\"],\n                        value=row[\"value\"],\n                        tags=tuple(json.loads(row[\"tags\"])) if row[\"tags\"] else (),\n                        metadata=json.loads(row[\"metadata\"]) if row[\"metadata\"] else {},\n                        timestamp=row[\"timestamp\"],\n                    )\n                    for row in rows\n                ]\n        except Exception as exc:\n            raise StorageError(f\"Failed to load metrics: {exc}\") from exc\n\n    async def close(self) -&gt; None:  # pragma: no cover\n        \"\"\"Close the connection pool.\"\"\"\n        if self._pool is not None:\n            await self._pool.close()\n            self._pool = None\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.postgres.PostgreSQLStorage.__init__","title":"<code>__init__(dsn='postgresql://localhost/agentprobe', min_pool_size=2, max_pool_size=10)</code>","text":"<p>Initialize the PostgreSQL storage.</p> <p>Parameters:</p> Name Type Description Default <code>dsn</code> <code>str</code> <p>PostgreSQL connection string.</p> <code>'postgresql://localhost/agentprobe'</code> <code>min_pool_size</code> <code>int</code> <p>Minimum pool connections.</p> <code>2</code> <code>max_pool_size</code> <code>int</code> <p>Maximum pool connections.</p> <code>10</code> Source code in <code>src/agentprobe/storage/postgres.py</code> <pre><code>def __init__(\n    self,\n    dsn: str = \"postgresql://localhost/agentprobe\",\n    min_pool_size: int = 2,\n    max_pool_size: int = 10,\n) -&gt; None:\n    \"\"\"Initialize the PostgreSQL storage.\n\n    Args:\n        dsn: PostgreSQL connection string.\n        min_pool_size: Minimum pool connections.\n        max_pool_size: Maximum pool connections.\n    \"\"\"\n    self._dsn = dsn\n    self._min_pool_size = min_pool_size\n    self._max_pool_size = max_pool_size\n    self._pool: Any = None\n    self._migration = SchemaMigration()\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.postgres.PostgreSQLStorage.setup","title":"<code>setup()</code>  <code>async</code>","text":"<p>Create the connection pool and run pending migrations.</p> <p>Raises:</p> Type Description <code>StorageError</code> <p>If connection or migration fails.</p> Source code in <code>src/agentprobe/storage/postgres.py</code> <pre><code>async def setup(self) -&gt; None:  # pragma: no cover\n    \"\"\"Create the connection pool and run pending migrations.\n\n    Raises:\n        StorageError: If connection or migration fails.\n    \"\"\"\n    try:\n        import asyncpg  # type: ignore[import-not-found]  # noqa: PLC0415\n\n        self._pool = await asyncpg.create_pool(\n            self._dsn,\n            min_size=self._min_pool_size,\n            max_size=self._max_pool_size,\n        )\n        await self._run_migrations()\n        logger.info(\"PostgreSQL storage initialized: %s\", self._dsn)\n    except Exception as exc:\n        raise StorageError(f\"Failed to initialize PostgreSQL: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.postgres.PostgreSQLStorage.save_trace","title":"<code>save_trace(trace)</code>  <code>async</code>","text":"<p>Persist a trace to PostgreSQL.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The trace to save.</p> required <p>Raises:</p> Type Description <code>StorageError</code> <p>If the save operation fails.</p> Source code in <code>src/agentprobe/storage/postgres.py</code> <pre><code>async def save_trace(self, trace: Trace) -&gt; None:\n    \"\"\"Persist a trace to PostgreSQL.\n\n    Args:\n        trace: The trace to save.\n\n    Raises:\n        StorageError: If the save operation fails.\n    \"\"\"\n    try:\n        data = trace.model_dump_json()\n        tags_json = json.dumps(list(trace.tags))\n        async with self._pool.acquire() as conn:\n            await conn.execute(\n                \"\"\"INSERT INTO traces\n                   (trace_id, agent_name, model, input_text, output_text,\n                    total_input_tokens, total_output_tokens, total_latency_ms,\n                    tags, data, created_at)\n                   VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)\n                   ON CONFLICT (trace_id) DO UPDATE SET data = $10\"\"\",\n                trace.trace_id,\n                trace.agent_name,\n                trace.model,\n                trace.input_text,\n                trace.output_text,\n                trace.total_input_tokens,\n                trace.total_output_tokens,\n                trace.total_latency_ms,\n                tags_json,\n                data,\n                trace.created_at,\n            )\n    except Exception as exc:\n        raise StorageError(f\"Failed to save trace: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.postgres.PostgreSQLStorage.load_trace","title":"<code>load_trace(trace_id)</code>  <code>async</code>","text":"<p>Load a trace by ID.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>The unique identifier.</p> required <p>Returns:</p> Type Description <code>Trace | None</code> <p>The trace if found, otherwise None.</p> Source code in <code>src/agentprobe/storage/postgres.py</code> <pre><code>async def load_trace(self, trace_id: str) -&gt; Trace | None:\n    \"\"\"Load a trace by ID.\n\n    Args:\n        trace_id: The unique identifier.\n\n    Returns:\n        The trace if found, otherwise None.\n    \"\"\"\n    try:\n        async with self._pool.acquire() as conn:\n            row = await conn.fetchrow(\"SELECT data FROM traces WHERE trace_id = $1\", trace_id)\n            if row is None:\n                return None\n            return Trace.model_validate_json(row[\"data\"])\n    except StorageError:\n        raise\n    except Exception as exc:\n        raise StorageError(f\"Failed to load trace: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.postgres.PostgreSQLStorage.list_traces","title":"<code>list_traces(agent_name=None, limit=100)</code>  <code>async</code>","text":"<p>List traces with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str | None</code> <p>Filter by agent name.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum results.</p> <code>100</code> <p>Returns:</p> Type Description <code>Sequence[Trace]</code> <p>A list of matching traces.</p> Source code in <code>src/agentprobe/storage/postgres.py</code> <pre><code>async def list_traces(\n    self,\n    agent_name: str | None = None,\n    limit: int = 100,\n) -&gt; Sequence[Trace]:\n    \"\"\"List traces with optional filtering.\n\n    Args:\n        agent_name: Filter by agent name.\n        limit: Maximum results.\n\n    Returns:\n        A list of matching traces.\n    \"\"\"\n    try:\n        async with self._pool.acquire() as conn:\n            if agent_name:\n                rows = await conn.fetch(\n                    \"SELECT data FROM traces WHERE agent_name = $1 \"\n                    \"ORDER BY created_at DESC LIMIT $2\",\n                    agent_name,\n                    limit,\n                )\n            else:\n                rows = await conn.fetch(\n                    \"SELECT data FROM traces ORDER BY created_at DESC LIMIT $1\",\n                    limit,\n                )\n            return [Trace.model_validate_json(row[\"data\"]) for row in rows]\n    except Exception as exc:\n        raise StorageError(f\"Failed to list traces: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.postgres.PostgreSQLStorage.save_result","title":"<code>save_result(result)</code>  <code>async</code>","text":"<p>Persist a test result.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>TestResult</code> <p>The test result to save.</p> required <p>Raises:</p> Type Description <code>StorageError</code> <p>If the save operation fails.</p> Source code in <code>src/agentprobe/storage/postgres.py</code> <pre><code>async def save_result(self, result: TestResult) -&gt; None:\n    \"\"\"Persist a test result.\n\n    Args:\n        result: The test result to save.\n\n    Raises:\n        StorageError: If the save operation fails.\n    \"\"\"\n    try:\n        data = result.model_dump_json()\n        async with self._pool.acquire() as conn:\n            await conn.execute(\n                \"\"\"INSERT INTO test_results\n                   (result_id, test_name, status, score, duration_ms, data, created_at)\n                   VALUES ($1, $2, $3, $4, $5, $6, $7)\n                   ON CONFLICT (result_id) DO UPDATE SET data = $6\"\"\",\n                result.result_id,\n                result.test_name,\n                result.status.value,\n                result.score,\n                result.duration_ms,\n                data,\n                result.created_at,\n            )\n    except Exception as exc:\n        raise StorageError(f\"Failed to save result: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.postgres.PostgreSQLStorage.load_results","title":"<code>load_results(test_name=None, limit=100)</code>  <code>async</code>","text":"<p>Load test results with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>test_name</code> <code>str | None</code> <p>Filter by test name.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum results.</p> <code>100</code> <p>Returns:</p> Type Description <code>Sequence[TestResult]</code> <p>A list of matching test results.</p> Source code in <code>src/agentprobe/storage/postgres.py</code> <pre><code>async def load_results(\n    self,\n    test_name: str | None = None,\n    limit: int = 100,\n) -&gt; Sequence[TestResult]:\n    \"\"\"Load test results with optional filtering.\n\n    Args:\n        test_name: Filter by test name.\n        limit: Maximum results.\n\n    Returns:\n        A list of matching test results.\n    \"\"\"\n    try:\n        async with self._pool.acquire() as conn:\n            if test_name:\n                rows = await conn.fetch(\n                    \"SELECT data FROM test_results WHERE test_name = $1 \"\n                    \"ORDER BY created_at DESC LIMIT $2\",\n                    test_name,\n                    limit,\n                )\n            else:\n                rows = await conn.fetch(\n                    \"SELECT data FROM test_results ORDER BY created_at DESC LIMIT $1\",\n                    limit,\n                )\n            return [TestResult.model_validate_json(row[\"data\"]) for row in rows]\n    except Exception as exc:\n        raise StorageError(f\"Failed to load results: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.postgres.PostgreSQLStorage.load_result","title":"<code>load_result(result_id)</code>  <code>async</code>","text":"<p>Load a single test result by ID.</p> <p>Parameters:</p> Name Type Description Default <code>result_id</code> <code>str</code> <p>The unique identifier.</p> required <p>Returns:</p> Type Description <code>TestResult | None</code> <p>The test result if found, otherwise None.</p> Source code in <code>src/agentprobe/storage/postgres.py</code> <pre><code>async def load_result(self, result_id: str) -&gt; TestResult | None:\n    \"\"\"Load a single test result by ID.\n\n    Args:\n        result_id: The unique identifier.\n\n    Returns:\n        The test result if found, otherwise None.\n    \"\"\"\n    try:\n        async with self._pool.acquire() as conn:\n            row = await conn.fetchrow(\n                \"SELECT data FROM test_results WHERE result_id = $1\", result_id\n            )\n            if row is None:\n                return None\n            return TestResult.model_validate_json(row[\"data\"])\n    except StorageError:\n        raise\n    except Exception as exc:\n        raise StorageError(f\"Failed to load result: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.postgres.PostgreSQLStorage.save_metrics","title":"<code>save_metrics(metrics)</code>  <code>async</code>","text":"<p>Persist a batch of metric values.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Sequence[MetricValue]</code> <p>The metric values to save.</p> required <p>Raises:</p> Type Description <code>StorageError</code> <p>If the save operation fails.</p> Source code in <code>src/agentprobe/storage/postgres.py</code> <pre><code>async def save_metrics(self, metrics: Sequence[MetricValue]) -&gt; None:\n    \"\"\"Persist a batch of metric values.\n\n    Args:\n        metrics: The metric values to save.\n\n    Raises:\n        StorageError: If the save operation fails.\n    \"\"\"\n    if not metrics:\n        return\n    try:\n        async with self._pool.acquire() as conn:\n            for mv in metrics:\n                tags_json = json.dumps(list(mv.tags))\n                meta_json = json.dumps(mv.metadata)\n                await conn.execute(\n                    \"\"\"INSERT INTO metrics (metric_name, value, tags, metadata, timestamp)\n                       VALUES ($1, $2, $3, $4, $5)\"\"\",\n                    mv.metric_name,\n                    mv.value,\n                    tags_json,\n                    meta_json,\n                    mv.timestamp,\n                )\n    except Exception as exc:\n        raise StorageError(f\"Failed to save metrics: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.postgres.PostgreSQLStorage.load_metrics","title":"<code>load_metrics(metric_name=None, limit=1000)</code>  <code>async</code>","text":"<p>Load metric values with optional filtering.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str | None</code> <p>Filter by metric name.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum values to return.</p> <code>1000</code> <p>Returns:</p> Type Description <code>Sequence[MetricValue]</code> <p>A sequence of matching metric values.</p> Source code in <code>src/agentprobe/storage/postgres.py</code> <pre><code>async def load_metrics(\n    self,\n    metric_name: str | None = None,\n    limit: int = 1000,\n) -&gt; Sequence[MetricValue]:\n    \"\"\"Load metric values with optional filtering.\n\n    Args:\n        metric_name: Filter by metric name.\n        limit: Maximum values to return.\n\n    Returns:\n        A sequence of matching metric values.\n    \"\"\"\n    try:\n        async with self._pool.acquire() as conn:\n            if metric_name:\n                rows = await conn.fetch(\n                    \"SELECT metric_name, value, tags, metadata, timestamp \"\n                    \"FROM metrics WHERE metric_name = $1 \"\n                    \"ORDER BY timestamp DESC LIMIT $2\",\n                    metric_name,\n                    limit,\n                )\n            else:\n                rows = await conn.fetch(\n                    \"SELECT metric_name, value, tags, metadata, timestamp \"\n                    \"FROM metrics ORDER BY timestamp DESC LIMIT $1\",\n                    limit,\n                )\n            return [\n                MetricValue(\n                    metric_name=row[\"metric_name\"],\n                    value=row[\"value\"],\n                    tags=tuple(json.loads(row[\"tags\"])) if row[\"tags\"] else (),\n                    metadata=json.loads(row[\"metadata\"]) if row[\"metadata\"] else {},\n                    timestamp=row[\"timestamp\"],\n                )\n                for row in rows\n            ]\n    except Exception as exc:\n        raise StorageError(f\"Failed to load metrics: {exc}\") from exc\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.postgres.PostgreSQLStorage.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the connection pool.</p> Source code in <code>src/agentprobe/storage/postgres.py</code> <pre><code>async def close(self) -&gt; None:  # pragma: no cover\n    \"\"\"Close the connection pool.\"\"\"\n    if self._pool is not None:\n        await self._pool.close()\n        self._pool = None\n</code></pre>"},{"location":"reference/api/storage/#migrations","title":"Migrations","text":""},{"location":"reference/api/storage/#agentprobe.storage.migrations","title":"<code>agentprobe.storage.migrations</code>","text":"<p>Schema migrations for storage backends.</p> <p>Provides a linear version-based migration system for database schemas. Each migration is a pair of (version, SQL statements) applied in order.</p>"},{"location":"reference/api/storage/#agentprobe.storage.migrations.SchemaMigration","title":"<code>SchemaMigration</code>","text":"<p>Manages linear schema migrations for PostgreSQL.</p> <p>Tracks the current schema version and applies any pending migrations in order.</p> Source code in <code>src/agentprobe/storage/migrations.py</code> <pre><code>class SchemaMigration:\n    \"\"\"Manages linear schema migrations for PostgreSQL.\n\n    Tracks the current schema version and applies any pending\n    migrations in order.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._migrations = list(_MIGRATIONS)\n\n    @property\n    def latest_version(self) -&gt; int:\n        \"\"\"Return the latest available schema version.\"\"\"\n        if not self._migrations:\n            return 0\n        return self._migrations[-1][0]\n\n    def get_pending(self, current_version: int) -&gt; list[tuple[int, str]]:\n        \"\"\"Get migrations that haven't been applied yet.\n\n        Args:\n            current_version: The currently applied schema version.\n\n        Returns:\n            List of (version, sql) tuples to apply.\n        \"\"\"\n        return [(v, sql) for v, sql in self._migrations if v &gt; current_version]\n\n    async def apply(\n        self,\n        current_version: int,\n        execute_fn: Any,\n    ) -&gt; int:\n        \"\"\"Apply pending migrations using the provided execution function.\n\n        Args:\n            current_version: The currently applied schema version.\n            execute_fn: Async callable that executes SQL strings.\n\n        Returns:\n            The new schema version after applying migrations.\n        \"\"\"\n        pending = self.get_pending(current_version)\n        if not pending:\n            logger.info(\"Schema is up to date at version %d\", current_version)\n            return current_version\n\n        for version, sql in pending:\n            logger.info(\"Applying migration V%d\", version)\n            await execute_fn(sql)\n\n        new_version = pending[-1][0]\n        logger.info(\"Schema migrated to version %d\", new_version)\n        return new_version\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.migrations.SchemaMigration.latest_version","title":"<code>latest_version</code>  <code>property</code>","text":"<p>Return the latest available schema version.</p>"},{"location":"reference/api/storage/#agentprobe.storage.migrations.SchemaMigration.get_pending","title":"<code>get_pending(current_version)</code>","text":"<p>Get migrations that haven't been applied yet.</p> <p>Parameters:</p> Name Type Description Default <code>current_version</code> <code>int</code> <p>The currently applied schema version.</p> required <p>Returns:</p> Type Description <code>list[tuple[int, str]]</code> <p>List of (version, sql) tuples to apply.</p> Source code in <code>src/agentprobe/storage/migrations.py</code> <pre><code>def get_pending(self, current_version: int) -&gt; list[tuple[int, str]]:\n    \"\"\"Get migrations that haven't been applied yet.\n\n    Args:\n        current_version: The currently applied schema version.\n\n    Returns:\n        List of (version, sql) tuples to apply.\n    \"\"\"\n    return [(v, sql) for v, sql in self._migrations if v &gt; current_version]\n</code></pre>"},{"location":"reference/api/storage/#agentprobe.storage.migrations.SchemaMigration.apply","title":"<code>apply(current_version, execute_fn)</code>  <code>async</code>","text":"<p>Apply pending migrations using the provided execution function.</p> <p>Parameters:</p> Name Type Description Default <code>current_version</code> <code>int</code> <p>The currently applied schema version.</p> required <code>execute_fn</code> <code>Any</code> <p>Async callable that executes SQL strings.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The new schema version after applying migrations.</p> Source code in <code>src/agentprobe/storage/migrations.py</code> <pre><code>async def apply(\n    self,\n    current_version: int,\n    execute_fn: Any,\n) -&gt; int:\n    \"\"\"Apply pending migrations using the provided execution function.\n\n    Args:\n        current_version: The currently applied schema version.\n        execute_fn: Async callable that executes SQL strings.\n\n    Returns:\n        The new schema version after applying migrations.\n    \"\"\"\n    pending = self.get_pending(current_version)\n    if not pending:\n        logger.info(\"Schema is up to date at version %d\", current_version)\n        return current_version\n\n    for version, sql in pending:\n        logger.info(\"Applying migration V%d\", version)\n        await execute_fn(sql)\n\n    new_version = pending[-1][0]\n    logger.info(\"Schema migrated to version %d\", new_version)\n    return new_version\n</code></pre>"},{"location":"reference/api/testing/","title":"Testing API Reference","text":""},{"location":"reference/api/testing/#agentprobe.testing","title":"<code>agentprobe.testing</code>","text":"<p>Pytest-native assertion helpers for agent trace validation.</p> <p>Provides fluent <code>assert_trace()</code> chains, <code>assert_score()</code> for evaluator thresholds, and <code>assert_cost()</code> for budget checks. All assertions raise standard <code>AssertionError</code> for native pytest introspection.</p>"},{"location":"reference/api/testing/#agentprobe.testing.TraceAssertion","title":"<code>TraceAssertion</code>","text":"<p>Fluent assertion chain for validating a <code>Trace</code>.</p> <p>All methods return <code>self</code> for chaining. Failed assertions raise <code>AssertionError</code> immediately.</p> Example <pre><code>assert_trace(trace).has_output().contains(\"hello\").not_contains(\"error\")\n</code></pre> Source code in <code>src/agentprobe/testing.py</code> <pre><code>class TraceAssertion:\n    \"\"\"Fluent assertion chain for validating a ``Trace``.\n\n    All methods return ``self`` for chaining. Failed assertions raise\n    ``AssertionError`` immediately.\n\n    Example:\n        ```python\n        assert_trace(trace).has_output().contains(\"hello\").not_contains(\"error\")\n        ```\n    \"\"\"\n\n    def __init__(self, trace: Trace) -&gt; None:\n        \"\"\"Initialize with a trace to validate.\n\n        Args:\n            trace: The execution trace to assert against.\n        \"\"\"\n        self._trace = trace\n\n    def has_output(self) -&gt; TraceAssertion:\n        \"\"\"Assert the trace has non-empty output text.\n\n        Returns:\n            Self for chaining.\n\n        Raises:\n            AssertionError: If output_text is empty.\n        \"\"\"\n        if not self._trace.output_text:\n            msg = \"Expected trace to have non-empty output, but output_text is empty\"\n            raise AssertionError(msg)\n        return self\n\n    def contains(self, substring: str) -&gt; TraceAssertion:\n        \"\"\"Assert the output contains a substring.\n\n        Args:\n            substring: The substring to search for.\n\n        Returns:\n            Self for chaining.\n\n        Raises:\n            AssertionError: If the substring is not found.\n        \"\"\"\n        if substring not in self._trace.output_text:\n            msg = (\n                f\"Expected output to contain {substring!r}, \"\n                f\"but got: {self._trace.output_text!r}\"\n            )\n            raise AssertionError(msg)\n        return self\n\n    def not_contains(self, substring: str) -&gt; TraceAssertion:\n        \"\"\"Assert the output does NOT contain a substring.\n\n        Args:\n            substring: The substring that should be absent.\n\n        Returns:\n            Self for chaining.\n\n        Raises:\n            AssertionError: If the substring is found.\n        \"\"\"\n        if substring in self._trace.output_text:\n            msg = (\n                f\"Expected output to NOT contain {substring!r}, \"\n                f\"but it was found in: {self._trace.output_text!r}\"\n            )\n            raise AssertionError(msg)\n        return self\n\n    def matches(self, pattern: str) -&gt; TraceAssertion:\n        \"\"\"Assert the output matches a regex pattern.\n\n        Args:\n            pattern: The regex pattern to match against.\n\n        Returns:\n            Self for chaining.\n\n        Raises:\n            AssertionError: If the pattern does not match.\n        \"\"\"\n        if re.search(pattern, self._trace.output_text) is None:\n            msg = (\n                f\"Expected output to match pattern {pattern!r}, \"\n                f\"but got: {self._trace.output_text!r}\"\n            )\n            raise AssertionError(msg)\n        return self\n\n    def has_tool_calls(self, min_count: int = 1) -&gt; TraceAssertion:\n        \"\"\"Assert the trace has at least ``min_count`` tool calls.\n\n        Args:\n            min_count: Minimum number of tool calls expected.\n\n        Returns:\n            Self for chaining.\n\n        Raises:\n            AssertionError: If fewer tool calls than expected.\n        \"\"\"\n        actual = len(self._trace.tool_calls)\n        if actual &lt; min_count:\n            msg = f\"Expected at least {min_count} tool call(s), but got {actual}\"\n            raise AssertionError(msg)\n        return self\n\n    def has_tool(self, name: str) -&gt; TraceAssertion:\n        \"\"\"Assert the trace contains a tool call with the given name.\n\n        Args:\n            name: The tool name to look for.\n\n        Returns:\n            Self for chaining.\n\n        Raises:\n            AssertionError: If no tool call with that name exists.\n        \"\"\"\n        tool_names = [tc.tool_name for tc in self._trace.tool_calls]\n        if name not in tool_names:\n            msg = f\"Expected tool call {name!r}, but found: {tool_names}\"\n            raise AssertionError(msg)\n        return self\n\n    def has_llm_calls(self, min_count: int = 1) -&gt; TraceAssertion:\n        \"\"\"Assert the trace has at least ``min_count`` LLM calls.\n\n        Args:\n            min_count: Minimum number of LLM calls expected.\n\n        Returns:\n            Self for chaining.\n\n        Raises:\n            AssertionError: If fewer LLM calls than expected.\n        \"\"\"\n        actual = len(self._trace.llm_calls)\n        if actual &lt; min_count:\n            msg = f\"Expected at least {min_count} LLM call(s), but got {actual}\"\n            raise AssertionError(msg)\n        return self\n\n    def output_length_less_than(self, n: int) -&gt; TraceAssertion:\n        \"\"\"Assert the output length is less than ``n`` characters.\n\n        Args:\n            n: Maximum allowed length (exclusive).\n\n        Returns:\n            Self for chaining.\n\n        Raises:\n            AssertionError: If output is too long.\n        \"\"\"\n        actual = len(self._trace.output_text)\n        if actual &gt;= n:\n            msg = f\"Expected output length &lt; {n}, but got {actual}\"\n            raise AssertionError(msg)\n        return self\n\n    def output_is_valid_json(self) -&gt; TraceAssertion:\n        \"\"\"Assert the output is valid JSON.\n\n        Returns:\n            Self for chaining.\n\n        Raises:\n            AssertionError: If output is not valid JSON.\n        \"\"\"\n        try:\n            json.loads(self._trace.output_text)\n        except (json.JSONDecodeError, TypeError) as exc:\n            msg = f\"Expected output to be valid JSON, but got error: {exc}\"\n            raise AssertionError(msg) from None\n        return self\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.testing.TraceAssertion.__init__","title":"<code>__init__(trace)</code>","text":"<p>Initialize with a trace to validate.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The execution trace to assert against.</p> required Source code in <code>src/agentprobe/testing.py</code> <pre><code>def __init__(self, trace: Trace) -&gt; None:\n    \"\"\"Initialize with a trace to validate.\n\n    Args:\n        trace: The execution trace to assert against.\n    \"\"\"\n    self._trace = trace\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.testing.TraceAssertion.has_output","title":"<code>has_output()</code>","text":"<p>Assert the trace has non-empty output text.</p> <p>Returns:</p> Type Description <code>TraceAssertion</code> <p>Self for chaining.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If output_text is empty.</p> Source code in <code>src/agentprobe/testing.py</code> <pre><code>def has_output(self) -&gt; TraceAssertion:\n    \"\"\"Assert the trace has non-empty output text.\n\n    Returns:\n        Self for chaining.\n\n    Raises:\n        AssertionError: If output_text is empty.\n    \"\"\"\n    if not self._trace.output_text:\n        msg = \"Expected trace to have non-empty output, but output_text is empty\"\n        raise AssertionError(msg)\n    return self\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.testing.TraceAssertion.contains","title":"<code>contains(substring)</code>","text":"<p>Assert the output contains a substring.</p> <p>Parameters:</p> Name Type Description Default <code>substring</code> <code>str</code> <p>The substring to search for.</p> required <p>Returns:</p> Type Description <code>TraceAssertion</code> <p>Self for chaining.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the substring is not found.</p> Source code in <code>src/agentprobe/testing.py</code> <pre><code>def contains(self, substring: str) -&gt; TraceAssertion:\n    \"\"\"Assert the output contains a substring.\n\n    Args:\n        substring: The substring to search for.\n\n    Returns:\n        Self for chaining.\n\n    Raises:\n        AssertionError: If the substring is not found.\n    \"\"\"\n    if substring not in self._trace.output_text:\n        msg = (\n            f\"Expected output to contain {substring!r}, \"\n            f\"but got: {self._trace.output_text!r}\"\n        )\n        raise AssertionError(msg)\n    return self\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.testing.TraceAssertion.not_contains","title":"<code>not_contains(substring)</code>","text":"<p>Assert the output does NOT contain a substring.</p> <p>Parameters:</p> Name Type Description Default <code>substring</code> <code>str</code> <p>The substring that should be absent.</p> required <p>Returns:</p> Type Description <code>TraceAssertion</code> <p>Self for chaining.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the substring is found.</p> Source code in <code>src/agentprobe/testing.py</code> <pre><code>def not_contains(self, substring: str) -&gt; TraceAssertion:\n    \"\"\"Assert the output does NOT contain a substring.\n\n    Args:\n        substring: The substring that should be absent.\n\n    Returns:\n        Self for chaining.\n\n    Raises:\n        AssertionError: If the substring is found.\n    \"\"\"\n    if substring in self._trace.output_text:\n        msg = (\n            f\"Expected output to NOT contain {substring!r}, \"\n            f\"but it was found in: {self._trace.output_text!r}\"\n        )\n        raise AssertionError(msg)\n    return self\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.testing.TraceAssertion.matches","title":"<code>matches(pattern)</code>","text":"<p>Assert the output matches a regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The regex pattern to match against.</p> required <p>Returns:</p> Type Description <code>TraceAssertion</code> <p>Self for chaining.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the pattern does not match.</p> Source code in <code>src/agentprobe/testing.py</code> <pre><code>def matches(self, pattern: str) -&gt; TraceAssertion:\n    \"\"\"Assert the output matches a regex pattern.\n\n    Args:\n        pattern: The regex pattern to match against.\n\n    Returns:\n        Self for chaining.\n\n    Raises:\n        AssertionError: If the pattern does not match.\n    \"\"\"\n    if re.search(pattern, self._trace.output_text) is None:\n        msg = (\n            f\"Expected output to match pattern {pattern!r}, \"\n            f\"but got: {self._trace.output_text!r}\"\n        )\n        raise AssertionError(msg)\n    return self\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.testing.TraceAssertion.has_tool_calls","title":"<code>has_tool_calls(min_count=1)</code>","text":"<p>Assert the trace has at least <code>min_count</code> tool calls.</p> <p>Parameters:</p> Name Type Description Default <code>min_count</code> <code>int</code> <p>Minimum number of tool calls expected.</p> <code>1</code> <p>Returns:</p> Type Description <code>TraceAssertion</code> <p>Self for chaining.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If fewer tool calls than expected.</p> Source code in <code>src/agentprobe/testing.py</code> <pre><code>def has_tool_calls(self, min_count: int = 1) -&gt; TraceAssertion:\n    \"\"\"Assert the trace has at least ``min_count`` tool calls.\n\n    Args:\n        min_count: Minimum number of tool calls expected.\n\n    Returns:\n        Self for chaining.\n\n    Raises:\n        AssertionError: If fewer tool calls than expected.\n    \"\"\"\n    actual = len(self._trace.tool_calls)\n    if actual &lt; min_count:\n        msg = f\"Expected at least {min_count} tool call(s), but got {actual}\"\n        raise AssertionError(msg)\n    return self\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.testing.TraceAssertion.has_tool","title":"<code>has_tool(name)</code>","text":"<p>Assert the trace contains a tool call with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The tool name to look for.</p> required <p>Returns:</p> Type Description <code>TraceAssertion</code> <p>Self for chaining.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If no tool call with that name exists.</p> Source code in <code>src/agentprobe/testing.py</code> <pre><code>def has_tool(self, name: str) -&gt; TraceAssertion:\n    \"\"\"Assert the trace contains a tool call with the given name.\n\n    Args:\n        name: The tool name to look for.\n\n    Returns:\n        Self for chaining.\n\n    Raises:\n        AssertionError: If no tool call with that name exists.\n    \"\"\"\n    tool_names = [tc.tool_name for tc in self._trace.tool_calls]\n    if name not in tool_names:\n        msg = f\"Expected tool call {name!r}, but found: {tool_names}\"\n        raise AssertionError(msg)\n    return self\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.testing.TraceAssertion.has_llm_calls","title":"<code>has_llm_calls(min_count=1)</code>","text":"<p>Assert the trace has at least <code>min_count</code> LLM calls.</p> <p>Parameters:</p> Name Type Description Default <code>min_count</code> <code>int</code> <p>Minimum number of LLM calls expected.</p> <code>1</code> <p>Returns:</p> Type Description <code>TraceAssertion</code> <p>Self for chaining.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If fewer LLM calls than expected.</p> Source code in <code>src/agentprobe/testing.py</code> <pre><code>def has_llm_calls(self, min_count: int = 1) -&gt; TraceAssertion:\n    \"\"\"Assert the trace has at least ``min_count`` LLM calls.\n\n    Args:\n        min_count: Minimum number of LLM calls expected.\n\n    Returns:\n        Self for chaining.\n\n    Raises:\n        AssertionError: If fewer LLM calls than expected.\n    \"\"\"\n    actual = len(self._trace.llm_calls)\n    if actual &lt; min_count:\n        msg = f\"Expected at least {min_count} LLM call(s), but got {actual}\"\n        raise AssertionError(msg)\n    return self\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.testing.TraceAssertion.output_length_less_than","title":"<code>output_length_less_than(n)</code>","text":"<p>Assert the output length is less than <code>n</code> characters.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Maximum allowed length (exclusive).</p> required <p>Returns:</p> Type Description <code>TraceAssertion</code> <p>Self for chaining.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If output is too long.</p> Source code in <code>src/agentprobe/testing.py</code> <pre><code>def output_length_less_than(self, n: int) -&gt; TraceAssertion:\n    \"\"\"Assert the output length is less than ``n`` characters.\n\n    Args:\n        n: Maximum allowed length (exclusive).\n\n    Returns:\n        Self for chaining.\n\n    Raises:\n        AssertionError: If output is too long.\n    \"\"\"\n    actual = len(self._trace.output_text)\n    if actual &gt;= n:\n        msg = f\"Expected output length &lt; {n}, but got {actual}\"\n        raise AssertionError(msg)\n    return self\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.testing.TraceAssertion.output_is_valid_json","title":"<code>output_is_valid_json()</code>","text":"<p>Assert the output is valid JSON.</p> <p>Returns:</p> Type Description <code>TraceAssertion</code> <p>Self for chaining.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If output is not valid JSON.</p> Source code in <code>src/agentprobe/testing.py</code> <pre><code>def output_is_valid_json(self) -&gt; TraceAssertion:\n    \"\"\"Assert the output is valid JSON.\n\n    Returns:\n        Self for chaining.\n\n    Raises:\n        AssertionError: If output is not valid JSON.\n    \"\"\"\n    try:\n        json.loads(self._trace.output_text)\n    except (json.JSONDecodeError, TypeError) as exc:\n        msg = f\"Expected output to be valid JSON, but got error: {exc}\"\n        raise AssertionError(msg) from None\n    return self\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.testing.assert_trace","title":"<code>assert_trace(trace)</code>","text":"<p>Create a fluent assertion chain for a trace.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The execution trace to validate.</p> required <p>Returns:</p> Type Description <code>TraceAssertion</code> <p>A TraceAssertion for chaining assertions.</p> Example <pre><code>assert_trace(trace).has_output().contains(\"hello\").has_tool(\"search\")\n</code></pre> Source code in <code>src/agentprobe/testing.py</code> <pre><code>def assert_trace(trace: Trace) -&gt; TraceAssertion:\n    \"\"\"Create a fluent assertion chain for a trace.\n\n    Args:\n        trace: The execution trace to validate.\n\n    Returns:\n        A TraceAssertion for chaining assertions.\n\n    Example:\n        ```python\n        assert_trace(trace).has_output().contains(\"hello\").has_tool(\"search\")\n        ```\n    \"\"\"\n    return TraceAssertion(trace)\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.testing.assert_score","title":"<code>assert_score(trace, evaluator, *, min_score=0.7, input_text='', test_name='assert_score')</code>  <code>async</code>","text":"<p>Run an evaluator and assert the score meets a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The execution trace to evaluate.</p> required <code>evaluator</code> <code>BaseEvaluator</code> <p>The evaluator to run.</p> required <code>min_score</code> <code>float</code> <p>Minimum acceptable score (0.0 to 1.0).</p> <code>0.7</code> <code>input_text</code> <code>str</code> <p>Input text for the test case context.</p> <code>''</code> <code>test_name</code> <code>str</code> <p>Name for the synthetic test case.</p> <code>'assert_score'</code> <p>Returns:</p> Type Description <code>EvalResult</code> <p>The EvalResult from the evaluator.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the score is below the threshold.</p> Source code in <code>src/agentprobe/testing.py</code> <pre><code>async def assert_score(\n    trace: Trace,\n    evaluator: BaseEvaluator,\n    *,\n    min_score: float = 0.7,\n    input_text: str = \"\",\n    test_name: str = \"assert_score\",\n) -&gt; EvalResult:\n    \"\"\"Run an evaluator and assert the score meets a threshold.\n\n    Args:\n        trace: The execution trace to evaluate.\n        evaluator: The evaluator to run.\n        min_score: Minimum acceptable score (0.0 to 1.0).\n        input_text: Input text for the test case context.\n        test_name: Name for the synthetic test case.\n\n    Returns:\n        The EvalResult from the evaluator.\n\n    Raises:\n        AssertionError: If the score is below the threshold.\n    \"\"\"\n    from agentprobe.core.models import TestCase\n\n    test_case = TestCase(name=test_name, input_text=input_text)\n    result = await evaluator.evaluate(test_case, trace)\n\n    if result.score &lt; min_score:\n        msg = (\n            f\"Expected score &gt;= {min_score}, but {evaluator.name} \"\n            f\"returned {result.score:.4f} ({result.verdict.value}): {result.reason}\"\n        )\n        raise AssertionError(msg)\n\n    return result\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.testing.assert_cost","title":"<code>assert_cost(trace, *, max_usd, calculator=None)</code>","text":"<p>Calculate trace cost and assert it is within budget.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The execution trace to price.</p> required <code>max_usd</code> <code>float</code> <p>Maximum allowed cost in USD.</p> required <code>calculator</code> <code>CostCalculator | None</code> <p>Optional cost calculator. Uses default pricing if None.</p> <code>None</code> <p>Returns:</p> Type Description <code>CostSummary</code> <p>The CostSummary from the calculator.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the cost exceeds the budget.</p> Source code in <code>src/agentprobe/testing.py</code> <pre><code>def assert_cost(\n    trace: Trace,\n    *,\n    max_usd: float,\n    calculator: CostCalculator | None = None,\n) -&gt; CostSummary:\n    \"\"\"Calculate trace cost and assert it is within budget.\n\n    Args:\n        trace: The execution trace to price.\n        max_usd: Maximum allowed cost in USD.\n        calculator: Optional cost calculator. Uses default pricing if None.\n\n    Returns:\n        The CostSummary from the calculator.\n\n    Raises:\n        AssertionError: If the cost exceeds the budget.\n    \"\"\"\n    calc = calculator or CostCalculator()\n    summary = calc.calculate_trace_cost(trace)\n\n    if summary.total_cost_usd &gt; max_usd:\n        msg = (\n            f\"Expected cost &lt;= ${max_usd:.6f}, \"\n            f\"but actual cost is ${summary.total_cost_usd:.6f}\"\n        )\n        raise AssertionError(msg)\n\n    return summary\n</code></pre>"},{"location":"reference/api/testing/#assert-helpers","title":"Assert Helpers","text":""},{"location":"reference/api/testing/#assert_tracetrace","title":"<code>assert_trace(trace)</code>","text":"<p>Create a fluent assertion chain for a <code>Trace</code>. Returns a <code>TraceAssertion</code>.</p> <pre><code>from agentprobe.testing import assert_trace\n\nassert_trace(trace).has_output().contains(\"hello\").has_tool(\"search\")\n</code></pre>"},{"location":"reference/api/testing/#traceassertion","title":"<code>TraceAssertion</code>","text":"<p>Fluent chain returned by <code>assert_trace()</code>. All methods return <code>self</code> for chaining and raise <code>AssertionError</code> on failure.</p> Method Description <code>.has_output()</code> Assert non-empty <code>output_text</code> <code>.contains(s)</code> Assert output contains substring <code>s</code> <code>.not_contains(s)</code> Assert output does NOT contain substring <code>s</code> <code>.matches(pattern)</code> Assert output matches regex <code>pattern</code> <code>.has_tool_calls(min_count=1)</code> Assert at least <code>min_count</code> tool calls <code>.has_tool(name)</code> Assert a tool call with <code>name</code> exists <code>.has_llm_calls(min_count=1)</code> Assert at least <code>min_count</code> LLM calls <code>.output_length_less_than(n)</code> Assert output length &lt; <code>n</code> <code>.output_is_valid_json()</code> Assert output parses as valid JSON"},{"location":"reference/api/testing/#assert_scoretrace-evaluator-min_score07","title":"<code>assert_score(trace, evaluator, *, min_score=0.7, ...)</code>","text":"<p>Async. Run an evaluator against a trace and assert the score meets a threshold.</p> <pre><code>result = await assert_score(trace, evaluator, min_score=0.8)\n# result is an EvalResult\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>trace</code> <code>Trace</code> required Execution trace to evaluate <code>evaluator</code> <code>BaseEvaluator</code> required Evaluator to run <code>min_score</code> <code>float</code> <code>0.7</code> Minimum acceptable score <code>input_text</code> <code>str</code> <code>\"\"</code> Input text for test case context <code>test_name</code> <code>str</code> <code>\"assert_score\"</code> Name for synthetic test case <p>Returns: <code>EvalResult</code></p> <p>Raises: <code>AssertionError</code> if <code>score &lt; min_score</code></p>"},{"location":"reference/api/testing/#assert_costtrace-max_usd-calculatornone","title":"<code>assert_cost(trace, *, max_usd, calculator=None)</code>","text":"<p>Sync. Calculate trace cost and assert it is within budget.</p> <pre><code>summary = assert_cost(trace, max_usd=0.01)\n# summary is a CostSummary\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>trace</code> <code>Trace</code> required Execution trace to price <code>max_usd</code> <code>float</code> required Maximum allowed cost in USD <code>calculator</code> <code>CostCalculator \\| None</code> <code>None</code> Custom calculator (uses defaults if None) <p>Returns: <code>CostSummary</code></p> <p>Raises: <code>AssertionError</code> if <code>total_cost_usd &gt; max_usd</code></p>"},{"location":"reference/api/testing/#pytest-plugin","title":"pytest Plugin","text":""},{"location":"reference/api/testing/#agentprobe.pytest_plugin","title":"<code>agentprobe.pytest_plugin</code>","text":"<p>Pytest plugin for AgentProbe: provides fixtures and markers.</p> <p>Auto-registered via the <code>pytest11</code> entry point. Provides:</p> <ul> <li><code>agentprobe</code> fixture (function-scoped): returns an <code>AgentProbeContext</code>   with <code>invoke()</code>, <code>evaluate()</code>, and <code>calculate_cost()</code> methods.</li> <li><code>agentprobe_config</code> fixture (session-scoped): loads <code>AgentProbeConfig</code>.</li> <li><code>agentprobe_storage</code> fixture (session-scoped): manages <code>SQLiteStorage</code>.</li> <li><code>--agentprobe-config</code>, <code>--agentprobe-trace-dir</code>, <code>--agentprobe-store-traces</code>   command-line options.</li> <li><code>@pytest.mark.agentprobe</code> marker for tagging tests.</li> </ul>"},{"location":"reference/api/testing/#agentprobe.pytest_plugin.AgentProbeContext","title":"<code>AgentProbeContext</code>","text":"<p>Per-test context for invoking agents and running evaluations.</p> <p>Created fresh for each test function by the <code>agentprobe</code> fixture.</p> <p>Attributes:</p> Name Type Description <code>traces</code> <code>list[Trace]</code> <p>All traces collected during this test.</p> <code>config</code> <code>AgentProbeConfig</code> <p>The AgentProbe configuration.</p> Source code in <code>src/agentprobe/pytest_plugin.py</code> <pre><code>class AgentProbeContext:\n    \"\"\"Per-test context for invoking agents and running evaluations.\n\n    Created fresh for each test function by the ``agentprobe`` fixture.\n\n    Attributes:\n        traces: All traces collected during this test.\n        config: The AgentProbe configuration.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: AgentProbeConfig,\n        storage: SQLiteStorage | None = None,\n        store_traces: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the context.\n\n        Args:\n            config: The AgentProbe configuration.\n            storage: Optional storage backend for persisting traces.\n            store_traces: Whether to persist traces after invocation.\n        \"\"\"\n        self._config = config\n        self._storage = storage\n        self._store_traces = store_traces\n        self._traces: list[Trace] = []\n        self._calculator: CostCalculator | None = None\n\n    @property\n    def config(self) -&gt; AgentProbeConfig:\n        \"\"\"Return the AgentProbe configuration.\"\"\"\n        return self._config\n\n    @property\n    def traces(self) -&gt; list[Trace]:\n        \"\"\"Return all traces collected during this test.\"\"\"\n        return list(self._traces)\n\n    @property\n    def last_trace(self) -&gt; Trace:\n        \"\"\"Return the most recent trace.\n\n        Returns:\n            The last collected trace.\n\n        Raises:\n            ValueError: If no traces have been collected.\n        \"\"\"\n        if not self._traces:\n            msg = \"No traces collected yet \u2014 call invoke() first\"\n            raise ValueError(msg)\n        return self._traces[-1]\n\n    async def invoke(\n        self,\n        input_text: str,\n        adapter: BaseAdapter,\n        **kwargs: Any,\n    ) -&gt; Trace:\n        \"\"\"Invoke an adapter and collect the trace.\n\n        Args:\n            input_text: The input prompt to send.\n            adapter: The agent adapter to invoke.\n            **kwargs: Additional arguments passed to the adapter.\n\n        Returns:\n            The execution trace from the adapter.\n        \"\"\"\n        trace = await adapter.invoke(input_text, **kwargs)\n        self._traces.append(trace)\n\n        if self._store_traces and self._storage is not None:\n            await self._storage.save_trace(trace)\n\n        return trace\n\n    async def evaluate(\n        self,\n        trace: Trace,\n        evaluator: BaseEvaluator,\n        input_text: str = \"\",\n        test_name: str = \"agentprobe_eval\",\n    ) -&gt; EvalResult:\n        \"\"\"Run an evaluator against a trace.\n\n        Args:\n            trace: The execution trace to evaluate.\n            evaluator: The evaluator to run.\n            input_text: Input text for the synthetic test case.\n            test_name: Name for the synthetic test case.\n\n        Returns:\n            The evaluation result.\n        \"\"\"\n        from agentprobe.core.models import TestCase\n\n        test_case = TestCase(name=test_name, input_text=input_text)\n        return await evaluator.evaluate(test_case, trace)\n\n    def calculate_cost(self, trace: Trace) -&gt; CostSummary:\n        \"\"\"Calculate the cost of a trace.\n\n        Args:\n            trace: The execution trace to price.\n\n        Returns:\n            A cost summary with per-model breakdown.\n        \"\"\"\n        if self._calculator is None:\n            self._calculator = CostCalculator()\n        return self._calculator.calculate_trace_cost(trace)\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.pytest_plugin.AgentProbeContext.config","title":"<code>config</code>  <code>property</code>","text":"<p>Return the AgentProbe configuration.</p>"},{"location":"reference/api/testing/#agentprobe.pytest_plugin.AgentProbeContext.traces","title":"<code>traces</code>  <code>property</code>","text":"<p>Return all traces collected during this test.</p>"},{"location":"reference/api/testing/#agentprobe.pytest_plugin.AgentProbeContext.last_trace","title":"<code>last_trace</code>  <code>property</code>","text":"<p>Return the most recent trace.</p> <p>Returns:</p> Type Description <code>Trace</code> <p>The last collected trace.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no traces have been collected.</p>"},{"location":"reference/api/testing/#agentprobe.pytest_plugin.AgentProbeContext.__init__","title":"<code>__init__(config, storage=None, store_traces=False)</code>","text":"<p>Initialize the context.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AgentProbeConfig</code> <p>The AgentProbe configuration.</p> required <code>storage</code> <code>SQLiteStorage | None</code> <p>Optional storage backend for persisting traces.</p> <code>None</code> <code>store_traces</code> <code>bool</code> <p>Whether to persist traces after invocation.</p> <code>False</code> Source code in <code>src/agentprobe/pytest_plugin.py</code> <pre><code>def __init__(\n    self,\n    config: AgentProbeConfig,\n    storage: SQLiteStorage | None = None,\n    store_traces: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the context.\n\n    Args:\n        config: The AgentProbe configuration.\n        storage: Optional storage backend for persisting traces.\n        store_traces: Whether to persist traces after invocation.\n    \"\"\"\n    self._config = config\n    self._storage = storage\n    self._store_traces = store_traces\n    self._traces: list[Trace] = []\n    self._calculator: CostCalculator | None = None\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.pytest_plugin.AgentProbeContext.invoke","title":"<code>invoke(input_text, adapter, **kwargs)</code>  <code>async</code>","text":"<p>Invoke an adapter and collect the trace.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The input prompt to send.</p> required <code>adapter</code> <code>BaseAdapter</code> <p>The agent adapter to invoke.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the adapter.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Trace</code> <p>The execution trace from the adapter.</p> Source code in <code>src/agentprobe/pytest_plugin.py</code> <pre><code>async def invoke(\n    self,\n    input_text: str,\n    adapter: BaseAdapter,\n    **kwargs: Any,\n) -&gt; Trace:\n    \"\"\"Invoke an adapter and collect the trace.\n\n    Args:\n        input_text: The input prompt to send.\n        adapter: The agent adapter to invoke.\n        **kwargs: Additional arguments passed to the adapter.\n\n    Returns:\n        The execution trace from the adapter.\n    \"\"\"\n    trace = await adapter.invoke(input_text, **kwargs)\n    self._traces.append(trace)\n\n    if self._store_traces and self._storage is not None:\n        await self._storage.save_trace(trace)\n\n    return trace\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.pytest_plugin.AgentProbeContext.evaluate","title":"<code>evaluate(trace, evaluator, input_text='', test_name='agentprobe_eval')</code>  <code>async</code>","text":"<p>Run an evaluator against a trace.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The execution trace to evaluate.</p> required <code>evaluator</code> <code>BaseEvaluator</code> <p>The evaluator to run.</p> required <code>input_text</code> <code>str</code> <p>Input text for the synthetic test case.</p> <code>''</code> <code>test_name</code> <code>str</code> <p>Name for the synthetic test case.</p> <code>'agentprobe_eval'</code> <p>Returns:</p> Type Description <code>EvalResult</code> <p>The evaluation result.</p> Source code in <code>src/agentprobe/pytest_plugin.py</code> <pre><code>async def evaluate(\n    self,\n    trace: Trace,\n    evaluator: BaseEvaluator,\n    input_text: str = \"\",\n    test_name: str = \"agentprobe_eval\",\n) -&gt; EvalResult:\n    \"\"\"Run an evaluator against a trace.\n\n    Args:\n        trace: The execution trace to evaluate.\n        evaluator: The evaluator to run.\n        input_text: Input text for the synthetic test case.\n        test_name: Name for the synthetic test case.\n\n    Returns:\n        The evaluation result.\n    \"\"\"\n    from agentprobe.core.models import TestCase\n\n    test_case = TestCase(name=test_name, input_text=input_text)\n    return await evaluator.evaluate(test_case, trace)\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.pytest_plugin.AgentProbeContext.calculate_cost","title":"<code>calculate_cost(trace)</code>","text":"<p>Calculate the cost of a trace.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The execution trace to price.</p> required <p>Returns:</p> Type Description <code>CostSummary</code> <p>A cost summary with per-model breakdown.</p> Source code in <code>src/agentprobe/pytest_plugin.py</code> <pre><code>def calculate_cost(self, trace: Trace) -&gt; CostSummary:\n    \"\"\"Calculate the cost of a trace.\n\n    Args:\n        trace: The execution trace to price.\n\n    Returns:\n        A cost summary with per-model breakdown.\n    \"\"\"\n    if self._calculator is None:\n        self._calculator = CostCalculator()\n    return self._calculator.calculate_trace_cost(trace)\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.pytest_plugin.pytest_addoption","title":"<code>pytest_addoption(parser)</code>","text":"<p>Add AgentProbe-specific command-line options.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>Parser</code> <p>The pytest argument parser.</p> required Source code in <code>src/agentprobe/pytest_plugin.py</code> <pre><code>def pytest_addoption(parser: pytest.Parser) -&gt; None:\n    \"\"\"Add AgentProbe-specific command-line options.\n\n    Args:\n        parser: The pytest argument parser.\n    \"\"\"\n    group = parser.getgroup(\"agentprobe\", \"AgentProbe agent testing\")\n    group.addoption(\n        \"--agentprobe-config\",\n        default=None,\n        help=\"Path to agentprobe.yaml config file.\",\n    )\n    group.addoption(\n        \"--agentprobe-trace-dir\",\n        default=None,\n        help=\"Directory to store trace database.\",\n    )\n    group.addoption(\n        \"--agentprobe-store-traces\",\n        action=\"store_true\",\n        default=False,\n        help=\"Persist traces to storage after each test.\",\n    )\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.pytest_plugin.pytest_configure","title":"<code>pytest_configure(config)</code>","text":"<p>Register the <code>agentprobe</code> marker.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The pytest configuration object.</p> required Source code in <code>src/agentprobe/pytest_plugin.py</code> <pre><code>def pytest_configure(config: pytest.Config) -&gt; None:\n    \"\"\"Register the ``agentprobe`` marker.\n\n    Args:\n        config: The pytest configuration object.\n    \"\"\"\n    config.addinivalue_line(\n        \"markers\",\n        \"agentprobe: marks tests using the AgentProbe framework\",\n    )\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.pytest_plugin.agentprobe_config","title":"<code>agentprobe_config(request)</code>","text":"<p>Load AgentProbe configuration (session-scoped).</p> <p>Reads from <code>--agentprobe-config</code> if provided, otherwise searches for <code>agentprobe.yaml</code> / <code>agentprobe.yml</code> or uses defaults.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>FixtureRequest</code> <p>The pytest fixture request.</p> required <p>Returns:</p> Type Description <code>AgentProbeConfig</code> <p>A validated AgentProbeConfig.</p> Source code in <code>src/agentprobe/pytest_plugin.py</code> <pre><code>@pytest.fixture(scope=\"session\")\ndef agentprobe_config(request: pytest.FixtureRequest) -&gt; AgentProbeConfig:\n    \"\"\"Load AgentProbe configuration (session-scoped).\n\n    Reads from ``--agentprobe-config`` if provided, otherwise searches\n    for ``agentprobe.yaml`` / ``agentprobe.yml`` or uses defaults.\n\n    Args:\n        request: The pytest fixture request.\n\n    Returns:\n        A validated AgentProbeConfig.\n    \"\"\"\n    config_path = request.config.getoption(\"--agentprobe-config\", default=None)\n    return load_config(config_path)\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.pytest_plugin.agentprobe_storage","title":"<code>agentprobe_storage(agentprobe_config, request)</code>  <code>async</code>","text":"<p>Create and manage a SQLite storage instance (session-scoped).</p> <p>Parameters:</p> Name Type Description Default <code>agentprobe_config</code> <code>AgentProbeConfig</code> <p>The loaded configuration.</p> required <code>request</code> <code>FixtureRequest</code> <p>The pytest fixture request.</p> required <p>Yields:</p> Type Description <code>AsyncGenerator[SQLiteStorage]</code> <p>An initialized SQLiteStorage instance.</p> Source code in <code>src/agentprobe/pytest_plugin.py</code> <pre><code>@pytest.fixture(scope=\"session\")\nasync def agentprobe_storage(\n    agentprobe_config: AgentProbeConfig,\n    request: pytest.FixtureRequest,\n) -&gt; AsyncGenerator[SQLiteStorage]:\n    \"\"\"Create and manage a SQLite storage instance (session-scoped).\n\n    Args:\n        agentprobe_config: The loaded configuration.\n        request: The pytest fixture request.\n\n    Yields:\n        An initialized SQLiteStorage instance.\n    \"\"\"\n    trace_dir = request.config.getoption(\"--agentprobe-trace-dir\", default=None)\n    db_path = trace_dir + \"/traces.db\" if trace_dir else agentprobe_config.trace.database_path\n\n    storage = SQLiteStorage(db_path=db_path)\n    await storage.setup()\n\n    yield storage\n\n    await storage.close()\n</code></pre>"},{"location":"reference/api/testing/#agentprobe.pytest_plugin.agentprobe","title":"<code>agentprobe(agentprobe_config, request)</code>","text":"<p>Provide a fresh AgentProbeContext for each test (function-scoped).</p> <p>Parameters:</p> Name Type Description Default <code>agentprobe_config</code> <code>AgentProbeConfig</code> <p>The session-scoped configuration.</p> required <code>request</code> <code>FixtureRequest</code> <p>The pytest fixture request.</p> required <p>Returns:</p> Type Description <code>AgentProbeContext</code> <p>An AgentProbeContext for invoking agents and running evaluations.</p> Source code in <code>src/agentprobe/pytest_plugin.py</code> <pre><code>@pytest.fixture\ndef agentprobe(\n    agentprobe_config: AgentProbeConfig,\n    request: pytest.FixtureRequest,\n) -&gt; AgentProbeContext:\n    \"\"\"Provide a fresh AgentProbeContext for each test (function-scoped).\n\n    Args:\n        agentprobe_config: The session-scoped configuration.\n        request: The pytest fixture request.\n\n    Returns:\n        An AgentProbeContext for invoking agents and running evaluations.\n    \"\"\"\n    store_traces = request.config.getoption(\"--agentprobe-store-traces\", default=False)\n\n    storage: SQLiteStorage | None = None\n    if store_traces:\n        storage = request.getfixturevalue(\"agentprobe_storage\")\n\n    return AgentProbeContext(\n        config=agentprobe_config,\n        storage=storage,\n        store_traces=bool(store_traces),\n    )\n</code></pre>"},{"location":"reference/api/testing/#fixtures","title":"Fixtures","text":""},{"location":"reference/api/testing/#agentprobe-function-scoped","title":"<code>agentprobe</code> (function-scoped)","text":"<p>Returns an <code>AgentProbeContext</code> for each test.</p> <pre><code>async def test_agent(agentprobe):\n    trace = await agentprobe.invoke(\"input\", adapter=my_adapter)\n</code></pre>"},{"location":"reference/api/testing/#agentprobe_config-session-scoped","title":"<code>agentprobe_config</code> (session-scoped)","text":"<p>Returns <code>AgentProbeConfig</code> loaded from <code>--agentprobe-config</code> or defaults.</p>"},{"location":"reference/api/testing/#agentprobe_storage-session-scoped","title":"<code>agentprobe_storage</code> (session-scoped)","text":"<p>Returns an initialized <code>SQLiteStorage</code>. Only instantiated when <code>--agentprobe-store-traces</code> is used.</p>"},{"location":"reference/api/testing/#agentprobecontext","title":"<code>AgentProbeContext</code>","text":"Method / Property Type Description <code>invoke(input_text, adapter, **kwargs)</code> async Invoke adapter and collect trace <code>evaluate(trace, evaluator, ...)</code> async Run evaluator on trace <code>calculate_cost(trace)</code> sync Calculate cost summary <code>traces</code> <code>list[Trace]</code> All traces collected in this test <code>last_trace</code> <code>Trace</code> Most recent trace (raises if empty) <code>config</code> <code>AgentProbeConfig</code> The loaded configuration"},{"location":"reference/api/testing/#cli-options","title":"CLI Options","text":"Option Description <code>--agentprobe-config PATH</code> Path to <code>agentprobe.yaml</code> <code>--agentprobe-trace-dir DIR</code> Directory for trace database <code>--agentprobe-store-traces</code> Persist traces to SQLite"},{"location":"reference/api/testing/#marker","title":"Marker","text":"<pre><code>@pytest.mark.agentprobe\nasync def test_my_agent(agentprobe):\n    ...\n</code></pre> <p>Disable the plugin: <code>pytest -p no:agentprobe</code></p>"},{"location":"reference/api/trace/","title":"Trace","text":"<p>Trace recording, replay, and time-travel debugging.</p>"},{"location":"reference/api/trace/#recorder","title":"Recorder","text":""},{"location":"reference/api/trace/#agentprobe.trace.recorder","title":"<code>agentprobe.trace.recorder</code>","text":"<p>Trace recorder: captures agent execution events into structured Traces.</p> <p>Provides an async context manager for recording LLM calls, tool invocations, and message exchanges during agent execution.</p>"},{"location":"reference/api/trace/#agentprobe.trace.recorder.TraceRecordingContext","title":"<code>TraceRecordingContext</code>","text":"<p>Mutable context for recording events during an agent run.</p> <p>Accumulates LLM calls, tool calls, and turns that will be assembled into a frozen Trace by <code>TraceRecorder.finalize()</code>.</p> Source code in <code>src/agentprobe/trace/recorder.py</code> <pre><code>class TraceRecordingContext:\n    \"\"\"Mutable context for recording events during an agent run.\n\n    Accumulates LLM calls, tool calls, and turns that will be\n    assembled into a frozen Trace by ``TraceRecorder.finalize()``.\n    \"\"\"\n\n    def __init__(self, agent_name: str, model: str | None = None) -&gt; None:\n        self.agent_name = agent_name\n        self.model = model\n        self.llm_calls: list[LLMCall] = []\n        self.tool_calls: list[ToolCall] = []\n        self.turns: list[Turn] = []\n        self.tags: list[str] = []\n        self.metadata: dict[str, Any] = {}\n        self._start_time = time.monotonic()\n\n    def record_llm_call(\n        self,\n        *,\n        model: str,\n        input_tokens: int = 0,\n        output_tokens: int = 0,\n        input_text: str = \"\",\n        output_text: str = \"\",\n        latency_ms: int = 0,\n        metadata: dict[str, Any] | None = None,\n    ) -&gt; LLMCall:\n        \"\"\"Record an LLM call event.\n\n        Args:\n            model: Model identifier.\n            input_tokens: Number of input tokens.\n            output_tokens: Number of output tokens.\n            input_text: Prompt text.\n            output_text: Response text.\n            latency_ms: Call latency in milliseconds.\n            metadata: Additional metadata.\n\n        Returns:\n            The recorded LLMCall object.\n        \"\"\"\n        call = LLMCall(\n            model=model,\n            input_tokens=input_tokens,\n            output_tokens=output_tokens,\n            input_text=input_text,\n            output_text=output_text,\n            latency_ms=latency_ms,\n            metadata=metadata or {},\n        )\n        self.llm_calls.append(call)\n        self.turns.append(Turn(turn_type=TurnType.LLM_CALL, content=output_text, llm_call=call))\n        if self.model is None:\n            self.model = model\n        return call\n\n    def record_tool_call(\n        self,\n        *,\n        tool_name: str,\n        tool_input: dict[str, Any] | None = None,\n        tool_output: Any = None,\n        success: bool = True,\n        error: str | None = None,\n        latency_ms: int = 0,\n    ) -&gt; ToolCall:\n        \"\"\"Record a tool call event.\n\n        Args:\n            tool_name: Name of the tool.\n            tool_input: Arguments passed to the tool.\n            tool_output: Output from the tool.\n            success: Whether the call succeeded.\n            error: Error message if failed.\n            latency_ms: Call latency in milliseconds.\n\n        Returns:\n            The recorded ToolCall object.\n        \"\"\"\n        call = ToolCall(\n            tool_name=tool_name,\n            tool_input=tool_input or {},\n            tool_output=tool_output,\n            success=success,\n            error=error,\n            latency_ms=latency_ms,\n        )\n        self.tool_calls.append(call)\n        self.turns.append(\n            Turn(\n                turn_type=TurnType.TOOL_CALL,\n                content=str(tool_output) if tool_output is not None else \"\",\n                tool_call=call,\n            )\n        )\n        return call\n\n    @property\n    def elapsed_ms(self) -&gt; int:\n        \"\"\"Return elapsed time since recording started.\"\"\"\n        return int((time.monotonic() - self._start_time) * 1000)\n</code></pre>"},{"location":"reference/api/trace/#agentprobe.trace.recorder.TraceRecordingContext.elapsed_ms","title":"<code>elapsed_ms</code>  <code>property</code>","text":"<p>Return elapsed time since recording started.</p>"},{"location":"reference/api/trace/#agentprobe.trace.recorder.TraceRecordingContext.record_llm_call","title":"<code>record_llm_call(*, model, input_tokens=0, output_tokens=0, input_text='', output_text='', latency_ms=0, metadata=None)</code>","text":"<p>Record an LLM call event.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier.</p> required <code>input_tokens</code> <code>int</code> <p>Number of input tokens.</p> <code>0</code> <code>output_tokens</code> <code>int</code> <p>Number of output tokens.</p> <code>0</code> <code>input_text</code> <code>str</code> <p>Prompt text.</p> <code>''</code> <code>output_text</code> <code>str</code> <p>Response text.</p> <code>''</code> <code>latency_ms</code> <code>int</code> <p>Call latency in milliseconds.</p> <code>0</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Additional metadata.</p> <code>None</code> <p>Returns:</p> Type Description <code>LLMCall</code> <p>The recorded LLMCall object.</p> Source code in <code>src/agentprobe/trace/recorder.py</code> <pre><code>def record_llm_call(\n    self,\n    *,\n    model: str,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n    input_text: str = \"\",\n    output_text: str = \"\",\n    latency_ms: int = 0,\n    metadata: dict[str, Any] | None = None,\n) -&gt; LLMCall:\n    \"\"\"Record an LLM call event.\n\n    Args:\n        model: Model identifier.\n        input_tokens: Number of input tokens.\n        output_tokens: Number of output tokens.\n        input_text: Prompt text.\n        output_text: Response text.\n        latency_ms: Call latency in milliseconds.\n        metadata: Additional metadata.\n\n    Returns:\n        The recorded LLMCall object.\n    \"\"\"\n    call = LLMCall(\n        model=model,\n        input_tokens=input_tokens,\n        output_tokens=output_tokens,\n        input_text=input_text,\n        output_text=output_text,\n        latency_ms=latency_ms,\n        metadata=metadata or {},\n    )\n    self.llm_calls.append(call)\n    self.turns.append(Turn(turn_type=TurnType.LLM_CALL, content=output_text, llm_call=call))\n    if self.model is None:\n        self.model = model\n    return call\n</code></pre>"},{"location":"reference/api/trace/#agentprobe.trace.recorder.TraceRecordingContext.record_tool_call","title":"<code>record_tool_call(*, tool_name, tool_input=None, tool_output=None, success=True, error=None, latency_ms=0)</code>","text":"<p>Record a tool call event.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool.</p> required <code>tool_input</code> <code>dict[str, Any] | None</code> <p>Arguments passed to the tool.</p> <code>None</code> <code>tool_output</code> <code>Any</code> <p>Output from the tool.</p> <code>None</code> <code>success</code> <code>bool</code> <p>Whether the call succeeded.</p> <code>True</code> <code>error</code> <code>str | None</code> <p>Error message if failed.</p> <code>None</code> <code>latency_ms</code> <code>int</code> <p>Call latency in milliseconds.</p> <code>0</code> <p>Returns:</p> Type Description <code>ToolCall</code> <p>The recorded ToolCall object.</p> Source code in <code>src/agentprobe/trace/recorder.py</code> <pre><code>def record_tool_call(\n    self,\n    *,\n    tool_name: str,\n    tool_input: dict[str, Any] | None = None,\n    tool_output: Any = None,\n    success: bool = True,\n    error: str | None = None,\n    latency_ms: int = 0,\n) -&gt; ToolCall:\n    \"\"\"Record a tool call event.\n\n    Args:\n        tool_name: Name of the tool.\n        tool_input: Arguments passed to the tool.\n        tool_output: Output from the tool.\n        success: Whether the call succeeded.\n        error: Error message if failed.\n        latency_ms: Call latency in milliseconds.\n\n    Returns:\n        The recorded ToolCall object.\n    \"\"\"\n    call = ToolCall(\n        tool_name=tool_name,\n        tool_input=tool_input or {},\n        tool_output=tool_output,\n        success=success,\n        error=error,\n        latency_ms=latency_ms,\n    )\n    self.tool_calls.append(call)\n    self.turns.append(\n        Turn(\n            turn_type=TurnType.TOOL_CALL,\n            content=str(tool_output) if tool_output is not None else \"\",\n            tool_call=call,\n        )\n    )\n    return call\n</code></pre>"},{"location":"reference/api/trace/#agentprobe.trace.recorder.TraceRecorder","title":"<code>TraceRecorder</code>","text":"<p>Records agent execution events into a structured Trace.</p> <p>Use as an async context manager via <code>recording()</code> to capture events, then call <code>finalize()</code> to produce the frozen Trace.</p> <p>Attributes:</p> Name Type Description <code>agent_name</code> <p>Name of the agent being recorded.</p> <code>model</code> <p>Primary model being used.</p> <code>tags</code> <p>Optional tags for filtering.</p> Example <pre><code>recorder = TraceRecorder(agent_name=\"support\")\n\nasync with recorder.recording() as ctx:\n    ctx.record_llm_call(model=\"claude-sonnet-4-5-20250929\", ...)\n    ctx.record_tool_call(tool_name=\"search\", ...)\n\ntrace = recorder.finalize(output=\"Done\")\n</code></pre> Source code in <code>src/agentprobe/trace/recorder.py</code> <pre><code>class TraceRecorder:\n    \"\"\"Records agent execution events into a structured Trace.\n\n    Use as an async context manager via ``recording()`` to capture\n    events, then call ``finalize()`` to produce the frozen Trace.\n\n    Attributes:\n        agent_name: Name of the agent being recorded.\n        model: Primary model being used.\n        tags: Optional tags for filtering.\n\n    Example:\n        ```python\n        recorder = TraceRecorder(agent_name=\"support\")\n\n        async with recorder.recording() as ctx:\n            ctx.record_llm_call(model=\"claude-sonnet-4-5-20250929\", ...)\n            ctx.record_tool_call(tool_name=\"search\", ...)\n\n        trace = recorder.finalize(output=\"Done\")\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_name: str,\n        model: str | None = None,\n        tags: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize a new trace recorder.\n\n        Args:\n            agent_name: Identifier for the agent being recorded.\n            model: Primary model name.\n            tags: Optional tags for categorization.\n\n        Raises:\n            ValueError: If agent_name is empty.\n        \"\"\"\n        if not agent_name:\n            msg = \"agent_name must not be empty\"\n            raise ValueError(msg)\n        self._agent_name = agent_name\n        self._model = model\n        self._tags = tags or []\n        self._context: TraceRecordingContext | None = None\n\n    @asynccontextmanager\n    async def recording(self) -&gt; AsyncGenerator[TraceRecordingContext, None]:\n        \"\"\"Start a recording session.\n\n        Yields a TraceRecordingContext for recording events.\n\n        Yields:\n            A mutable recording context.\n        \"\"\"\n        self._context = TraceRecordingContext(\n            agent_name=self._agent_name,\n            model=self._model,\n        )\n        self._context.tags = list(self._tags)\n        logger.debug(\"Started recording for agent '%s'\", self._agent_name)\n        try:\n            yield self._context\n        except Exception:\n            logger.exception(\"Error during recording for agent '%s'\", self._agent_name)\n            raise\n        finally:\n            logger.debug(\n                \"Recording ended for agent '%s': %d LLM calls, %d tool calls\",\n                self._agent_name,\n                len(self._context.llm_calls),\n                len(self._context.tool_calls),\n            )\n\n    def finalize(\n        self,\n        *,\n        input_text: str = \"\",\n        output: str = \"\",\n    ) -&gt; Trace:\n        \"\"\"Produce a frozen Trace from the recorded events.\n\n        Args:\n            input_text: The input given to the agent.\n            output: The final output from the agent.\n\n        Returns:\n            An immutable Trace object.\n\n        Raises:\n            TraceError: If no recording session was started.\n        \"\"\"\n        if self._context is None:\n            raise TraceError(\"No recording session \u2014 call recording() first\")\n\n        ctx = self._context\n        total_input = sum(c.input_tokens for c in ctx.llm_calls)\n        total_output = sum(c.output_tokens for c in ctx.llm_calls)\n\n        trace = Trace(\n            agent_name=ctx.agent_name,\n            model=ctx.model,\n            input_text=input_text,\n            output_text=output,\n            turns=tuple(ctx.turns),\n            llm_calls=tuple(ctx.llm_calls),\n            tool_calls=tuple(ctx.tool_calls),\n            total_input_tokens=total_input,\n            total_output_tokens=total_output,\n            total_latency_ms=ctx.elapsed_ms,\n            tags=tuple(ctx.tags),\n            metadata=ctx.metadata,\n            created_at=datetime.now(UTC),\n        )\n\n        self._context = None\n        return trace\n</code></pre>"},{"location":"reference/api/trace/#agentprobe.trace.recorder.TraceRecorder.__init__","title":"<code>__init__(agent_name, model=None, tags=None)</code>","text":"<p>Initialize a new trace recorder.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>Identifier for the agent being recorded.</p> required <code>model</code> <code>str | None</code> <p>Primary model name.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Optional tags for categorization.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If agent_name is empty.</p> Source code in <code>src/agentprobe/trace/recorder.py</code> <pre><code>def __init__(\n    self,\n    agent_name: str,\n    model: str | None = None,\n    tags: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Initialize a new trace recorder.\n\n    Args:\n        agent_name: Identifier for the agent being recorded.\n        model: Primary model name.\n        tags: Optional tags for categorization.\n\n    Raises:\n        ValueError: If agent_name is empty.\n    \"\"\"\n    if not agent_name:\n        msg = \"agent_name must not be empty\"\n        raise ValueError(msg)\n    self._agent_name = agent_name\n    self._model = model\n    self._tags = tags or []\n    self._context: TraceRecordingContext | None = None\n</code></pre>"},{"location":"reference/api/trace/#agentprobe.trace.recorder.TraceRecorder.recording","title":"<code>recording()</code>  <code>async</code>","text":"<p>Start a recording session.</p> <p>Yields a TraceRecordingContext for recording events.</p> <p>Yields:</p> Type Description <code>AsyncGenerator[TraceRecordingContext, None]</code> <p>A mutable recording context.</p> Source code in <code>src/agentprobe/trace/recorder.py</code> <pre><code>@asynccontextmanager\nasync def recording(self) -&gt; AsyncGenerator[TraceRecordingContext, None]:\n    \"\"\"Start a recording session.\n\n    Yields a TraceRecordingContext for recording events.\n\n    Yields:\n        A mutable recording context.\n    \"\"\"\n    self._context = TraceRecordingContext(\n        agent_name=self._agent_name,\n        model=self._model,\n    )\n    self._context.tags = list(self._tags)\n    logger.debug(\"Started recording for agent '%s'\", self._agent_name)\n    try:\n        yield self._context\n    except Exception:\n        logger.exception(\"Error during recording for agent '%s'\", self._agent_name)\n        raise\n    finally:\n        logger.debug(\n            \"Recording ended for agent '%s': %d LLM calls, %d tool calls\",\n            self._agent_name,\n            len(self._context.llm_calls),\n            len(self._context.tool_calls),\n        )\n</code></pre>"},{"location":"reference/api/trace/#agentprobe.trace.recorder.TraceRecorder.finalize","title":"<code>finalize(*, input_text='', output='')</code>","text":"<p>Produce a frozen Trace from the recorded events.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The input given to the agent.</p> <code>''</code> <code>output</code> <code>str</code> <p>The final output from the agent.</p> <code>''</code> <p>Returns:</p> Type Description <code>Trace</code> <p>An immutable Trace object.</p> <p>Raises:</p> Type Description <code>TraceError</code> <p>If no recording session was started.</p> Source code in <code>src/agentprobe/trace/recorder.py</code> <pre><code>def finalize(\n    self,\n    *,\n    input_text: str = \"\",\n    output: str = \"\",\n) -&gt; Trace:\n    \"\"\"Produce a frozen Trace from the recorded events.\n\n    Args:\n        input_text: The input given to the agent.\n        output: The final output from the agent.\n\n    Returns:\n        An immutable Trace object.\n\n    Raises:\n        TraceError: If no recording session was started.\n    \"\"\"\n    if self._context is None:\n        raise TraceError(\"No recording session \u2014 call recording() first\")\n\n    ctx = self._context\n    total_input = sum(c.input_tokens for c in ctx.llm_calls)\n    total_output = sum(c.output_tokens for c in ctx.llm_calls)\n\n    trace = Trace(\n        agent_name=ctx.agent_name,\n        model=ctx.model,\n        input_text=input_text,\n        output_text=output,\n        turns=tuple(ctx.turns),\n        llm_calls=tuple(ctx.llm_calls),\n        tool_calls=tuple(ctx.tool_calls),\n        total_input_tokens=total_input,\n        total_output_tokens=total_output,\n        total_latency_ms=ctx.elapsed_ms,\n        tags=tuple(ctx.tags),\n        metadata=ctx.metadata,\n        created_at=datetime.now(UTC),\n    )\n\n    self._context = None\n    return trace\n</code></pre>"},{"location":"reference/api/trace/#replay-engine","title":"Replay Engine","text":""},{"location":"reference/api/trace/#agentprobe.trace.replay","title":"<code>agentprobe.trace.replay</code>","text":"<p>Trace replay engine for re-executing recorded traces.</p> <p>Supports pure replay from recorded data, with optional mock overrides for tool calls and outputs. Computes a ReplayDiff showing differences between original and replayed results.</p>"},{"location":"reference/api/trace/#agentprobe.trace.replay.ReplayEngine","title":"<code>ReplayEngine</code>","text":"<p>Replays recorded traces for comparison and testing.</p> <p>In pure replay mode, tool calls return their recorded outputs. Optional mock functions can override specific tools.</p> <p>Attributes:</p> Name Type Description <code>mock_tools</code> <p>Mapping of tool names to mock functions.</p> <code>mock_output</code> <p>If set, override the replay output text.</p> Source code in <code>src/agentprobe/trace/replay.py</code> <pre><code>class ReplayEngine:\n    \"\"\"Replays recorded traces for comparison and testing.\n\n    In pure replay mode, tool calls return their recorded outputs.\n    Optional mock functions can override specific tools.\n\n    Attributes:\n        mock_tools: Mapping of tool names to mock functions.\n        mock_output: If set, override the replay output text.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mock_tools: dict[str, Callable[..., Any]] | None = None,\n        mock_output: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the replay engine.\n\n        Args:\n            mock_tools: Optional tool name to mock function mapping.\n            mock_output: If set, override the output text.\n        \"\"\"\n        self._mock_tools = mock_tools or {}\n        self._mock_output = mock_output\n\n    def replay(self, trace: Trace) -&gt; Trace:\n        \"\"\"Replay a trace, applying any mock overrides.\n\n        Args:\n            trace: The original trace to replay.\n\n        Returns:\n            A new trace with mock overrides applied.\n        \"\"\"\n        if not self._mock_tools and self._mock_output is None:\n            return trace\n\n        modified_calls: list[ToolCall] = []\n        for tc in trace.tool_calls:\n            mock_fn = self._mock_tools.get(tc.tool_name)\n            if mock_fn is not None:\n                try:\n                    mock_result = mock_fn(tc.tool_input)\n                    modified_calls.append(tc.model_copy(update={\"tool_output\": mock_result}))\n                except Exception as exc:\n                    modified_calls.append(\n                        tc.model_copy(\n                            update={\n                                \"success\": False,\n                                \"error\": f\"Mock error: {exc}\",\n                                \"tool_output\": None,\n                            }\n                        )\n                    )\n            else:\n                modified_calls.append(tc)\n\n        output = self._mock_output if self._mock_output is not None else trace.output_text\n\n        return trace.model_copy(\n            update={\n                \"tool_calls\": tuple(modified_calls),\n                \"output_text\": output,\n            },\n        )\n\n    def diff(self, original: Trace, replay: Trace) -&gt; ReplayDiff:\n        \"\"\"Compute the diff between an original trace and a replay.\n\n        Args:\n            original: The original trace.\n            replay: The replayed trace.\n\n        Returns:\n            A ReplayDiff showing the differences.\n        \"\"\"\n        tool_diffs: list[DiffItem] = []\n\n        max_len = max(len(original.tool_calls), len(replay.tool_calls))\n        for i in range(max_len):\n            if i &lt; len(original.tool_calls) and i &lt; len(replay.tool_calls):\n                orig_tc = original.tool_calls[i]\n                replay_tc = replay.tool_calls[i]\n                sim = _tool_call_similarity(orig_tc, replay_tc)\n                tool_diffs.append(\n                    DiffItem(\n                        dimension=f\"tool_call_{i}\",\n                        expected=orig_tc.tool_name,\n                        actual=replay_tc.tool_name,\n                        similarity=round(sim, 4),\n                    )\n                )\n            elif i &lt; len(original.tool_calls):\n                tool_diffs.append(\n                    DiffItem(\n                        dimension=f\"tool_call_{i}\",\n                        expected=original.tool_calls[i].tool_name,\n                        actual=None,\n                        similarity=0.0,\n                    )\n                )\n            else:\n                tool_diffs.append(\n                    DiffItem(\n                        dimension=f\"tool_call_{i}\",\n                        expected=None,\n                        actual=replay.tool_calls[i].tool_name,\n                        similarity=0.0,\n                    )\n                )\n\n        output_matches = original.output_text == replay.output_text\n\n        return ReplayDiff(\n            original_trace_id=original.trace_id,\n            replay_trace_id=replay.trace_id,\n            tool_call_diffs=tuple(tool_diffs),\n            output_matches=output_matches,\n            original_output=original.output_text,\n            replay_output=replay.output_text,\n        )\n</code></pre>"},{"location":"reference/api/trace/#agentprobe.trace.replay.ReplayEngine.__init__","title":"<code>__init__(*, mock_tools=None, mock_output=None)</code>","text":"<p>Initialize the replay engine.</p> <p>Parameters:</p> Name Type Description Default <code>mock_tools</code> <code>dict[str, Callable[..., Any]] | None</code> <p>Optional tool name to mock function mapping.</p> <code>None</code> <code>mock_output</code> <code>str | None</code> <p>If set, override the output text.</p> <code>None</code> Source code in <code>src/agentprobe/trace/replay.py</code> <pre><code>def __init__(\n    self,\n    *,\n    mock_tools: dict[str, Callable[..., Any]] | None = None,\n    mock_output: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the replay engine.\n\n    Args:\n        mock_tools: Optional tool name to mock function mapping.\n        mock_output: If set, override the output text.\n    \"\"\"\n    self._mock_tools = mock_tools or {}\n    self._mock_output = mock_output\n</code></pre>"},{"location":"reference/api/trace/#agentprobe.trace.replay.ReplayEngine.replay","title":"<code>replay(trace)</code>","text":"<p>Replay a trace, applying any mock overrides.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The original trace to replay.</p> required <p>Returns:</p> Type Description <code>Trace</code> <p>A new trace with mock overrides applied.</p> Source code in <code>src/agentprobe/trace/replay.py</code> <pre><code>def replay(self, trace: Trace) -&gt; Trace:\n    \"\"\"Replay a trace, applying any mock overrides.\n\n    Args:\n        trace: The original trace to replay.\n\n    Returns:\n        A new trace with mock overrides applied.\n    \"\"\"\n    if not self._mock_tools and self._mock_output is None:\n        return trace\n\n    modified_calls: list[ToolCall] = []\n    for tc in trace.tool_calls:\n        mock_fn = self._mock_tools.get(tc.tool_name)\n        if mock_fn is not None:\n            try:\n                mock_result = mock_fn(tc.tool_input)\n                modified_calls.append(tc.model_copy(update={\"tool_output\": mock_result}))\n            except Exception as exc:\n                modified_calls.append(\n                    tc.model_copy(\n                        update={\n                            \"success\": False,\n                            \"error\": f\"Mock error: {exc}\",\n                            \"tool_output\": None,\n                        }\n                    )\n                )\n        else:\n            modified_calls.append(tc)\n\n    output = self._mock_output if self._mock_output is not None else trace.output_text\n\n    return trace.model_copy(\n        update={\n            \"tool_calls\": tuple(modified_calls),\n            \"output_text\": output,\n        },\n    )\n</code></pre>"},{"location":"reference/api/trace/#agentprobe.trace.replay.ReplayEngine.diff","title":"<code>diff(original, replay)</code>","text":"<p>Compute the diff between an original trace and a replay.</p> <p>Parameters:</p> Name Type Description Default <code>original</code> <code>Trace</code> <p>The original trace.</p> required <code>replay</code> <code>Trace</code> <p>The replayed trace.</p> required <p>Returns:</p> Type Description <code>ReplayDiff</code> <p>A ReplayDiff showing the differences.</p> Source code in <code>src/agentprobe/trace/replay.py</code> <pre><code>def diff(self, original: Trace, replay: Trace) -&gt; ReplayDiff:\n    \"\"\"Compute the diff between an original trace and a replay.\n\n    Args:\n        original: The original trace.\n        replay: The replayed trace.\n\n    Returns:\n        A ReplayDiff showing the differences.\n    \"\"\"\n    tool_diffs: list[DiffItem] = []\n\n    max_len = max(len(original.tool_calls), len(replay.tool_calls))\n    for i in range(max_len):\n        if i &lt; len(original.tool_calls) and i &lt; len(replay.tool_calls):\n            orig_tc = original.tool_calls[i]\n            replay_tc = replay.tool_calls[i]\n            sim = _tool_call_similarity(orig_tc, replay_tc)\n            tool_diffs.append(\n                DiffItem(\n                    dimension=f\"tool_call_{i}\",\n                    expected=orig_tc.tool_name,\n                    actual=replay_tc.tool_name,\n                    similarity=round(sim, 4),\n                )\n            )\n        elif i &lt; len(original.tool_calls):\n            tool_diffs.append(\n                DiffItem(\n                    dimension=f\"tool_call_{i}\",\n                    expected=original.tool_calls[i].tool_name,\n                    actual=None,\n                    similarity=0.0,\n                )\n            )\n        else:\n            tool_diffs.append(\n                DiffItem(\n                    dimension=f\"tool_call_{i}\",\n                    expected=None,\n                    actual=replay.tool_calls[i].tool_name,\n                    similarity=0.0,\n                )\n            )\n\n    output_matches = original.output_text == replay.output_text\n\n    return ReplayDiff(\n        original_trace_id=original.trace_id,\n        replay_trace_id=replay.trace_id,\n        tool_call_diffs=tuple(tool_diffs),\n        output_matches=output_matches,\n        original_output=original.output_text,\n        replay_output=replay.output_text,\n    )\n</code></pre>"},{"location":"reference/api/trace/#time-travel","title":"Time Travel","text":""},{"location":"reference/api/trace/#agentprobe.trace.time_travel","title":"<code>agentprobe.trace.time_travel</code>","text":"<p>Time-travel debugger for step-by-step trace inspection.</p> <p>Provides indexed access to individual turns in a trace with cumulative metrics at each step, enabling debugging and analysis of agent execution flow.</p>"},{"location":"reference/api/trace/#agentprobe.trace.time_travel.TimeTravel","title":"<code>TimeTravel</code>","text":"<p>Step-by-step trace inspector with cumulative metrics.</p> <p>Pre-computes a list of TraceStep objects on construction, providing indexed access and iteration over the trace timeline with cumulative token, cost, and latency metrics at each step.</p> <p>Attributes:</p> Name Type Description <code>trace</code> <code>Trace</code> <p>The trace being inspected.</p> Source code in <code>src/agentprobe/trace/time_travel.py</code> <pre><code>class TimeTravel:\n    \"\"\"Step-by-step trace inspector with cumulative metrics.\n\n    Pre-computes a list of TraceStep objects on construction,\n    providing indexed access and iteration over the trace timeline\n    with cumulative token, cost, and latency metrics at each step.\n\n    Attributes:\n        trace: The trace being inspected.\n    \"\"\"\n\n    def __init__(\n        self, trace: Trace, *, cost_per_1k_input: float = 0.0, cost_per_1k_output: float = 0.0\n    ) -&gt; None:\n        \"\"\"Initialize the time-travel debugger.\n\n        Args:\n            trace: The trace to inspect.\n            cost_per_1k_input: Cost per 1K input tokens for cumulative cost.\n            cost_per_1k_output: Cost per 1K output tokens for cumulative cost.\n        \"\"\"\n        self._trace = trace\n        self._steps = self._build_steps(trace, cost_per_1k_input, cost_per_1k_output)\n\n    @property\n    def trace(self) -&gt; Trace:\n        \"\"\"Return the underlying trace.\"\"\"\n        return self._trace\n\n    @property\n    def total_steps(self) -&gt; int:\n        \"\"\"Return the total number of steps.\"\"\"\n        return len(self._steps)\n\n    @staticmethod\n    def _build_steps(\n        trace: Trace,\n        cost_per_1k_input: float,\n        cost_per_1k_output: float,\n    ) -&gt; list[TraceStep]:\n        \"\"\"Build the list of trace steps with cumulative metrics.\"\"\"\n        steps: list[TraceStep] = []\n        cum_input = 0\n        cum_output = 0\n        cum_cost = 0.0\n        cum_latency = 0\n\n        for i, turn in enumerate(trace.turns):\n            if turn.turn_type == TurnType.LLM_CALL and turn.llm_call is not None:\n                cum_input += turn.llm_call.input_tokens\n                cum_output += turn.llm_call.output_tokens\n                cum_cost += (\n                    turn.llm_call.input_tokens / 1000.0 * cost_per_1k_input\n                    + turn.llm_call.output_tokens / 1000.0 * cost_per_1k_output\n                )\n                cum_latency += turn.llm_call.latency_ms\n            elif turn.turn_type == TurnType.TOOL_CALL and turn.tool_call is not None:\n                cum_latency += turn.tool_call.latency_ms\n\n            steps.append(\n                TraceStep(\n                    step_index=i,\n                    turn=turn,\n                    cumulative_input_tokens=cum_input,\n                    cumulative_output_tokens=cum_output,\n                    cumulative_cost_usd=round(cum_cost, 6),\n                    cumulative_latency_ms=cum_latency,\n                )\n            )\n\n        return steps\n\n    def __len__(self) -&gt; int:\n        return len(self._steps)\n\n    def __getitem__(self, index: int) -&gt; TraceStep:\n        \"\"\"Get a step by index.\n\n        Args:\n            index: Zero-based step index. Supports negative indexing.\n\n        Returns:\n            The TraceStep at the given index.\n\n        Raises:\n            IndexError: If the index is out of range.\n        \"\"\"\n        return self._steps[index]\n\n    def __iter__(self) -&gt; Iterator[TraceStep]:\n        return iter(self._steps)\n\n    def steps(self) -&gt; list[TraceStep]:\n        \"\"\"Return all steps as a list.\"\"\"\n        return list(self._steps)\n\n    def rerun_from(self, step_index: int) -&gt; list[TraceStep]:\n        \"\"\"Return all steps from a given index onward.\n\n        Args:\n            step_index: Zero-based starting index.\n\n        Returns:\n            Steps from the given index to the end.\n\n        Raises:\n            IndexError: If step_index is out of range.\n        \"\"\"\n        if step_index &lt; 0 or step_index &gt;= len(self._steps):\n            raise IndexError(f\"Step index {step_index} out of range [0, {len(self._steps)})\")\n        return list(self._steps[step_index:])\n</code></pre>"},{"location":"reference/api/trace/#agentprobe.trace.time_travel.TimeTravel.trace","title":"<code>trace</code>  <code>property</code>","text":"<p>Return the underlying trace.</p>"},{"location":"reference/api/trace/#agentprobe.trace.time_travel.TimeTravel.total_steps","title":"<code>total_steps</code>  <code>property</code>","text":"<p>Return the total number of steps.</p>"},{"location":"reference/api/trace/#agentprobe.trace.time_travel.TimeTravel.__init__","title":"<code>__init__(trace, *, cost_per_1k_input=0.0, cost_per_1k_output=0.0)</code>","text":"<p>Initialize the time-travel debugger.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>The trace to inspect.</p> required <code>cost_per_1k_input</code> <code>float</code> <p>Cost per 1K input tokens for cumulative cost.</p> <code>0.0</code> <code>cost_per_1k_output</code> <code>float</code> <p>Cost per 1K output tokens for cumulative cost.</p> <code>0.0</code> Source code in <code>src/agentprobe/trace/time_travel.py</code> <pre><code>def __init__(\n    self, trace: Trace, *, cost_per_1k_input: float = 0.0, cost_per_1k_output: float = 0.0\n) -&gt; None:\n    \"\"\"Initialize the time-travel debugger.\n\n    Args:\n        trace: The trace to inspect.\n        cost_per_1k_input: Cost per 1K input tokens for cumulative cost.\n        cost_per_1k_output: Cost per 1K output tokens for cumulative cost.\n    \"\"\"\n    self._trace = trace\n    self._steps = self._build_steps(trace, cost_per_1k_input, cost_per_1k_output)\n</code></pre>"},{"location":"reference/api/trace/#agentprobe.trace.time_travel.TimeTravel.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get a step by index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Zero-based step index. Supports negative indexing.</p> required <p>Returns:</p> Type Description <code>TraceStep</code> <p>The TraceStep at the given index.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If the index is out of range.</p> Source code in <code>src/agentprobe/trace/time_travel.py</code> <pre><code>def __getitem__(self, index: int) -&gt; TraceStep:\n    \"\"\"Get a step by index.\n\n    Args:\n        index: Zero-based step index. Supports negative indexing.\n\n    Returns:\n        The TraceStep at the given index.\n\n    Raises:\n        IndexError: If the index is out of range.\n    \"\"\"\n    return self._steps[index]\n</code></pre>"},{"location":"reference/api/trace/#agentprobe.trace.time_travel.TimeTravel.steps","title":"<code>steps()</code>","text":"<p>Return all steps as a list.</p> Source code in <code>src/agentprobe/trace/time_travel.py</code> <pre><code>def steps(self) -&gt; list[TraceStep]:\n    \"\"\"Return all steps as a list.\"\"\"\n    return list(self._steps)\n</code></pre>"},{"location":"reference/api/trace/#agentprobe.trace.time_travel.TimeTravel.rerun_from","title":"<code>rerun_from(step_index)</code>","text":"<p>Return all steps from a given index onward.</p> <p>Parameters:</p> Name Type Description Default <code>step_index</code> <code>int</code> <p>Zero-based starting index.</p> required <p>Returns:</p> Type Description <code>list[TraceStep]</code> <p>Steps from the given index to the end.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If step_index is out of range.</p> Source code in <code>src/agentprobe/trace/time_travel.py</code> <pre><code>def rerun_from(self, step_index: int) -&gt; list[TraceStep]:\n    \"\"\"Return all steps from a given index onward.\n\n    Args:\n        step_index: Zero-based starting index.\n\n    Returns:\n        Steps from the given index to the end.\n\n    Raises:\n        IndexError: If step_index is out of range.\n    \"\"\"\n    if step_index &lt; 0 or step_index &gt;= len(self._steps):\n        raise IndexError(f\"Step index {step_index} out of range [0, {len(self._steps)})\")\n    return list(self._steps[step_index:])\n</code></pre>"}]}